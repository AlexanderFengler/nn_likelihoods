{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load my own functions\n",
    "import keras_to_numpy as ktnp\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "import make_data_wfpt as mdw\n",
    "from kde_training_utilities import kde_load_data\n",
    "import ddm_data_simulation as ds\n",
    "import cddm_data_simulation as cds\n",
    "import boundary_functions as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle some cuda business\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f05edb860b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "model_path = '/media/data_cifs/afengler/data/kde/ddm/keras_models/dnnregressor_ddm_06_28_19_00_58_26/model_0' \n",
    "ckpt_path = '/media/data_cifs/afengler/data/kde/ddm/keras_models/dnnregressor_ddm_06_28_19_00_58_26/ckpt_0_final'\n",
    "\n",
    "model = keras.models.load_model(model_path)\n",
    "model.load_weights(ckpt_path)\n",
    "\n",
    "# model_path = \"/home/tony/repos/temp_models/keras_models/dnnregressor_ddm_06_28_19_00_58_26/model_0\"\n",
    "# ckpt_path = \"/home/tony/repos/temp_models/keras_models/dnnregressor_ddm_06_28_19_00_58_26/ckpt_0_final\"\n",
    "\n",
    "# model = keras.models.load_model(model_path)\n",
    "# model.load_weights(ckpt_path)\n",
    "\n",
    "# network_path = \"/home/tony/repos/temp_models/keras_models/\\\n",
    "# dnnregressoranalytical_ddm_07_25_19_15_50_52/model.h5\"\n",
    "\n",
    "#model = keras.models.load_model(network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases, activations = ktnp.extract_architecture(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ktnp.log_p(np.array([[0.5, 1, .7, 1, 1]]), weights, biases, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array([[0.5, 1, .7, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_from_meta_data(file_path  = ''):\n",
    "    tmp = pickle.load(open(file_path, 'rb'))[2]\n",
    "    params = []\n",
    "    for key in tmp.keys():\n",
    "        if key == 'delta_t':\n",
    "            break\n",
    "        if key != 's':\n",
    "            params.append(key)\n",
    "    return params   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function boundary_functions.constant(t=0)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundary #= eval('bf.constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations -----\n",
    "n_runs = 1\n",
    "n_samples = 2500\n",
    "feature_file_path = '/media/data_cifs/afengler/data/kde/ddm/train_test_data/test_features.pickle'\n",
    "mle_out_path = '/media/data_cifs/afengler/data/kde/ddm/mle_runs'\n",
    "\n",
    "# NOTE PARAMETERS: \n",
    "# WEIBULL: [v, a, w, node, shape, scale]\n",
    "# param_bounds = [(-1, 1), (0.3, 2), (0.3, 0.7), (0.01, 0.01), (0, np.pi / 2.2)]\n",
    "\n",
    "# my_optim_columns = ['v_sim', 'a_sim', 'w_sim',\n",
    "#                     'v_mle', 'a_mle', 'w_mle', 'n_samples']\n",
    "\n",
    "# Get parameter names in correct ordering:\n",
    "#dat = pickle.load(open(feature_file_path,'rb'))\n",
    "meta_data_file_path = '/media/data_cifs/afengler/data/kde/ddm/train_test_data/meta_data.pickle'\n",
    "parameter_names = get_params_from_meta_data(file_path = meta_data_file_path)\n",
    "\n",
    "param_bounds = [(-1, 1), (0.5, 2), (0.3, 0.7)]\n",
    "\n",
    "\n",
    "#parameter_names = list(dat.keys())[:-2] # :-1 to get rid of 'rt' and 'choice' here\n",
    "\n",
    "# Make columns for optimizer result table\n",
    "p_sim = []\n",
    "p_mle = []\n",
    "param_bounds = []\n",
    "\n",
    "for parameter_name in parameter_names:\n",
    "    p_sim.append(parameter_name + '_sim')\n",
    "    p_mle.append(parameter_name + '_mle')\n",
    "    #param_bounds = param_bounds.append()\n",
    "    \n",
    "my_optim_columns = p_sim + p_mle + ['n_samples']\n",
    "\n",
    "# Initialize the data frame in which to store optimizer results\n",
    "optim_results = pd.DataFrame(np.zeros((n_runs, len(my_optim_columns))), columns = my_optim_columns)\n",
    "optim_results.iloc[:, 2 * len(parameter_names)] = n_samples\n",
    "\n",
    "# define boundary\n",
    "boundary = bf.constant\n",
    "boundary_multiplicative = True\n",
    "\n",
    "\n",
    "# get network architecture\n",
    "weights, biases, activations = ktnp.extract_architecture(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_params(param_bounds = []):\n",
    "    params = np.zeros(len(param_bounds))\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        params[i] = np.random.uniform(low = param_bounds[i][0], high = param_bounds[i][1])\n",
    "        \n",
    "    return params\n",
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the likelihood function\n",
    "def log_p(params = [0, 1, 0.9], model = [], data = [], ll_min = 1e-29):\n",
    "    # Make feature array\n",
    "    feature_array = np.zeros((data[0].shape[0], len(params) + 2))\n",
    "    \n",
    "    # Store parameters\n",
    "    cnt = 0\n",
    "    for i in range(0, len(params), 1):\n",
    "        feature_array[:, i] = params[i]\n",
    "        cnt += 1\n",
    "    \n",
    "    # Store rts and choices\n",
    "    feature_array[:, cnt] = data[0].ravel() # rts\n",
    "    feature_array[:, cnt + 1] = data[1].ravel() # choices\n",
    "    \n",
    "    # Get model predictions\n",
    "    prediction = np.maximum(model.predict(feature_array), ll_min)\n",
    "    \n",
    "    return(- np.sum(np.log(prediction)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = np.tile(true_params, (data.shape[0], 1))\n",
    "inp = np.concatenate([param_grid, data], axis=1)\n",
    "\n",
    "prediction = np_predict(inp, weights, biases, activations)\n",
    "prediction.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_params = make_params(param_bounds = param_bounds)\n",
    "boundary_params = {}\n",
    "ddm_dat_tmp = cds.ddm_flexbound(v = tmp_params[0],\n",
    "                                    a = tmp_params[1],\n",
    "                                    w = tmp_params[2],\n",
    "                                    s = 1,\n",
    "                                    delta_t = 0.001,\n",
    "                                    max_t = 20,\n",
    "                                    n_samples = n_samples,\n",
    "                                    boundary_fun = boundary, # function of t (and potentially other parameters) that takes in (t, *args)\n",
    "                                    boundary_multiplicative = boundary_multiplicative, # CAREFUL: CHECK IF BOUND\n",
    "                                    boundary_params = boundary_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5480.963312381616"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_np = np.concatenate([ddm_dat_tmp[0], ddm_dat_tmp[1]], axis = 1)\n",
    "t = ktnp.log_p(tmp_params, weights, biases, activations, data_np)\n",
    "#params, weights, biases, activations, data\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for run 0: \n",
      "[-0.85818865  1.75881296  0.42076234]\n",
      "Mean rt for current run: \n",
      "1.6219672\n",
      "running sequential\n",
      "differential_evolution step 1: f(x)= 3691.8\n",
      "differential_evolution step 2: f(x)= 3669.8\n",
      "differential_evolution step 3: f(x)= 3642.52\n",
      "differential_evolution step 4: f(x)= 3630.24\n",
      "differential_evolution step 5: f(x)= 3629.41\n",
      "differential_evolution step 6: f(x)= 3618.85\n",
      "differential_evolution step 7: f(x)= 3618.85\n",
      "differential_evolution step 8: f(x)= 3607.54\n",
      "differential_evolution step 9: f(x)= 3607.54\n",
      "differential_evolution step 10: f(x)= 3605.27\n",
      "differential_evolution step 11: f(x)= 3602.54\n",
      "00:01:24\n",
      "running sequential ktnp\n",
      "differential_evolution step 1: f(x)= 3677.35\n",
      "differential_evolution step 2: f(x)= 3654.28\n",
      "differential_evolution step 3: f(x)= 3645.29\n",
      "differential_evolution step 4: f(x)= 3645.29\n",
      "differential_evolution step 5: f(x)= 3632.93\n",
      "differential_evolution step 6: f(x)= 3605.83\n",
      "differential_evolution step 7: f(x)= 3605.71\n",
      "differential_evolution step 8: f(x)= 3605.71\n",
      "differential_evolution step 9: f(x)= 3604.22\n",
      "differential_evolution step 10: f(x)= 3604.22\n",
      "00:00:18\n",
      "running parallel\n",
      "differential_evolution step 1: f(x)= 3644.58\n",
      "differential_evolution step 2: f(x)= 3644.58\n",
      "differential_evolution step 3: f(x)= 3644.58\n",
      "differential_evolution step 4: f(x)= 3613.63\n",
      "differential_evolution step 5: f(x)= 3609.16\n",
      "differential_evolution step 6: f(x)= 3606.34\n",
      "differential_evolution step 7: f(x)= 3606.34\n",
      "differential_evolution step 8: f(x)= 3606.34\n",
      "differential_evolution step 9: f(x)= 3606.34\n",
      "differential_evolution step 10: f(x)= 3606.34\n",
      "differential_evolution step 11: f(x)= 3605.76\n",
      "00:00:20\n",
      "Solution vector of current run: \n",
      "[-0.75773137  1.6361231   0.42711392]\n",
      "Solution vector of current run parallel: \n",
      "[-0.75925631  1.64520258  0.42779863]\n",
      "Solution vector of current run seq ktnp\n",
      "[-0.75925523  1.64520298  0.42779694]\n",
      "The run took: \n",
      "00:02:04\n"
     ]
    }
   ],
   "source": [
    "# Main loop ----------- TD: Parallelize\n",
    "for i in range(0, n_runs, 1): \n",
    "    \n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate set of parameters\n",
    "    tmp_params = make_params(param_bounds = param_bounds)\n",
    "    \n",
    "    # Store in output file\n",
    "    optim_results.iloc[i, :len(parameter_names)] = tmp_params\n",
    "    \n",
    "    # Print some info on run\n",
    "    print('Parameters for run ' + str(i) + ': ')\n",
    "    print(tmp_params)\n",
    "    \n",
    "    # Define boundary params\n",
    "    boundary_params = {}\n",
    "    \n",
    "    # Run model simulations\n",
    "    ddm_dat_tmp = cds.ddm_flexbound(v = tmp_params[0],\n",
    "                                    a = tmp_params[1],\n",
    "                                    w = tmp_params[2],\n",
    "                                    s = 1,\n",
    "                                    delta_t = 0.001,\n",
    "                                    max_t = 20,\n",
    "                                    n_samples = n_samples,\n",
    "                                    boundary_fun = boundary, # function of t (and potentially other parameters) that takes in (t, *args)\n",
    "                                    boundary_multiplicative = boundary_multiplicative, # CAREFUL: CHECK IF BOUND\n",
    "                                    boundary_params = boundary_params)\n",
    "        \n",
    "    # Print some info on run\n",
    "    print('Mean rt for current run: ')\n",
    "    print(np.mean(ddm_dat_tmp[0]))\n",
    "    \n",
    "    # Run optimizer standard\n",
    "    print('running sequential')\n",
    "    start_time_sequential = time.time()\n",
    "    out = differential_evolution(log_p, \n",
    "                                 bounds = param_bounds, \n",
    "                                 args = (model, ddm_dat_tmp), \n",
    "                                 popsize = 30,\n",
    "                                 disp = True)\n",
    "    elapsed_sequential = time.time() - start_time_sequential\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_sequential)))\n",
    "    \n",
    "    # Run optimizer sequential with ktnp\n",
    "    print('running sequential ktnp')\n",
    "    start_time_sequential_np = time.time()\n",
    "    data_np = np.concatenate([ddm_dat_tmp[0], ddm_dat_tmp[1]], axis = 1)\n",
    "    out_seq_ktnp = differential_evolution(ktnp.log_p, \n",
    "                                 bounds = param_bounds, \n",
    "                                 args = (weights, biases, activations, data_np), \n",
    "                                 popsize = 30,\n",
    "                                 disp = True)\n",
    "    elapsed_sequential_np = time.time() - start_time_sequential_np\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_sequential_np)))\n",
    "    \n",
    "    # Run optimizer parallel\n",
    "    print('running parallel')\n",
    "    start_time_parallel = time.time()\n",
    "    data_np = np.concatenate([ddm_dat_tmp[0], ddm_dat_tmp[1]], axis = 1)\n",
    "    out_parallel = differential_evolution(ktnp.log_p, \n",
    "                                          bounds = param_bounds,\n",
    "                                          args = (weights, biases, activations, data_np),\n",
    "                                          popsize = 30,\n",
    "                                          disp = True, \n",
    "                                          workers = -1)\n",
    "    elapsed_parallel = time.time() - start_time_parallel\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_parallel)))\n",
    "\n",
    "    # Print some info\n",
    "    print('Solution vector of current run: ')\n",
    "    print(out.x)\n",
    "    \n",
    "    print('Solution vector of current run parallel: ')\n",
    "    print(out_parallel.x)\n",
    "    \n",
    "    print('Solution vector of current run seq ktnp')\n",
    "    print(out_seq_ktnp.x)\n",
    "    \n",
    "    print('The run took: ')\n",
    "    elapsed = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed)))\n",
    "    \n",
    "    # Store result in output file\n",
    "    optim_results.iloc[i, len(parameter_names):(2*len(parameter_names))] = out.x\n",
    "# -----------------------\n",
    "\n",
    "# Save optimization results to file\n",
    "optim_results.to_csv(mle_out_path + '/mle_results_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results\n",
    "optim_results = pd.read_csv(os.getcwd() + '/experiments/ddm_flexbound_kde_mle_fix_v_0_c1_0_w_unbiased_arange_2_3/optim_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['v_sim'], optim_results['v_mle'], c = optim_results['c2_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for v\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['v_mle'], 1), np.expand_dims(optim_results['v_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['v_mle'], 1), np.expand_dims(optim_results['v_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['a_sim'], optim_results['a_mle'], c = optim_results['c2_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for a\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['a_mle'], 1), np.expand_dims(optim_results['a_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['a_mle'], 1), np.expand_dims(optim_results['a_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['w_sim'], optim_results['w_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for w\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['w_mle'], 1), np.expand_dims(optim_results['w_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['w_mle'], 1), np.expand_dims(optim_results['w_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['c1_sim'], optim_results['c1_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for c1\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['c1_mle'], 1), np.expand_dims(optim_results['c1_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['c1_mle'], 1), np.expand_dims(optim_results['c1_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['c2_sim'], optim_results['c2_mle'], c = optim_results['a_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for w\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['c2_mle'], 1), np.expand_dims(optim_results['c2_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['c2_mle'], 1), np.expand_dims(optim_results['c2_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "testing = np.tile(np.array([1,2,3]), (100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(testing.T, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddm_dat_tmp = cds.ddm_flexbound(v = tmp_params[0],\n",
    "                                a = tmp_params[1],\n",
    "                                w = tmp_params[2],\n",
    "                                s = 1,\n",
    "                                delta_t = 0.001,\n",
    "                                max_t = 20,\n",
    "                                n_samples = n_samples,\n",
    "                                boundary_fun = boundary, # function of t (and potentially other parameters) that takes in (t, *args)\n",
    "                                boundary_multiplicative = boundary_multiplicative, # CAREFUL: CHECK IF BOUND\n",
    "                                boundary_params = boundary_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
