{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "from kde_training_utilities import kde_load_data\n",
    "from kde_training_utilities import kde_make_train_test_split\n",
    "import make_data_wfpt as mdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16348381029164850821\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7930876733673738056\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12048649421\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 72182277514369685\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12856655918892481973\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Handle some cuda business\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer()\n",
    "\n",
    "# Define folder in which dataset lies\n",
    "data_folder = '/media/data_cifs/afengler/data/kde/full_ddm/train_test_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get files in folder\n",
      "check if we have a train and test sets already\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'looks like a train test split exists in folder: Please remove before running this function'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make train test split\n",
    "kde_make_train_test_split(folder = data_folder,\n",
    "                          p_train = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train test split\n",
    "cpm.data['train_features'], cpm.data['train_labels'], cpm.data['test_features'], cpm.data['test_labels'] = kde_load_data(folder = data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111566113, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['test_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446278279, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['train_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features'].iloc[171247010, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features']['log_l'] = cpm.data['train_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features'].sort_values(by = 'log_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features'].iloc[22428, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_labels'][22428, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_shape': 3,\n",
       " 'output_shape': 1,\n",
       " 'output_activation': 'sigmoid',\n",
       " 'hidden_layers': [20, 20, 20, 20],\n",
       " 'hidden_activations': ['relu', 'relu', 'relu', 'relu'],\n",
       " 'l1_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l1_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'optimizer': 'Nadam',\n",
       " 'loss': 'mse',\n",
       " 'metrics': ['mse']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'callback_funs': ['ReduceLROnPlateau', 'EarlyStopping', 'ModelCheckpoint'],\n",
       " 'plateau_patience': 10,\n",
       " 'min_delta': 0.0001,\n",
       " 'early_stopping_patience': 15,\n",
       " 'callback_monitor': 'loss',\n",
       " 'min_learning_rate': 1e-07,\n",
       " 'red_coef_learning_rate': 0.1,\n",
       " 'ckpt_period': 10,\n",
       " 'ckpt_save_best_only': True,\n",
       " 'ckpt_save_weights_only': True,\n",
       " 'max_train_epochs': 2000,\n",
       " 'batch_size': 10000,\n",
       " 'warm_start': False,\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_cnt': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'choice_probabilities',\n",
       " 'model_directory': '/media/data_cifs/afengler/git_repos/nn_likelihoods/keras_models',\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_name': 'dnnregressor',\n",
       " 'data_type_signature': '_choice_probabilities_analytic_',\n",
       " 'timestamp': '07_02_19_18_18_12',\n",
       " 'training_data_size': 2500000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters concerning data storage\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, specify new set of parameters here:\n",
    "# Model params\n",
    "cpm.model_params['output_activation'] = 'linear'\n",
    "cpm.model_params['hidden_layers'] = [20, 40, 60, 80, 100, 120]\n",
    "cpm.model_params['hidden_activations'] = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "cpm.model_params['input_shape'] = cpm.data['train_features'].shape[1]\n",
    "# cpm.model_params['l1_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# cpm.model_params['l2_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l1_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# Train params\n",
    "cpm.train_params['batch_size'] = 1000000\n",
    "cpm.train_params['max_train_epochs'] = 250\n",
    "cpm.train_params['min_delta'] = 0.00001\n",
    "\n",
    "\n",
    "# Data params\n",
    "cpm.data_params['data_type'] = 'kde'\n",
    "cpm.data_params['data_type_signature'] = '_full_ddm_'\n",
    "cpm.data_params['training_data_size'] = cpm.data['train_features'].shape[0]\n",
    "cpm.data_params['timestamp'] = datetime.now().strftime('%m_%d_%y_%H_%M_%S')\n",
    "cpm.data_params['model_directory'] = '/media/data_cifs/afengler/data/kde/full_ddm/keras_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Make model\n",
    "cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 446278279 samples, validate on 111566113 samples\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/250\n",
      "446278279/446278279 [==============================] - 1209s 3us/sample - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 2/250\n",
      "446278279/446278279 [==============================] - 1531s 3us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 3/250\n",
      "446278279/446278279 [==============================] - 1236s 3us/sample - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 4/250\n",
      "446278279/446278279 [==============================] - 1418s 3us/sample - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 5/250\n",
      "446278279/446278279 [==============================] - 1419s 3us/sample - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 6/250\n",
      "446278279/446278279 [==============================] - 1228s 3us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 7/250\n",
      "446278279/446278279 [==============================] - 1131s 3us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 6.7246e-04 - val_mean_squared_error: 6.7246e-04\n",
      "Epoch 8/250\n",
      "446278279/446278279 [==============================] - 1148s 3us/sample - loss: 9.8948e-04 - mean_squared_error: 9.8948e-04 - val_loss: 8.8724e-04 - val_mean_squared_error: 8.8724e-04\n",
      "Epoch 9/250\n",
      "446278279/446278279 [==============================] - 1160s 3us/sample - loss: 9.1789e-04 - mean_squared_error: 9.1789e-04 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 10/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 8.2823e-04 - mean_squared_error: 8.2823e-04\n",
      "Epoch 00010: val_loss improved from inf to 0.00073, saving model to /media/data_cifs/afengler/data/kde/full_ddm/keras_models//dnnregressor_full_ddm_07_02_19_18_47_36/ckpt_0_10\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f2ad0236080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "446278279/446278279 [==============================] - 1200s 3us/sample - loss: 8.2818e-04 - mean_squared_error: 8.2818e-04 - val_loss: 7.2872e-04 - val_mean_squared_error: 7.2872e-04\n",
      "Epoch 11/250\n",
      "446278279/446278279 [==============================] - 1916s 4us/sample - loss: 7.7108e-04 - mean_squared_error: 7.7108e-04 - val_loss: 5.3513e-04 - val_mean_squared_error: 5.3513e-04\n",
      "Epoch 12/250\n",
      "446278279/446278279 [==============================] - 1948s 4us/sample - loss: 6.9693e-04 - mean_squared_error: 6.9693e-04 - val_loss: 6.5564e-04 - val_mean_squared_error: 6.5564e-04\n",
      "Epoch 13/250\n",
      "446278279/446278279 [==============================] - 1091s 2us/sample - loss: 6.6277e-04 - mean_squared_error: 6.6276e-04 - val_loss: 5.9014e-04 - val_mean_squared_error: 5.9014e-04\n",
      "Epoch 14/250\n",
      "446278279/446278279 [==============================] - 1097s 2us/sample - loss: 6.2905e-04 - mean_squared_error: 6.2905e-04 - val_loss: 9.1504e-04 - val_mean_squared_error: 9.1504e-04\n",
      "Epoch 15/250\n",
      "446278279/446278279 [==============================] - 1124s 3us/sample - loss: 6.0846e-04 - mean_squared_error: 6.0846e-04 - val_loss: 4.3193e-04 - val_mean_squared_error: 4.3193e-04\n",
      "Epoch 16/250\n",
      "446278279/446278279 [==============================] - 1141s 3us/sample - loss: 5.7384e-04 - mean_squared_error: 5.7384e-04 - val_loss: 5.4206e-04 - val_mean_squared_error: 5.4206e-04\n",
      "Epoch 17/250\n",
      "446278279/446278279 [==============================] - 1156s 3us/sample - loss: 5.3045e-04 - mean_squared_error: 5.3045e-04 - val_loss: 5.1294e-04 - val_mean_squared_error: 5.1294e-04\n",
      "Epoch 18/250\n",
      "446278279/446278279 [==============================] - 1175s 3us/sample - loss: 5.1340e-04 - mean_squared_error: 5.1340e-04 - val_loss: 3.8068e-04 - val_mean_squared_error: 3.8068e-04\n",
      "Epoch 19/250\n",
      "446278279/446278279 [==============================] - 1197s 3us/sample - loss: 4.9378e-04 - mean_squared_error: 4.9378e-04 - val_loss: 8.0725e-04 - val_mean_squared_error: 8.0725e-04\n",
      "Epoch 20/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 4.7777e-04 - mean_squared_error: 4.7777e-04\n",
      "Epoch 00020: val_loss improved from 0.00073 to 0.00052, saving model to /media/data_cifs/afengler/data/kde/full_ddm/keras_models//dnnregressor_full_ddm_07_02_19_18_47_36/ckpt_0_20\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f2ad0236080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "446278279/446278279 [==============================] - 1059s 2us/sample - loss: 4.7777e-04 - mean_squared_error: 4.7777e-04 - val_loss: 5.1792e-04 - val_mean_squared_error: 5.1792e-04\n",
      "Epoch 21/250\n",
      "446278279/446278279 [==============================] - 1058s 2us/sample - loss: 4.5474e-04 - mean_squared_error: 4.5474e-04 - val_loss: 4.8076e-04 - val_mean_squared_error: 4.8076e-04\n",
      "Epoch 22/250\n",
      "446278279/446278279 [==============================] - 1089s 2us/sample - loss: 4.4435e-04 - mean_squared_error: 4.4435e-04 - val_loss: 5.5253e-04 - val_mean_squared_error: 5.5253e-04\n",
      "Epoch 23/250\n",
      "446278279/446278279 [==============================] - 1099s 2us/sample - loss: 4.2862e-04 - mean_squared_error: 4.2862e-04 - val_loss: 3.3049e-04 - val_mean_squared_error: 3.3049e-04\n",
      "Epoch 24/250\n",
      "446278279/446278279 [==============================] - 1127s 3us/sample - loss: 4.0645e-04 - mean_squared_error: 4.0645e-04 - val_loss: 3.3626e-04 - val_mean_squared_error: 3.3626e-04\n",
      "Epoch 25/250\n",
      "446278279/446278279 [==============================] - 1201s 3us/sample - loss: 4.0941e-04 - mean_squared_error: 4.0941e-04 - val_loss: 3.6281e-04 - val_mean_squared_error: 3.6281e-04\n",
      "Epoch 26/250\n",
      "446278279/446278279 [==============================] - 1176s 3us/sample - loss: 3.8661e-04 - mean_squared_error: 3.8661e-04 - val_loss: 3.3348e-04 - val_mean_squared_error: 3.3348e-04\n",
      "Epoch 27/250\n",
      "446278279/446278279 [==============================] - 1181s 3us/sample - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 7.0573e-04 - val_mean_squared_error: 7.0573e-04\n",
      "Epoch 28/250\n",
      "446278279/446278279 [==============================] - 1200s 3us/sample - loss: 5.6686e-04 - mean_squared_error: 5.6686e-04 - val_loss: 5.0465e-04 - val_mean_squared_error: 5.0465e-04\n",
      "Epoch 29/250\n",
      "446278279/446278279 [==============================] - 1224s 3us/sample - loss: 4.7197e-04 - mean_squared_error: 4.7197e-04 - val_loss: 4.1059e-04 - val_mean_squared_error: 4.1059e-04\n",
      "Epoch 30/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 4.4206e-04 - mean_squared_error: 4.4206e-04\n",
      "Epoch 00030: val_loss did not improve from 0.00052\n",
      "446278279/446278279 [==============================] - 1308s 3us/sample - loss: 4.4217e-04 - mean_squared_error: 4.4217e-04 - val_loss: 6.4110e-04 - val_mean_squared_error: 6.4110e-04\n",
      "Epoch 31/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 4.2539e-04 - mean_squared_error: 4.2539e-04\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "446278279/446278279 [==============================] - 1335s 3us/sample - loss: 4.2539e-04 - mean_squared_error: 4.2539e-04 - val_loss: 4.1831e-04 - val_mean_squared_error: 4.1831e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/250\n",
      "446278279/446278279 [==============================] - 1330s 3us/sample - loss: 2.9696e-04 - mean_squared_error: 2.9696e-04 - val_loss: 2.9511e-04 - val_mean_squared_error: 2.9511e-04\n",
      "Epoch 33/250\n",
      "446278279/446278279 [==============================] - 1354s 3us/sample - loss: 2.9401e-04 - mean_squared_error: 2.9401e-04 - val_loss: 2.9292e-04 - val_mean_squared_error: 2.9292e-04\n",
      "Epoch 34/250\n",
      "446278279/446278279 [==============================] - 1392s 3us/sample - loss: 2.9176e-04 - mean_squared_error: 2.9176e-04 - val_loss: 2.9062e-04 - val_mean_squared_error: 2.9062e-04\n",
      "Epoch 35/250\n",
      "446278279/446278279 [==============================] - 1410s 3us/sample - loss: 2.8938e-04 - mean_squared_error: 2.8938e-04 - val_loss: 2.8811e-04 - val_mean_squared_error: 2.8811e-04\n",
      "Epoch 36/250\n",
      "446278279/446278279 [==============================] - 1428s 3us/sample - loss: 2.8673e-04 - mean_squared_error: 2.8673e-04 - val_loss: 2.8531e-04 - val_mean_squared_error: 2.8531e-04\n",
      "Epoch 37/250\n",
      "446278279/446278279 [==============================] - 1053s 2us/sample - loss: 2.8382e-04 - mean_squared_error: 2.8382e-04 - val_loss: 2.8238e-04 - val_mean_squared_error: 2.8238e-04\n",
      "Epoch 38/250\n",
      "446278279/446278279 [==============================] - 1060s 2us/sample - loss: 2.8071e-04 - mean_squared_error: 2.8071e-04 - val_loss: 2.7928e-04 - val_mean_squared_error: 2.7928e-04\n",
      "Epoch 39/250\n",
      "446278279/446278279 [==============================] - 1083s 2us/sample - loss: 2.7736e-04 - mean_squared_error: 2.7736e-04 - val_loss: 2.7556e-04 - val_mean_squared_error: 2.7556e-04\n",
      "Epoch 40/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 2.7383e-04 - mean_squared_error: 2.7383e-04\n",
      "Epoch 00040: val_loss improved from 0.00052 to 0.00027, saving model to /media/data_cifs/afengler/data/kde/full_ddm/keras_models//dnnregressor_full_ddm_07_02_19_18_47_36/ckpt_0_40\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f2ad0236080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "446278279/446278279 [==============================] - 1102s 2us/sample - loss: 2.7383e-04 - mean_squared_error: 2.7383e-04 - val_loss: 2.7205e-04 - val_mean_squared_error: 2.7205e-04\n",
      "Epoch 41/250\n",
      "446278279/446278279 [==============================] - 1131s 3us/sample - loss: 2.7017e-04 - mean_squared_error: 2.7017e-04 - val_loss: 2.6892e-04 - val_mean_squared_error: 2.6892e-04\n",
      "Epoch 42/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 2.6711e-04 - mean_squared_error: 2.6711e-04\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "446278279/446278279 [==============================] - 1158s 3us/sample - loss: 2.6711e-04 - mean_squared_error: 2.6711e-04 - val_loss: 2.6696e-04 - val_mean_squared_error: 2.6696e-04\n",
      "Epoch 43/250\n",
      "446278279/446278279 [==============================] - 1176s 3us/sample - loss: 2.6429e-04 - mean_squared_error: 2.6429e-04 - val_loss: 2.6404e-04 - val_mean_squared_error: 2.6404e-04\n",
      "Epoch 44/250\n",
      "446278279/446278279 [==============================] - 1188s 3us/sample - loss: 2.6387e-04 - mean_squared_error: 2.6387e-04 - val_loss: 2.6358e-04 - val_mean_squared_error: 2.6358e-04\n",
      "Epoch 45/250\n",
      "446278279/446278279 [==============================] - 1207s 3us/sample - loss: 2.6339e-04 - mean_squared_error: 2.6339e-04 - val_loss: 2.6306e-04 - val_mean_squared_error: 2.6306e-04\n",
      "Epoch 46/250\n",
      "446278279/446278279 [==============================] - 1231s 3us/sample - loss: 2.6283e-04 - mean_squared_error: 2.6283e-04 - val_loss: 2.6251e-04 - val_mean_squared_error: 2.6251e-04\n",
      "Epoch 47/250\n",
      "446278279/446278279 [==============================] - 1255s 3us/sample - loss: 2.6220e-04 - mean_squared_error: 2.6220e-04 - val_loss: 2.6184e-04 - val_mean_squared_error: 2.6184e-04\n",
      "Epoch 48/250\n",
      "446278279/446278279 [==============================] - 1262s 3us/sample - loss: 2.6150e-04 - mean_squared_error: 2.6150e-04 - val_loss: 2.6106e-04 - val_mean_squared_error: 2.6106e-04\n",
      "Epoch 49/250\n",
      "446278279/446278279 [==============================] - 1310s 3us/sample - loss: 2.6073e-04 - mean_squared_error: 2.6073e-04 - val_loss: 2.6031e-04 - val_mean_squared_error: 2.6031e-04\n",
      "Epoch 50/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 2.5990e-04 - mean_squared_error: 2.5990e-04\n",
      "Epoch 00050: val_loss improved from 0.00027 to 0.00026, saving model to /media/data_cifs/afengler/data/kde/full_ddm/keras_models//dnnregressor_full_ddm_07_02_19_18_47_36/ckpt_0_50\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f2ad0236080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "446278279/446278279 [==============================] - 1373s 3us/sample - loss: 2.5990e-04 - mean_squared_error: 2.5990e-04 - val_loss: 2.5944e-04 - val_mean_squared_error: 2.5944e-04\n",
      "Epoch 51/250\n",
      "446278279/446278279 [==============================] - 1034s 2us/sample - loss: 2.5902e-04 - mean_squared_error: 2.5902e-04 - val_loss: 2.5854e-04 - val_mean_squared_error: 2.5854e-04\n",
      "Epoch 52/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 2.5811e-04 - mean_squared_error: 2.5811e-04\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "446278279/446278279 [==============================] - 1070s 2us/sample - loss: 2.5811e-04 - mean_squared_error: 2.5811e-04 - val_loss: 2.5766e-04 - val_mean_squared_error: 2.5766e-04\n",
      "Epoch 53/250\n",
      "446278279/446278279 [==============================] - 1118s 3us/sample - loss: 2.5757e-04 - mean_squared_error: 2.5757e-04 - val_loss: 2.5747e-04 - val_mean_squared_error: 2.5747e-04\n",
      "Epoch 54/250\n",
      "446278279/446278279 [==============================] - 1103s 2us/sample - loss: 2.5746e-04 - mean_squared_error: 2.5746e-04 - val_loss: 2.5735e-04 - val_mean_squared_error: 2.5735e-04\n",
      "Epoch 55/250\n",
      "446278279/446278279 [==============================] - 1112s 2us/sample - loss: 2.5735e-04 - mean_squared_error: 2.5735e-04 - val_loss: 2.5724e-04 - val_mean_squared_error: 2.5724e-04\n",
      "Epoch 56/250\n",
      "446278279/446278279 [==============================] - 1153s 3us/sample - loss: 2.5723e-04 - mean_squared_error: 2.5723e-04 - val_loss: 2.5711e-04 - val_mean_squared_error: 2.5711e-04\n",
      "Epoch 57/250\n",
      "446278279/446278279 [==============================] - 1175s 3us/sample - loss: 2.5710e-04 - mean_squared_error: 2.5710e-04 - val_loss: 2.5698e-04 - val_mean_squared_error: 2.5698e-04\n",
      "Epoch 58/250\n",
      "446278279/446278279 [==============================] - 1181s 3us/sample - loss: 2.5697e-04 - mean_squared_error: 2.5697e-04 - val_loss: 2.5684e-04 - val_mean_squared_error: 2.5684e-04\n",
      "Epoch 59/250\n",
      "446278279/446278279 [==============================] - 1213s 3us/sample - loss: 2.5683e-04 - mean_squared_error: 2.5683e-04 - val_loss: 2.5670e-04 - val_mean_squared_error: 2.5670e-04\n",
      "Epoch 60/250\n",
      "446000000/446278279 [============================>.] - ETA: 0s - loss: 2.5668e-04 - mean_squared_error: 2.5668e-04\n",
      "Epoch 00060: val_loss improved from 0.00026 to 0.00026, saving model to /media/data_cifs/afengler/data/kde/full_ddm/keras_models//dnnregressor_full_ddm_07_02_19_18_47_36/ckpt_0_60\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f2ad0236080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "446278279/446278279 [==============================] - 1227s 3us/sample - loss: 2.5668e-04 - mean_squared_error: 2.5668e-04 - val_loss: 2.5655e-04 - val_mean_squared_error: 2.5655e-04\n",
      "Epoch 00060: early stopping\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f2ad0236080>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003249</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  mean_squared_error  val_loss  val_mean_squared_error        lr\n",
       "0   0.018647            0.018647  0.004492                0.004492  0.002000\n",
       "1   0.003249            0.003249  0.004684                0.004684  0.002000\n",
       "2   0.002198            0.002198  0.001775                0.001775  0.002000\n",
       "3   0.001688            0.001688  0.001304                0.001304  0.002000\n",
       "4   0.001450            0.001450  0.001174                0.001174  0.002000\n",
       "5   0.001216            0.001216  0.001538                0.001538  0.002000\n",
       "6   0.001109            0.001109  0.000672                0.000672  0.002000\n",
       "7   0.000989            0.000989  0.000887                0.000887  0.002000\n",
       "8   0.000918            0.000918  0.001224                0.001224  0.002000\n",
       "9   0.000828            0.000828  0.000729                0.000729  0.002000\n",
       "10  0.000771            0.000771  0.000535                0.000535  0.002000\n",
       "11  0.000697            0.000697  0.000656                0.000656  0.002000\n",
       "12  0.000663            0.000663  0.000590                0.000590  0.002000\n",
       "13  0.000629            0.000629  0.000915                0.000915  0.002000\n",
       "14  0.000608            0.000608  0.000432                0.000432  0.002000\n",
       "15  0.000574            0.000574  0.000542                0.000542  0.002000\n",
       "16  0.000530            0.000530  0.000513                0.000513  0.002000\n",
       "17  0.000513            0.000513  0.000381                0.000381  0.002000\n",
       "18  0.000494            0.000494  0.000807                0.000807  0.002000\n",
       "19  0.000478            0.000478  0.000518                0.000518  0.002000\n",
       "20  0.000455            0.000455  0.000481                0.000481  0.002000\n",
       "21  0.000444            0.000444  0.000553                0.000553  0.002000\n",
       "22  0.000429            0.000429  0.000330                0.000330  0.002000\n",
       "23  0.000406            0.000406  0.000336                0.000336  0.002000\n",
       "24  0.000409            0.000409  0.000363                0.000363  0.002000\n",
       "25  0.000387            0.000387  0.000333                0.000333  0.002000\n",
       "26  0.003693            0.003693  0.000706                0.000706  0.002000\n",
       "27  0.000567            0.000567  0.000505                0.000505  0.002000\n",
       "28  0.000472            0.000472  0.000411                0.000411  0.002000\n",
       "29  0.000442            0.000442  0.000641                0.000641  0.002000\n",
       "30  0.000425            0.000425  0.000418                0.000418  0.002000\n",
       "31  0.000297            0.000297  0.000295                0.000295  0.000200\n",
       "32  0.000294            0.000294  0.000293                0.000293  0.000200\n",
       "33  0.000292            0.000292  0.000291                0.000291  0.000200\n",
       "34  0.000289            0.000289  0.000288                0.000288  0.000200\n",
       "35  0.000287            0.000287  0.000285                0.000285  0.000200\n",
       "36  0.000284            0.000284  0.000282                0.000282  0.000200\n",
       "37  0.000281            0.000281  0.000279                0.000279  0.000200\n",
       "38  0.000277            0.000277  0.000276                0.000276  0.000200\n",
       "39  0.000274            0.000274  0.000272                0.000272  0.000200\n",
       "40  0.000270            0.000270  0.000269                0.000269  0.000200\n",
       "41  0.000267            0.000267  0.000267                0.000267  0.000200\n",
       "42  0.000264            0.000264  0.000264                0.000264  0.000020\n",
       "43  0.000264            0.000264  0.000264                0.000264  0.000020\n",
       "44  0.000263            0.000263  0.000263                0.000263  0.000020\n",
       "45  0.000263            0.000263  0.000263                0.000263  0.000020\n",
       "46  0.000262            0.000262  0.000262                0.000262  0.000020\n",
       "47  0.000261            0.000261  0.000261                0.000261  0.000020\n",
       "48  0.000261            0.000261  0.000260                0.000260  0.000020\n",
       "49  0.000260            0.000260  0.000259                0.000259  0.000020\n",
       "50  0.000259            0.000259  0.000259                0.000259  0.000020\n",
       "51  0.000258            0.000258  0.000258                0.000258  0.000020\n",
       "52  0.000258            0.000258  0.000257                0.000257  0.000002\n",
       "53  0.000257            0.000257  0.000257                0.000257  0.000002\n",
       "54  0.000257            0.000257  0.000257                0.000257  0.000002\n",
       "55  0.000257            0.000257  0.000257                0.000257  0.000002\n",
       "56  0.000257            0.000257  0.000257                0.000257  0.000002\n",
       "57  0.000257            0.000257  0.000257                0.000257  0.000002\n",
       "58  0.000257            0.000257  0.000257                0.000257  0.000002\n",
       "59  0.000257            0.000257  0.000257                0.000257  0.000002"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "cpm.run_training(save_history = True, \n",
    "                 warm_start = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
