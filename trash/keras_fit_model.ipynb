{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "import make_data_wfpt as mdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datafile used to read in training data: data_storage/train_data_400000_from_simulation_mix_v_len_2_n_2_09_09_18_20_13_21\n",
      "datafile used to read in test data: data_storage/test_data_400000_from_simulation_mix_v_len_2_n_2_09_09_18_20_13_21\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# Choice probabilities\n",
    "# train_f, train_l, test_f, test_l = mdw.train_test_from_file_choice_probabilities(n_samples = 2500000,\n",
    "#                                                             f_signature = '_choice_probabilities_analytic_',\n",
    "#                                                                                 backend = 'keras')\n",
    "\n",
    "# rt_choice\n",
    "train_f, train_l, test_f, test_l = mdw.train_test_from_file_rt_choice(n_samples = 400000,\n",
    "                                                                      f_signature = '_from_simulation_mix_v_len_2_',\n",
    "                                                                      backend = 'keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer()\n",
    "cpm.data['train_features'] = train_f\n",
    "cpm.data['train_labels'] = train_l\n",
    "cpm.data['test_features'] = test_f\n",
    "cpm.data['test_labels'] = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_shape': 3,\n",
       " 'output_shape': 1,\n",
       " 'output_activation': 'sigmoid',\n",
       " 'hidden_layers': [20, 20, 20, 20],\n",
       " 'hidden_activations': ['relu', 'relu', 'relu', 'relu'],\n",
       " 'l1_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l1_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'optimizer': 'Nadam',\n",
       " 'loss': 'mse',\n",
       " 'metrics': ['mse']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'callback_funs': ['ReduceLROnPlateau', 'EarlyStopping', 'ModelCheckpoint'],\n",
       " 'plateau_patience': 10,\n",
       " 'min_delta': 0.0001,\n",
       " 'early_stopping_patience': 15,\n",
       " 'callback_monitor': 'loss',\n",
       " 'min_learning_rate': 1e-07,\n",
       " 'red_coef_learning_rate': 0.1,\n",
       " 'ckpt_period': 10,\n",
       " 'ckpt_save_best_only': True,\n",
       " 'ckpt_save_weights_only': True,\n",
       " 'max_train_epochs': 2000,\n",
       " 'batch_size': 10000,\n",
       " 'warm_start': False,\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_cnt': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'choice_probabilities',\n",
       " 'model_directory': '/home/afengler/git_repos/nn_likelihoods/keras_models',\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_name': 'dnnregressor',\n",
       " 'data_type_signature': '_choice_probabilities_analytic_',\n",
       " 'timestamp': '09_09_18_21_48_44',\n",
       " 'training_data_size': 2500000}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters concerning data storage\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, specify new set of parameters here:\n",
    "# Model params\n",
    "cpm.model_params['output_activation'] = 'linear'\n",
    "cpm.model_params['input_shape'] = 5\n",
    "\n",
    "# Data params\n",
    "cpm.data_params['data_type'] = 'wfpt'\n",
    "cpm.data_params['data_type_signature'] = '_choice_rt_mix_v_len_2_'\n",
    "cpm.data_params['training_data_size'] = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320201 samples, validate on 79799 samples\n",
      "Epoch 1/2000\n",
      "320201/320201 [==============================] - 10s 30us/step - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 2/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 2.9472e-04 - val_mean_squared_error: 2.9472e-04\n",
      "Epoch 3/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 5.7668e-04 - mean_squared_error: 5.7668e-04 - val_loss: 2.5075e-04 - val_mean_squared_error: 2.5075e-04\n",
      "Epoch 4/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 3.3919e-04 - mean_squared_error: 3.3919e-04 - val_loss: 8.1298e-04 - val_mean_squared_error: 8.1298e-04\n",
      "Epoch 5/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 2.6224e-04 - mean_squared_error: 2.6224e-04 - val_loss: 1.7538e-04 - val_mean_squared_error: 1.7538e-04\n",
      "Epoch 6/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 2.1872e-04 - mean_squared_error: 2.1872e-04 - val_loss: 1.2407e-04 - val_mean_squared_error: 1.2407e-04\n",
      "Epoch 7/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 1.7770e-04 - mean_squared_error: 1.7770e-04 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 8/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 1.6307e-04 - mean_squared_error: 1.6307e-04 - val_loss: 4.3643e-05 - val_mean_squared_error: 4.3643e-05\n",
      "Epoch 9/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 1.3066e-04 - mean_squared_error: 1.3066e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 10/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 1.2403e-04 - mean_squared_error: 1.2403e-04 - val_loss: 1.8823e-05 - val_mean_squared_error: 1.8823e-05\n",
      "Epoch 11/2000\n",
      "320201/320201 [==============================] - 9s 29us/step - loss: 1.1292e-04 - mean_squared_error: 1.1292e-04 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 12/2000\n",
      "320201/320201 [==============================] - 10s 30us/step - loss: 9.8382e-05 - mean_squared_error: 9.8382e-05 - val_loss: 4.6196e-05 - val_mean_squared_error: 4.6196e-05\n",
      "Epoch 13/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 1.1075e-04 - mean_squared_error: 1.1075e-04 - val_loss: 2.6345e-05 - val_mean_squared_error: 2.6345e-05\n",
      "Epoch 14/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 9.0075e-05 - mean_squared_error: 9.0075e-05 - val_loss: 3.6446e-05 - val_mean_squared_error: 3.6446e-05\n",
      "Epoch 15/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 8.8624e-05 - mean_squared_error: 8.8624e-05 - val_loss: 2.2790e-05 - val_mean_squared_error: 2.2790e-05\n",
      "Epoch 16/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 8.1144e-05 - mean_squared_error: 8.1144e-05 - val_loss: 2.9186e-05 - val_mean_squared_error: 2.9186e-05\n",
      "Epoch 17/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 7.7318e-05 - mean_squared_error: 7.7318e-05 - val_loss: 9.5949e-05 - val_mean_squared_error: 9.5949e-05\n",
      "Epoch 18/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 7.0615e-05 - mean_squared_error: 7.0615e-05 - val_loss: 7.2453e-05 - val_mean_squared_error: 7.2453e-05\n",
      "Epoch 19/2000\n",
      "320201/320201 [==============================] - 11s 33us/step - loss: 6.6802e-05 - mean_squared_error: 6.6802e-05 - val_loss: 6.0486e-05 - val_mean_squared_error: 6.0486e-05\n",
      "Epoch 20/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 6.4500e-05 - mean_squared_error: 6.4500e-05 - val_loss: 9.8721e-05 - val_mean_squared_error: 9.8721e-05\n",
      "Epoch 21/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 6.2266e-05 - mean_squared_error: 6.2266e-05 - val_loss: 1.0338e-05 - val_mean_squared_error: 1.0338e-05\n",
      "Epoch 22/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 4.3807e-06 - mean_squared_error: 4.3807e-06 - val_loss: 4.2663e-06 - val_mean_squared_error: 4.2663e-06\n",
      "Epoch 23/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 4.1200e-06 - mean_squared_error: 4.1200e-06 - val_loss: 4.7826e-06 - val_mean_squared_error: 4.7826e-06\n",
      "Epoch 24/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.7964e-06 - mean_squared_error: 3.7964e-06 - val_loss: 4.2750e-06 - val_mean_squared_error: 4.2750e-06\n",
      "Epoch 25/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.6018e-06 - mean_squared_error: 3.6018e-06 - val_loss: 4.5455e-06 - val_mean_squared_error: 4.5455e-06\n",
      "Epoch 26/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.4667e-06 - mean_squared_error: 3.4667e-06 - val_loss: 2.9608e-06 - val_mean_squared_error: 2.9608e-06\n",
      "Epoch 27/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.3500e-06 - mean_squared_error: 3.3500e-06 - val_loss: 6.1220e-06 - val_mean_squared_error: 6.1220e-06\n",
      "Epoch 28/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.2651e-06 - mean_squared_error: 3.2651e-06 - val_loss: 4.8963e-06 - val_mean_squared_error: 4.8963e-06\n",
      "Epoch 29/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.2178e-06 - mean_squared_error: 3.2178e-06 - val_loss: 5.3255e-06 - val_mean_squared_error: 5.3255e-06\n",
      "Epoch 30/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.1957e-06 - mean_squared_error: 3.1957e-06 - val_loss: 3.8081e-06 - val_mean_squared_error: 3.8081e-06\n",
      "Epoch 31/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.1287e-06 - mean_squared_error: 3.1287e-06 - val_loss: 2.6725e-06 - val_mean_squared_error: 2.6725e-06\n",
      "Epoch 32/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 3.0223e-06 - mean_squared_error: 3.0223e-06 - val_loss: 2.5651e-06 - val_mean_squared_error: 2.5651e-06\n",
      "Epoch 33/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 1.9173e-06 - mean_squared_error: 1.9173e-06 - val_loss: 1.8157e-06 - val_mean_squared_error: 1.8157e-06\n",
      "Epoch 34/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 1.9124e-06 - mean_squared_error: 1.9124e-06 - val_loss: 1.8937e-06 - val_mean_squared_error: 1.8937e-06\n",
      "Epoch 35/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 1.9044e-06 - mean_squared_error: 1.9044e-06 - val_loss: 2.0718e-06 - val_mean_squared_error: 2.0718e-06\n",
      "Epoch 36/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 1.8976e-06 - mean_squared_error: 1.8976e-06 - val_loss: 1.8322e-06 - val_mean_squared_error: 1.8322e-06\n",
      "Epoch 37/2000\n",
      "320201/320201 [==============================] - 10s 32us/step - loss: 1.8899e-06 - mean_squared_error: 1.8899e-06 - val_loss: 2.0043e-06 - val_mean_squared_error: 2.0043e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.023299</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.00200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  val_mean_squared_error      loss  mean_squared_error       lr\n",
       "0   0.001016                0.001016  0.023299            0.023299  0.00200\n",
       "1   0.000295                0.000295  0.001022            0.001022  0.00200\n",
       "2   0.000251                0.000251  0.000577            0.000577  0.00200\n",
       "3   0.000813                0.000813  0.000339            0.000339  0.00200\n",
       "4   0.000175                0.000175  0.000262            0.000262  0.00200\n",
       "5   0.000124                0.000124  0.000219            0.000219  0.00200\n",
       "6   0.013904                0.013904  0.000178            0.000178  0.00200\n",
       "7   0.000044                0.000044  0.000163            0.000163  0.00200\n",
       "8   0.001494                0.001494  0.000131            0.000131  0.00200\n",
       "9   0.000019                0.000019  0.000124            0.000124  0.00200\n",
       "10  0.001465                0.001465  0.000113            0.000113  0.00200\n",
       "11  0.000046                0.000046  0.000098            0.000098  0.00200\n",
       "12  0.000026                0.000026  0.000111            0.000111  0.00200\n",
       "13  0.000036                0.000036  0.000090            0.000090  0.00200\n",
       "14  0.000023                0.000023  0.000089            0.000089  0.00200\n",
       "15  0.000029                0.000029  0.000081            0.000081  0.00200\n",
       "16  0.000096                0.000096  0.000077            0.000077  0.00200\n",
       "17  0.000072                0.000072  0.000071            0.000071  0.00200\n",
       "18  0.000060                0.000060  0.000067            0.000067  0.00200\n",
       "19  0.000099                0.000099  0.000064            0.000064  0.00200\n",
       "20  0.000010                0.000010  0.000062            0.000062  0.00200\n",
       "21  0.000004                0.000004  0.000004            0.000004  0.00020\n",
       "22  0.000005                0.000005  0.000004            0.000004  0.00020\n",
       "23  0.000004                0.000004  0.000004            0.000004  0.00020\n",
       "24  0.000005                0.000005  0.000004            0.000004  0.00020\n",
       "25  0.000003                0.000003  0.000003            0.000003  0.00020\n",
       "26  0.000006                0.000006  0.000003            0.000003  0.00020\n",
       "27  0.000005                0.000005  0.000003            0.000003  0.00020\n",
       "28  0.000005                0.000005  0.000003            0.000003  0.00020\n",
       "29  0.000004                0.000004  0.000003            0.000003  0.00020\n",
       "30  0.000003                0.000003  0.000003            0.000003  0.00020\n",
       "31  0.000003                0.000003  0.000003            0.000003  0.00020\n",
       "32  0.000002                0.000002  0.000002            0.000002  0.00002\n",
       "33  0.000002                0.000002  0.000002            0.000002  0.00002\n",
       "34  0.000002                0.000002  0.000002            0.000002  0.00002\n",
       "35  0.000002                0.000002  0.000002            0.000002  0.00002\n",
       "36  0.000002                0.000002  0.000002            0.000002  0.00002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "cpm.run_training(save_history = True, \n",
    "                 warm_start = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
