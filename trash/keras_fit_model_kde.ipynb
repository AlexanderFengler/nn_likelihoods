{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "import time\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Load my own functions\n",
    "import dnn_train_eval as dnnk\n",
    "import kde_training_utilities as kde_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2921097248114361109\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16322291133431236558\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12048649421\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11692343452256698950\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16107915650273343691\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Handle some cuda business\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data_folder = '/media/data_cifs/afengler/data/kde/ddm/train_test_data/'\n",
    "config_file_path = '/media/data_cifs/afengler/git_repos/nn_likelihoods/dnn_train_eval.yaml'\n",
    "\n",
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer(yaml_config_file_path = config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train test split\n",
    "cpm.data['train_features'], cpm.data['train_labels'], cpm.data['test_features'], cpm.data['test_labels'] = kde_utils.kde_load_data(folder = data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters concerning structure and naming of data\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Make model\n",
    "cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 198810436 samples, validate on 49703414 samples\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/250\n",
      "198810436/198810436 [==============================] - 313s 2us/sample - loss: 0.0744 - mean_squared_error: 0.0744 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "Epoch 2/250\n",
      "198810436/198810436 [==============================] - 322s 2us/sample - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 3/250\n",
      "198810436/198810436 [==============================] - 307s 2us/sample - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 4/250\n",
      "198810436/198810436 [==============================] - 313s 2us/sample - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 5/250\n",
      "198810436/198810436 [==============================] - 308s 2us/sample - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 6/250\n",
      "198810436/198810436 [==============================] - 321s 2us/sample - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 7/250\n",
      "198810436/198810436 [==============================] - 313s 2us/sample - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 8/250\n",
      "198810436/198810436 [==============================] - 318s 2us/sample - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 9/250\n",
      "198810436/198810436 [==============================] - 314s 2us/sample - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 10/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 0.0031 - mean_squared_error: 0.0031\n",
      "Epoch 00010: val_loss improved from inf to 0.00262, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_10\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "198810436/198810436 [==============================] - 319s 2us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 11/250\n",
      "198810436/198810436 [==============================] - 321s 2us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 12/250\n",
      "198810436/198810436 [==============================] - 291s 1us/sample - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 13/250\n",
      "198810436/198810436 [==============================] - 286s 1us/sample - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 14/250\n",
      "198810436/198810436 [==============================] - 295s 1us/sample - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 15/250\n",
      "198810436/198810436 [==============================] - 318s 2us/sample - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 16/250\n",
      "198810436/198810436 [==============================] - 314s 2us/sample - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 17/250\n",
      "198810436/198810436 [==============================] - 317s 2us/sample - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 18/250\n",
      "198810436/198810436 [==============================] - 320s 2us/sample - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 19/250\n",
      "198810436/198810436 [==============================] - 308s 2us/sample - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 20/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 0.0018 - mean_squared_error: 0.0018\n",
      "Epoch 00020: val_loss improved from 0.00262 to 0.00122, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_20\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "198810436/198810436 [==============================] - 299s 2us/sample - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 21/250\n",
      "198810436/198810436 [==============================] - 310s 2us/sample - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 22/250\n",
      "198810436/198810436 [==============================] - 303s 2us/sample - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 23/250\n",
      "198810436/198810436 [==============================] - 306s 2us/sample - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 24/250\n",
      "198810436/198810436 [==============================] - 441s 2us/sample - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 25/250\n",
      "198810436/198810436 [==============================] - 361s 2us/sample - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 26/250\n",
      "198810436/198810436 [==============================] - 429s 2us/sample - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 27/250\n",
      "198810436/198810436 [==============================] - 422s 2us/sample - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 28/250\n",
      "198810436/198810436 [==============================] - 429s 2us/sample - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 29/250\n",
      "198810436/198810436 [==============================] - 419s 2us/sample - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 30/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 0.0012 - mean_squared_error: 0.0012\n",
      "Epoch 00030: val_loss improved from 0.00122 to 0.00112, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_30\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "198810436/198810436 [==============================] - 439s 2us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 31/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198810436/198810436 [==============================] - 473s 2us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 32/250\n",
      "198810436/198810436 [==============================] - 496s 2us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 33/250\n",
      "198810436/198810436 [==============================] - 457s 2us/sample - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 9.4979e-04 - val_mean_squared_error: 9.4979e-04\n",
      "Epoch 34/250\n",
      "198810436/198810436 [==============================] - 480s 2us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 8.7755e-04 - val_mean_squared_error: 8.7755e-04\n",
      "Epoch 35/250\n",
      "198810436/198810436 [==============================] - 402s 2us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 36/250\n",
      "198810436/198810436 [==============================] - 466s 2us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 37/250\n",
      "198810436/198810436 [==============================] - 304s 2us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 9.0830e-04 - val_mean_squared_error: 9.0830e-04\n",
      "Epoch 38/250\n",
      "198810436/198810436 [==============================] - 441s 2us/sample - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 39/250\n",
      "198810436/198810436 [==============================] - 471s 2us/sample - loss: 9.4116e-04 - mean_squared_error: 9.4116e-04 - val_loss: 7.7953e-04 - val_mean_squared_error: 7.7953e-04\n",
      "Epoch 40/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 0.0010 - mean_squared_error: 0.0010\n",
      "Epoch 00040: val_loss did not improve from 0.00112\n",
      "198810436/198810436 [==============================] - 434s 2us/sample - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 41/250\n",
      "198810436/198810436 [==============================] - 425s 2us/sample - loss: 9.6127e-04 - mean_squared_error: 9.6127e-04 - val_loss: 8.0570e-04 - val_mean_squared_error: 8.0570e-04\n",
      "Epoch 42/250\n",
      "198810436/198810436 [==============================] - 427s 2us/sample - loss: 9.2862e-04 - mean_squared_error: 9.2862e-04 - val_loss: 8.0830e-04 - val_mean_squared_error: 8.0830e-04\n",
      "Epoch 43/250\n",
      "198810436/198810436 [==============================] - 446s 2us/sample - loss: 9.3404e-04 - mean_squared_error: 9.3404e-04 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 44/250\n",
      "198810436/198810436 [==============================] - 428s 2us/sample - loss: 8.5915e-04 - mean_squared_error: 8.5915e-04 - val_loss: 8.5356e-04 - val_mean_squared_error: 8.5356e-04\n",
      "Epoch 45/250\n",
      "198810436/198810436 [==============================] - 458s 2us/sample - loss: 8.7933e-04 - mean_squared_error: 8.7933e-04 - val_loss: 8.8880e-04 - val_mean_squared_error: 8.8880e-04\n",
      "Epoch 46/250\n",
      "198810436/198810436 [==============================] - 435s 2us/sample - loss: 9.3306e-04 - mean_squared_error: 9.3306e-04 - val_loss: 8.4596e-04 - val_mean_squared_error: 8.4596e-04\n",
      "Epoch 47/250\n",
      "198810436/198810436 [==============================] - 427s 2us/sample - loss: 8.4157e-04 - mean_squared_error: 8.4157e-04 - val_loss: 7.6609e-04 - val_mean_squared_error: 7.6609e-04\n",
      "Epoch 48/250\n",
      "198810436/198810436 [==============================] - 408s 2us/sample - loss: 8.4583e-04 - mean_squared_error: 8.4583e-04 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 49/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 8.5444e-04 - mean_squared_error: 8.5444e-04\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "198810436/198810436 [==============================] - 444s 2us/sample - loss: 8.5411e-04 - mean_squared_error: 8.5411e-04 - val_loss: 7.3408e-04 - val_mean_squared_error: 7.3408e-04\n",
      "Epoch 50/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 4.5456e-04 - mean_squared_error: 4.5456e-04\n",
      "Epoch 00050: val_loss improved from 0.00112 to 0.00045, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_50\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "198810436/198810436 [==============================] - 481s 2us/sample - loss: 4.5456e-04 - mean_squared_error: 4.5456e-04 - val_loss: 4.4968e-04 - val_mean_squared_error: 4.4968e-04\n",
      "Epoch 51/250\n",
      "198810436/198810436 [==============================] - 445s 2us/sample - loss: 4.4894e-04 - mean_squared_error: 4.4894e-04 - val_loss: 4.4821e-04 - val_mean_squared_error: 4.4821e-04\n",
      "Epoch 52/250\n",
      "198810436/198810436 [==============================] - 479s 2us/sample - loss: 4.4765e-04 - mean_squared_error: 4.4765e-04 - val_loss: 4.4705e-04 - val_mean_squared_error: 4.4705e-04\n",
      "Epoch 53/250\n",
      "198810436/198810436 [==============================] - 486s 2us/sample - loss: 4.4658e-04 - mean_squared_error: 4.4658e-04 - val_loss: 4.4605e-04 - val_mean_squared_error: 4.4605e-04\n",
      "Epoch 54/250\n",
      "198810436/198810436 [==============================] - 445s 2us/sample - loss: 4.4562e-04 - mean_squared_error: 4.4562e-04 - val_loss: 4.4512e-04 - val_mean_squared_error: 4.4512e-04\n",
      "Epoch 55/250\n",
      "198810436/198810436 [==============================] - 475s 2us/sample - loss: 4.4471e-04 - mean_squared_error: 4.4471e-04 - val_loss: 4.4420e-04 - val_mean_squared_error: 4.4420e-04\n",
      "Epoch 56/250\n",
      "198810436/198810436 [==============================] - 481s 2us/sample - loss: 4.4381e-04 - mean_squared_error: 4.4381e-04 - val_loss: 4.4334e-04 - val_mean_squared_error: 4.4334e-04\n",
      "Epoch 57/250\n",
      "198810436/198810436 [==============================] - 472s 2us/sample - loss: 4.4292e-04 - mean_squared_error: 4.4292e-04 - val_loss: 4.4242e-04 - val_mean_squared_error: 4.4242e-04\n",
      "Epoch 58/250\n",
      "198810436/198810436 [==============================] - 423s 2us/sample - loss: 4.4201e-04 - mean_squared_error: 4.4201e-04 - val_loss: 4.4150e-04 - val_mean_squared_error: 4.4150e-04\n",
      "Epoch 59/250\n",
      "198810436/198810436 [==============================] - 433s 2us/sample - loss: 4.4109e-04 - mean_squared_error: 4.4109e-04 - val_loss: 4.4058e-04 - val_mean_squared_error: 4.4058e-04\n",
      "Epoch 60/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 4.4016e-04 - mean_squared_error: 4.4016e-04\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00045 to 0.00044, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_60\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "198810436/198810436 [==============================] - 473s 2us/sample - loss: 4.4015e-04 - mean_squared_error: 4.4015e-04 - val_loss: 4.3963e-04 - val_mean_squared_error: 4.3963e-04\n",
      "Epoch 61/250\n",
      "198810436/198810436 [==============================] - 394s 2us/sample - loss: 4.3955e-04 - mean_squared_error: 4.3955e-04 - val_loss: 4.3949e-04 - val_mean_squared_error: 4.3949e-04\n",
      "Epoch 62/250\n",
      "198810436/198810436 [==============================] - 398s 2us/sample - loss: 4.3944e-04 - mean_squared_error: 4.3944e-04 - val_loss: 4.3939e-04 - val_mean_squared_error: 4.3939e-04\n",
      "Epoch 63/250\n",
      "198810436/198810436 [==============================] - 441s 2us/sample - loss: 4.3933e-04 - mean_squared_error: 4.3933e-04 - val_loss: 4.3927e-04 - val_mean_squared_error: 4.3927e-04\n",
      "Epoch 64/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198810436/198810436 [==============================] - 363s 2us/sample - loss: 4.3920e-04 - mean_squared_error: 4.3920e-04 - val_loss: 4.3914e-04 - val_mean_squared_error: 4.3914e-04\n",
      "Epoch 65/250\n",
      "198810436/198810436 [==============================] - 382s 2us/sample - loss: 4.3907e-04 - mean_squared_error: 4.3907e-04 - val_loss: 4.3902e-04 - val_mean_squared_error: 4.3902e-04\n",
      "Epoch 66/250\n",
      "198810436/198810436 [==============================] - 421s 2us/sample - loss: 4.3893e-04 - mean_squared_error: 4.3893e-04 - val_loss: 4.3885e-04 - val_mean_squared_error: 4.3885e-04\n",
      "Epoch 67/250\n",
      "198810436/198810436 [==============================] - 436s 2us/sample - loss: 4.3877e-04 - mean_squared_error: 4.3877e-04 - val_loss: 4.3869e-04 - val_mean_squared_error: 4.3869e-04\n",
      "Epoch 68/250\n",
      "198810436/198810436 [==============================] - 389s 2us/sample - loss: 4.3860e-04 - mean_squared_error: 4.3860e-04 - val_loss: 4.3853e-04 - val_mean_squared_error: 4.3853e-04\n",
      "Epoch 69/250\n",
      "198810436/198810436 [==============================] - 469s 2us/sample - loss: 4.3842e-04 - mean_squared_error: 4.3842e-04 - val_loss: 4.3833e-04 - val_mean_squared_error: 4.3833e-04\n",
      "Epoch 70/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 4.3824e-04 - mean_squared_error: 4.3824e-04\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00044 to 0.00044, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_70\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "198810436/198810436 [==============================] - 364s 2us/sample - loss: 4.3822e-04 - mean_squared_error: 4.3822e-04 - val_loss: 4.3813e-04 - val_mean_squared_error: 4.3813e-04\n",
      "Epoch 71/250\n",
      "198810436/198810436 [==============================] - 490s 2us/sample - loss: 4.3808e-04 - mean_squared_error: 4.3808e-04 - val_loss: 4.3808e-04 - val_mean_squared_error: 4.3808e-04\n",
      "Epoch 72/250\n",
      "198810436/198810436 [==============================] - 424s 2us/sample - loss: 4.3806e-04 - mean_squared_error: 4.3806e-04 - val_loss: 4.3806e-04 - val_mean_squared_error: 4.3806e-04\n",
      "Epoch 73/250\n",
      "198810436/198810436 [==============================] - 391s 2us/sample - loss: 4.3803e-04 - mean_squared_error: 4.3803e-04 - val_loss: 4.3803e-04 - val_mean_squared_error: 4.3803e-04\n",
      "Epoch 74/250\n",
      "198810436/198810436 [==============================] - 454s 2us/sample - loss: 4.3800e-04 - mean_squared_error: 4.3800e-04 - val_loss: 4.3800e-04 - val_mean_squared_error: 4.3800e-04\n",
      "Epoch 75/250\n",
      "198810436/198810436 [==============================] - 471s 2us/sample - loss: 4.3797e-04 - mean_squared_error: 4.3797e-04 - val_loss: 4.3797e-04 - val_mean_squared_error: 4.3797e-04\n",
      "Epoch 76/250\n",
      "198810436/198810436 [==============================] - 467s 2us/sample - loss: 4.3794e-04 - mean_squared_error: 4.3794e-04 - val_loss: 4.3794e-04 - val_mean_squared_error: 4.3794e-04\n",
      "Epoch 77/250\n",
      "198810436/198810436 [==============================] - 404s 2us/sample - loss: 4.3791e-04 - mean_squared_error: 4.3791e-04 - val_loss: 4.3790e-04 - val_mean_squared_error: 4.3790e-04\n",
      "Epoch 78/250\n",
      "198810436/198810436 [==============================] - 403s 2us/sample - loss: 4.3787e-04 - mean_squared_error: 4.3787e-04 - val_loss: 4.3787e-04 - val_mean_squared_error: 4.3787e-04\n",
      "Epoch 79/250\n",
      "198810436/198810436 [==============================] - 448s 2us/sample - loss: 4.3783e-04 - mean_squared_error: 4.3783e-04 - val_loss: 4.3782e-04 - val_mean_squared_error: 4.3782e-04\n",
      "Epoch 80/250\n",
      "198000000/198810436 [============================>.] - ETA: 1s - loss: 4.3778e-04 - mean_squared_error: 4.3778e-04\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 2.000000222324161e-07.\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00044 to 0.00044, saving model to /media/data_cifs/afengler/data/kde/ddm/keras_models/mlp_ddm_kde_08_05_19_16_48_38/ckpt_0_80\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "198810436/198810436 [==============================] - 447s 2us/sample - loss: 4.3778e-04 - mean_squared_error: 4.3778e-04 - val_loss: 4.3777e-04 - val_mean_squared_error: 4.3777e-04\n",
      "Epoch 81/250\n",
      "198810436/198810436 [==============================] - 442s 2us/sample - loss: 4.3775e-04 - mean_squared_error: 4.3775e-04 - val_loss: 4.3777e-04 - val_mean_squared_error: 4.3777e-04\n",
      "Epoch 82/250\n",
      "198810436/198810436 [==============================] - 419s 2us/sample - loss: 4.3775e-04 - mean_squared_error: 4.3775e-04 - val_loss: 4.3776e-04 - val_mean_squared_error: 4.3776e-04\n",
      "Epoch 83/250\n",
      "198810436/198810436 [==============================] - 428s 2us/sample - loss: 4.3774e-04 - mean_squared_error: 4.3774e-04 - val_loss: 4.3775e-04 - val_mean_squared_error: 4.3775e-04\n",
      "Epoch 84/250\n",
      "198810436/198810436 [==============================] - 405s 2us/sample - loss: 4.3773e-04 - mean_squared_error: 4.3773e-04 - val_loss: 4.3775e-04 - val_mean_squared_error: 4.3775e-04\n",
      "Epoch 00084: early stopping\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Nadam object at 0x7f90f4e15320>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.074418</td>\n",
       "      <td>0.074418</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013593</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.008687</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>0.004599</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>2.000000e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>2.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>2.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>2.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>2.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>2.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>2.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.000000e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  mean_squared_error  val_loss  val_mean_squared_error  \\\n",
       "0   0.074418            0.074418  0.020376                0.020376   \n",
       "1   0.013593            0.013593  0.010386                0.010386   \n",
       "2   0.008687            0.008687  0.006502                0.006502   \n",
       "3   0.007172            0.007172  0.006076                0.006076   \n",
       "4   0.005423            0.005423  0.004599                0.004599   \n",
       "5   0.004827            0.004827  0.003730                0.003730   \n",
       "6   0.004314            0.004314  0.004129                0.004129   \n",
       "7   0.003674            0.003674  0.005705                0.005705   \n",
       "8   0.003440            0.003440  0.003060                0.003060   \n",
       "9   0.003100            0.003100  0.002619                0.002619   \n",
       "10  0.002920            0.002920  0.004154                0.004154   \n",
       "11  0.002714            0.002714  0.002216                0.002216   \n",
       "12  0.002357            0.002357  0.002283                0.002283   \n",
       "13  0.002521            0.002521  0.001615                0.001615   \n",
       "14  0.002303            0.002303  0.003444                0.003444   \n",
       "15  0.002165            0.002165  0.001808                0.001808   \n",
       "16  0.001821            0.001821  0.001720                0.001720   \n",
       "17  0.002127            0.002127  0.001526                0.001526   \n",
       "18  0.001846            0.001846  0.001415                0.001415   \n",
       "19  0.001833            0.001833  0.001223                0.001223   \n",
       "20  0.001697            0.001697  0.001823                0.001823   \n",
       "21  0.001625            0.001625  0.002344                0.002344   \n",
       "22  0.001615            0.001615  0.001712                0.001712   \n",
       "23  0.001544            0.001544  0.001149                0.001149   \n",
       "24  0.001476            0.001476  0.001352                0.001352   \n",
       "25  0.001448            0.001448  0.001140                0.001140   \n",
       "26  0.001323            0.001323  0.001250                0.001250   \n",
       "27  0.001309            0.001309  0.001358                0.001358   \n",
       "28  0.001382            0.001382  0.001092                0.001092   \n",
       "29  0.001233            0.001233  0.001124                0.001124   \n",
       "..       ...                 ...       ...                     ...   \n",
       "54  0.000445            0.000445  0.000444                0.000444   \n",
       "55  0.000444            0.000444  0.000443                0.000443   \n",
       "56  0.000443            0.000443  0.000442                0.000442   \n",
       "57  0.000442            0.000442  0.000442                0.000442   \n",
       "58  0.000441            0.000441  0.000441                0.000441   \n",
       "59  0.000440            0.000440  0.000440                0.000440   \n",
       "60  0.000440            0.000440  0.000439                0.000439   \n",
       "61  0.000439            0.000439  0.000439                0.000439   \n",
       "62  0.000439            0.000439  0.000439                0.000439   \n",
       "63  0.000439            0.000439  0.000439                0.000439   \n",
       "64  0.000439            0.000439  0.000439                0.000439   \n",
       "65  0.000439            0.000439  0.000439                0.000439   \n",
       "66  0.000439            0.000439  0.000439                0.000439   \n",
       "67  0.000439            0.000439  0.000439                0.000439   \n",
       "68  0.000438            0.000438  0.000438                0.000438   \n",
       "69  0.000438            0.000438  0.000438                0.000438   \n",
       "70  0.000438            0.000438  0.000438                0.000438   \n",
       "71  0.000438            0.000438  0.000438                0.000438   \n",
       "72  0.000438            0.000438  0.000438                0.000438   \n",
       "73  0.000438            0.000438  0.000438                0.000438   \n",
       "74  0.000438            0.000438  0.000438                0.000438   \n",
       "75  0.000438            0.000438  0.000438                0.000438   \n",
       "76  0.000438            0.000438  0.000438                0.000438   \n",
       "77  0.000438            0.000438  0.000438                0.000438   \n",
       "78  0.000438            0.000438  0.000438                0.000438   \n",
       "79  0.000438            0.000438  0.000438                0.000438   \n",
       "80  0.000438            0.000438  0.000438                0.000438   \n",
       "81  0.000438            0.000438  0.000438                0.000438   \n",
       "82  0.000438            0.000438  0.000438                0.000438   \n",
       "83  0.000438            0.000438  0.000438                0.000438   \n",
       "\n",
       "              lr  \n",
       "0   2.000000e-03  \n",
       "1   2.000000e-03  \n",
       "2   2.000000e-03  \n",
       "3   2.000000e-03  \n",
       "4   2.000000e-03  \n",
       "5   2.000000e-03  \n",
       "6   2.000000e-03  \n",
       "7   2.000000e-03  \n",
       "8   2.000000e-03  \n",
       "9   2.000000e-03  \n",
       "10  2.000000e-03  \n",
       "11  2.000000e-03  \n",
       "12  2.000000e-03  \n",
       "13  2.000000e-03  \n",
       "14  2.000000e-03  \n",
       "15  2.000000e-03  \n",
       "16  2.000000e-03  \n",
       "17  2.000000e-03  \n",
       "18  2.000000e-03  \n",
       "19  2.000000e-03  \n",
       "20  2.000000e-03  \n",
       "21  2.000000e-03  \n",
       "22  2.000000e-03  \n",
       "23  2.000000e-03  \n",
       "24  2.000000e-03  \n",
       "25  2.000000e-03  \n",
       "26  2.000000e-03  \n",
       "27  2.000000e-03  \n",
       "28  2.000000e-03  \n",
       "29  2.000000e-03  \n",
       "..           ...  \n",
       "54  2.000000e-04  \n",
       "55  2.000000e-04  \n",
       "56  2.000000e-04  \n",
       "57  2.000000e-04  \n",
       "58  2.000000e-04  \n",
       "59  2.000000e-04  \n",
       "60  2.000000e-05  \n",
       "61  2.000000e-05  \n",
       "62  2.000000e-05  \n",
       "63  2.000000e-05  \n",
       "64  2.000000e-05  \n",
       "65  2.000000e-05  \n",
       "66  2.000000e-05  \n",
       "67  2.000000e-05  \n",
       "68  2.000000e-05  \n",
       "69  2.000000e-05  \n",
       "70  2.000000e-06  \n",
       "71  2.000000e-06  \n",
       "72  2.000000e-06  \n",
       "73  2.000000e-06  \n",
       "74  2.000000e-06  \n",
       "75  2.000000e-06  \n",
       "76  2.000000e-06  \n",
       "77  2.000000e-06  \n",
       "78  2.000000e-06  \n",
       "79  2.000000e-06  \n",
       "80  2.000000e-07  \n",
       "81  2.000000e-07  \n",
       "82  2.000000e-07  \n",
       "83  2.000000e-07  \n",
       "\n",
       "[84 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "cpm.run_training(save_history = True, \n",
    "                 warm_start = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
