{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "from kde_training_utilities import kde_load_data\n",
    "import make_data_wfpt as mdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6-tf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer()\n",
    "\n",
    "# Load data\n",
    "data_folder = os.getcwd() + '/data_storage/kde/weibull/train_test_data'\n",
    "\n",
    "# rt_choice\n",
    "cpm.data['train_features'], cpm.data['train_labels'], cpm.data['test_features'], cpm.data['test_labels'] = kde_load_data(folder = data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v</th>\n",
       "      <th>a</th>\n",
       "      <th>w</th>\n",
       "      <th>node</th>\n",
       "      <th>shape</th>\n",
       "      <th>scale</th>\n",
       "      <th>rt</th>\n",
       "      <th>choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.387593</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.193834</td>\n",
       "      <td>4.306623</td>\n",
       "      <td>34.083296</td>\n",
       "      <td>5.679733</td>\n",
       "      <td>0.090612</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.889704</td>\n",
       "      <td>3.252047</td>\n",
       "      <td>0.750895</td>\n",
       "      <td>2.027924</td>\n",
       "      <td>17.592057</td>\n",
       "      <td>1.784538</td>\n",
       "      <td>3.283445</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.435071</td>\n",
       "      <td>1.169904</td>\n",
       "      <td>0.505879</td>\n",
       "      <td>0.295873</td>\n",
       "      <td>33.433546</td>\n",
       "      <td>6.033153</td>\n",
       "      <td>-0.493210</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.623121</td>\n",
       "      <td>1.258147</td>\n",
       "      <td>0.595502</td>\n",
       "      <td>2.541998</td>\n",
       "      <td>19.354642</td>\n",
       "      <td>9.211129</td>\n",
       "      <td>1.042804</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991182</td>\n",
       "      <td>1.202782</td>\n",
       "      <td>0.246382</td>\n",
       "      <td>1.547758</td>\n",
       "      <td>35.114721</td>\n",
       "      <td>3.834703</td>\n",
       "      <td>2.098449</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.188025</td>\n",
       "      <td>3.717582</td>\n",
       "      <td>0.388364</td>\n",
       "      <td>4.446054</td>\n",
       "      <td>47.014771</td>\n",
       "      <td>0.170039</td>\n",
       "      <td>1.440652</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.023425</td>\n",
       "      <td>0.921280</td>\n",
       "      <td>0.775702</td>\n",
       "      <td>1.833487</td>\n",
       "      <td>11.921771</td>\n",
       "      <td>4.924695</td>\n",
       "      <td>19.748926</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.656587</td>\n",
       "      <td>3.518704</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.560251</td>\n",
       "      <td>28.318027</td>\n",
       "      <td>5.966226</td>\n",
       "      <td>0.684367</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.232534</td>\n",
       "      <td>1.749036</td>\n",
       "      <td>0.222152</td>\n",
       "      <td>2.496463</td>\n",
       "      <td>7.479898</td>\n",
       "      <td>5.335776</td>\n",
       "      <td>0.237713</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.029434</td>\n",
       "      <td>2.993509</td>\n",
       "      <td>0.257198</td>\n",
       "      <td>1.714895</td>\n",
       "      <td>46.531127</td>\n",
       "      <td>3.884016</td>\n",
       "      <td>1.936733</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.088913</td>\n",
       "      <td>2.642174</td>\n",
       "      <td>0.179949</td>\n",
       "      <td>1.336380</td>\n",
       "      <td>42.242068</td>\n",
       "      <td>5.198544</td>\n",
       "      <td>-0.806206</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.278530</td>\n",
       "      <td>1.290133</td>\n",
       "      <td>0.567125</td>\n",
       "      <td>0.543606</td>\n",
       "      <td>30.498091</td>\n",
       "      <td>5.952630</td>\n",
       "      <td>2.633174</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.152612</td>\n",
       "      <td>2.332875</td>\n",
       "      <td>0.541389</td>\n",
       "      <td>3.006691</td>\n",
       "      <td>42.347631</td>\n",
       "      <td>7.347411</td>\n",
       "      <td>2.155157</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.570542</td>\n",
       "      <td>2.922494</td>\n",
       "      <td>0.577381</td>\n",
       "      <td>1.644999</td>\n",
       "      <td>5.251499</td>\n",
       "      <td>3.979568</td>\n",
       "      <td>-0.046828</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.746177</td>\n",
       "      <td>3.878623</td>\n",
       "      <td>0.533058</td>\n",
       "      <td>3.636375</td>\n",
       "      <td>27.391024</td>\n",
       "      <td>9.923536</td>\n",
       "      <td>2.398350</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.163048</td>\n",
       "      <td>3.905715</td>\n",
       "      <td>0.513502</td>\n",
       "      <td>4.209215</td>\n",
       "      <td>25.881124</td>\n",
       "      <td>2.516928</td>\n",
       "      <td>2.053585</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.412432</td>\n",
       "      <td>0.520390</td>\n",
       "      <td>0.590882</td>\n",
       "      <td>1.545538</td>\n",
       "      <td>7.246546</td>\n",
       "      <td>9.994705</td>\n",
       "      <td>0.290609</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.885119</td>\n",
       "      <td>2.940138</td>\n",
       "      <td>0.596902</td>\n",
       "      <td>1.750793</td>\n",
       "      <td>40.581236</td>\n",
       "      <td>6.582852</td>\n",
       "      <td>2.088757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.929471</td>\n",
       "      <td>2.732369</td>\n",
       "      <td>0.233246</td>\n",
       "      <td>4.326123</td>\n",
       "      <td>48.582689</td>\n",
       "      <td>1.696989</td>\n",
       "      <td>1.556593</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.840169</td>\n",
       "      <td>0.664343</td>\n",
       "      <td>0.439486</td>\n",
       "      <td>2.627667</td>\n",
       "      <td>48.568808</td>\n",
       "      <td>4.052626</td>\n",
       "      <td>0.426916</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.257268</td>\n",
       "      <td>2.203681</td>\n",
       "      <td>0.353238</td>\n",
       "      <td>0.144108</td>\n",
       "      <td>27.919512</td>\n",
       "      <td>5.976094</td>\n",
       "      <td>1.371632</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.546841</td>\n",
       "      <td>1.902672</td>\n",
       "      <td>0.178477</td>\n",
       "      <td>1.804070</td>\n",
       "      <td>8.975893</td>\n",
       "      <td>1.562939</td>\n",
       "      <td>1.361155</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.124626</td>\n",
       "      <td>3.031705</td>\n",
       "      <td>0.434974</td>\n",
       "      <td>1.595334</td>\n",
       "      <td>22.880903</td>\n",
       "      <td>5.867202</td>\n",
       "      <td>18.003532</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.172790</td>\n",
       "      <td>1.129731</td>\n",
       "      <td>0.762760</td>\n",
       "      <td>0.869566</td>\n",
       "      <td>12.942216</td>\n",
       "      <td>8.144815</td>\n",
       "      <td>5.558725</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.070373</td>\n",
       "      <td>2.621606</td>\n",
       "      <td>0.558026</td>\n",
       "      <td>1.013296</td>\n",
       "      <td>47.613911</td>\n",
       "      <td>7.922789</td>\n",
       "      <td>11.428328</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.391232</td>\n",
       "      <td>3.566816</td>\n",
       "      <td>0.322157</td>\n",
       "      <td>0.300889</td>\n",
       "      <td>31.558188</td>\n",
       "      <td>6.770125</td>\n",
       "      <td>1.433009</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.139175</td>\n",
       "      <td>0.924110</td>\n",
       "      <td>0.810155</td>\n",
       "      <td>4.460781</td>\n",
       "      <td>32.969493</td>\n",
       "      <td>2.229423</td>\n",
       "      <td>0.236939</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.591127</td>\n",
       "      <td>0.397926</td>\n",
       "      <td>3.272902</td>\n",
       "      <td>24.536832</td>\n",
       "      <td>6.551876</td>\n",
       "      <td>-0.820670</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.699546</td>\n",
       "      <td>3.748739</td>\n",
       "      <td>0.599408</td>\n",
       "      <td>1.772396</td>\n",
       "      <td>24.462488</td>\n",
       "      <td>7.917074</td>\n",
       "      <td>-0.985680</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.945466</td>\n",
       "      <td>1.284955</td>\n",
       "      <td>0.795704</td>\n",
       "      <td>2.136170</td>\n",
       "      <td>48.263905</td>\n",
       "      <td>9.925459</td>\n",
       "      <td>1.100896</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732597</th>\n",
       "      <td>1.055970</td>\n",
       "      <td>0.764062</td>\n",
       "      <td>0.387379</td>\n",
       "      <td>3.281473</td>\n",
       "      <td>15.407849</td>\n",
       "      <td>5.497189</td>\n",
       "      <td>0.742173</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732598</th>\n",
       "      <td>1.348788</td>\n",
       "      <td>2.148042</td>\n",
       "      <td>0.402366</td>\n",
       "      <td>0.851390</td>\n",
       "      <td>30.918705</td>\n",
       "      <td>4.995074</td>\n",
       "      <td>5.167174</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732599</th>\n",
       "      <td>-0.107324</td>\n",
       "      <td>0.868552</td>\n",
       "      <td>0.531343</td>\n",
       "      <td>4.859351</td>\n",
       "      <td>17.893153</td>\n",
       "      <td>1.765616</td>\n",
       "      <td>0.373294</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732600</th>\n",
       "      <td>1.861877</td>\n",
       "      <td>0.735148</td>\n",
       "      <td>0.575991</td>\n",
       "      <td>0.562755</td>\n",
       "      <td>16.826970</td>\n",
       "      <td>8.713659</td>\n",
       "      <td>0.270238</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732601</th>\n",
       "      <td>1.042539</td>\n",
       "      <td>2.777815</td>\n",
       "      <td>0.837099</td>\n",
       "      <td>4.975230</td>\n",
       "      <td>9.501909</td>\n",
       "      <td>4.002152</td>\n",
       "      <td>0.306225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732602</th>\n",
       "      <td>0.537108</td>\n",
       "      <td>2.534330</td>\n",
       "      <td>0.196213</td>\n",
       "      <td>4.656232</td>\n",
       "      <td>44.821499</td>\n",
       "      <td>2.815566</td>\n",
       "      <td>2.454539</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732603</th>\n",
       "      <td>2.211612</td>\n",
       "      <td>2.259958</td>\n",
       "      <td>0.604169</td>\n",
       "      <td>1.670824</td>\n",
       "      <td>41.194534</td>\n",
       "      <td>5.455511</td>\n",
       "      <td>0.678016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732604</th>\n",
       "      <td>-1.612625</td>\n",
       "      <td>1.228672</td>\n",
       "      <td>0.680642</td>\n",
       "      <td>3.097185</td>\n",
       "      <td>35.981743</td>\n",
       "      <td>6.635551</td>\n",
       "      <td>0.455448</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732605</th>\n",
       "      <td>-1.255255</td>\n",
       "      <td>0.554846</td>\n",
       "      <td>0.317702</td>\n",
       "      <td>2.219555</td>\n",
       "      <td>16.218391</td>\n",
       "      <td>1.771625</td>\n",
       "      <td>0.372294</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732606</th>\n",
       "      <td>-1.388272</td>\n",
       "      <td>1.606864</td>\n",
       "      <td>0.329839</td>\n",
       "      <td>2.277367</td>\n",
       "      <td>30.033354</td>\n",
       "      <td>5.810224</td>\n",
       "      <td>5.701374</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732607</th>\n",
       "      <td>-1.214202</td>\n",
       "      <td>1.385026</td>\n",
       "      <td>0.771398</td>\n",
       "      <td>0.649466</td>\n",
       "      <td>38.169321</td>\n",
       "      <td>0.792026</td>\n",
       "      <td>1.470143</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732608</th>\n",
       "      <td>2.185643</td>\n",
       "      <td>1.712770</td>\n",
       "      <td>0.387077</td>\n",
       "      <td>4.589472</td>\n",
       "      <td>36.986003</td>\n",
       "      <td>6.947764</td>\n",
       "      <td>0.776972</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732609</th>\n",
       "      <td>-1.336015</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>0.267870</td>\n",
       "      <td>3.607035</td>\n",
       "      <td>16.338581</td>\n",
       "      <td>3.560868</td>\n",
       "      <td>0.360104</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732610</th>\n",
       "      <td>1.740313</td>\n",
       "      <td>1.749100</td>\n",
       "      <td>0.638697</td>\n",
       "      <td>1.421285</td>\n",
       "      <td>10.876413</td>\n",
       "      <td>5.100338</td>\n",
       "      <td>-0.793576</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732611</th>\n",
       "      <td>1.962545</td>\n",
       "      <td>1.478444</td>\n",
       "      <td>0.339122</td>\n",
       "      <td>1.281012</td>\n",
       "      <td>11.243443</td>\n",
       "      <td>6.107089</td>\n",
       "      <td>1.386564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732612</th>\n",
       "      <td>0.061304</td>\n",
       "      <td>3.240872</td>\n",
       "      <td>0.843087</td>\n",
       "      <td>1.558409</td>\n",
       "      <td>3.302305</td>\n",
       "      <td>6.291962</td>\n",
       "      <td>10.396213</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732613</th>\n",
       "      <td>-2.324104</td>\n",
       "      <td>3.084135</td>\n",
       "      <td>0.769524</td>\n",
       "      <td>1.261798</td>\n",
       "      <td>4.601924</td>\n",
       "      <td>8.101644</td>\n",
       "      <td>-0.054313</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732614</th>\n",
       "      <td>1.168935</td>\n",
       "      <td>3.372968</td>\n",
       "      <td>0.346523</td>\n",
       "      <td>1.513110</td>\n",
       "      <td>36.816665</td>\n",
       "      <td>2.701397</td>\n",
       "      <td>5.643349</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732615</th>\n",
       "      <td>0.296823</td>\n",
       "      <td>2.795158</td>\n",
       "      <td>0.235963</td>\n",
       "      <td>2.208846</td>\n",
       "      <td>40.230061</td>\n",
       "      <td>1.391241</td>\n",
       "      <td>6.800658</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732616</th>\n",
       "      <td>1.500740</td>\n",
       "      <td>2.770160</td>\n",
       "      <td>0.389596</td>\n",
       "      <td>2.277313</td>\n",
       "      <td>47.658283</td>\n",
       "      <td>7.535263</td>\n",
       "      <td>2.749999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732617</th>\n",
       "      <td>1.494193</td>\n",
       "      <td>3.958902</td>\n",
       "      <td>0.833298</td>\n",
       "      <td>2.195919</td>\n",
       "      <td>24.831090</td>\n",
       "      <td>6.885065</td>\n",
       "      <td>11.457883</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732618</th>\n",
       "      <td>1.669950</td>\n",
       "      <td>1.036002</td>\n",
       "      <td>0.799442</td>\n",
       "      <td>2.195461</td>\n",
       "      <td>14.346978</td>\n",
       "      <td>4.473664</td>\n",
       "      <td>0.109765</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732619</th>\n",
       "      <td>-0.336425</td>\n",
       "      <td>3.573870</td>\n",
       "      <td>0.728778</td>\n",
       "      <td>2.974915</td>\n",
       "      <td>31.692075</td>\n",
       "      <td>1.912317</td>\n",
       "      <td>5.191304</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732620</th>\n",
       "      <td>0.146312</td>\n",
       "      <td>0.989230</td>\n",
       "      <td>0.746997</td>\n",
       "      <td>3.462072</td>\n",
       "      <td>1.371015</td>\n",
       "      <td>4.738066</td>\n",
       "      <td>0.315168</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732621</th>\n",
       "      <td>-2.330846</td>\n",
       "      <td>3.099517</td>\n",
       "      <td>0.431913</td>\n",
       "      <td>3.574290</td>\n",
       "      <td>43.674536</td>\n",
       "      <td>0.395041</td>\n",
       "      <td>0.831981</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732622</th>\n",
       "      <td>-0.034950</td>\n",
       "      <td>0.643013</td>\n",
       "      <td>0.488431</td>\n",
       "      <td>0.313770</td>\n",
       "      <td>41.264803</td>\n",
       "      <td>4.426686</td>\n",
       "      <td>0.166149</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732623</th>\n",
       "      <td>1.393517</td>\n",
       "      <td>2.642556</td>\n",
       "      <td>0.312346</td>\n",
       "      <td>4.383985</td>\n",
       "      <td>10.135852</td>\n",
       "      <td>1.043207</td>\n",
       "      <td>-0.928630</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732624</th>\n",
       "      <td>0.283436</td>\n",
       "      <td>1.671125</td>\n",
       "      <td>0.713976</td>\n",
       "      <td>0.830460</td>\n",
       "      <td>15.307978</td>\n",
       "      <td>2.741631</td>\n",
       "      <td>0.516173</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732625</th>\n",
       "      <td>-1.305478</td>\n",
       "      <td>1.581501</td>\n",
       "      <td>0.635238</td>\n",
       "      <td>2.415941</td>\n",
       "      <td>9.820164</td>\n",
       "      <td>4.512591</td>\n",
       "      <td>10.638097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155732626</th>\n",
       "      <td>-0.050337</td>\n",
       "      <td>0.511255</td>\n",
       "      <td>0.434172</td>\n",
       "      <td>0.141774</td>\n",
       "      <td>43.352454</td>\n",
       "      <td>9.409501</td>\n",
       "      <td>0.042045</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155732627 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  v         a         w      node      shape     scale  \\\n",
       "0          0.387593  0.659091  0.193834  4.306623  34.083296  5.679733   \n",
       "1         -1.889704  3.252047  0.750895  2.027924  17.592057  1.784538   \n",
       "2         -1.435071  1.169904  0.505879  0.295873  33.433546  6.033153   \n",
       "3         -1.623121  1.258147  0.595502  2.541998  19.354642  9.211129   \n",
       "4          0.991182  1.202782  0.246382  1.547758  35.114721  3.834703   \n",
       "5         -0.188025  3.717582  0.388364  4.446054  47.014771  0.170039   \n",
       "6         -0.023425  0.921280  0.775702  1.833487  11.921771  4.924695   \n",
       "7         -1.656587  3.518704  0.243900  0.560251  28.318027  5.966226   \n",
       "8         -2.232534  1.749036  0.222152  2.496463   7.479898  5.335776   \n",
       "9          1.029434  2.993509  0.257198  1.714895  46.531127  3.884016   \n",
       "10        -2.088913  2.642174  0.179949  1.336380  42.242068  5.198544   \n",
       "11        -1.278530  1.290133  0.567125  0.543606  30.498091  5.952630   \n",
       "12        -0.152612  2.332875  0.541389  3.006691  42.347631  7.347411   \n",
       "13        -0.570542  2.922494  0.577381  1.644999   5.251499  3.979568   \n",
       "14         1.746177  3.878623  0.533058  3.636375  27.391024  9.923536   \n",
       "15        -2.163048  3.905715  0.513502  4.209215  25.881124  2.516928   \n",
       "16         1.412432  0.520390  0.590882  1.545538   7.246546  9.994705   \n",
       "17         0.885119  2.940138  0.596902  1.750793  40.581236  6.582852   \n",
       "18         0.929471  2.732369  0.233246  4.326123  48.582689  1.696989   \n",
       "19        -0.840169  0.664343  0.439486  2.627667  48.568808  4.052626   \n",
       "20         2.257268  2.203681  0.353238  0.144108  27.919512  5.976094   \n",
       "21        -1.546841  1.902672  0.178477  1.804070   8.975893  1.562939   \n",
       "22        -1.124626  3.031705  0.434974  1.595334  22.880903  5.867202   \n",
       "23        -0.172790  1.129731  0.762760  0.869566  12.942216  8.144815   \n",
       "24        -0.070373  2.621606  0.558026  1.013296  47.613911  7.922789   \n",
       "25        -1.391232  3.566816  0.322157  0.300889  31.558188  6.770125   \n",
       "26        -0.139175  0.924110  0.810155  4.460781  32.969493  2.229423   \n",
       "27         0.234562  0.591127  0.397926  3.272902  24.536832  6.551876   \n",
       "28         1.699546  3.748739  0.599408  1.772396  24.462488  7.917074   \n",
       "29        -1.945466  1.284955  0.795704  2.136170  48.263905  9.925459   \n",
       "...             ...       ...       ...       ...        ...       ...   \n",
       "155732597  1.055970  0.764062  0.387379  3.281473  15.407849  5.497189   \n",
       "155732598  1.348788  2.148042  0.402366  0.851390  30.918705  4.995074   \n",
       "155732599 -0.107324  0.868552  0.531343  4.859351  17.893153  1.765616   \n",
       "155732600  1.861877  0.735148  0.575991  0.562755  16.826970  8.713659   \n",
       "155732601  1.042539  2.777815  0.837099  4.975230   9.501909  4.002152   \n",
       "155732602  0.537108  2.534330  0.196213  4.656232  44.821499  2.815566   \n",
       "155732603  2.211612  2.259958  0.604169  1.670824  41.194534  5.455511   \n",
       "155732604 -1.612625  1.228672  0.680642  3.097185  35.981743  6.635551   \n",
       "155732605 -1.255255  0.554846  0.317702  2.219555  16.218391  1.771625   \n",
       "155732606 -1.388272  1.606864  0.329839  2.277367  30.033354  5.810224   \n",
       "155732607 -1.214202  1.385026  0.771398  0.649466  38.169321  0.792026   \n",
       "155732608  2.185643  1.712770  0.387077  4.589472  36.986003  6.947764   \n",
       "155732609 -1.336015  0.736268  0.267870  3.607035  16.338581  3.560868   \n",
       "155732610  1.740313  1.749100  0.638697  1.421285  10.876413  5.100338   \n",
       "155732611  1.962545  1.478444  0.339122  1.281012  11.243443  6.107089   \n",
       "155732612  0.061304  3.240872  0.843087  1.558409   3.302305  6.291962   \n",
       "155732613 -2.324104  3.084135  0.769524  1.261798   4.601924  8.101644   \n",
       "155732614  1.168935  3.372968  0.346523  1.513110  36.816665  2.701397   \n",
       "155732615  0.296823  2.795158  0.235963  2.208846  40.230061  1.391241   \n",
       "155732616  1.500740  2.770160  0.389596  2.277313  47.658283  7.535263   \n",
       "155732617  1.494193  3.958902  0.833298  2.195919  24.831090  6.885065   \n",
       "155732618  1.669950  1.036002  0.799442  2.195461  14.346978  4.473664   \n",
       "155732619 -0.336425  3.573870  0.728778  2.974915  31.692075  1.912317   \n",
       "155732620  0.146312  0.989230  0.746997  3.462072   1.371015  4.738066   \n",
       "155732621 -2.330846  3.099517  0.431913  3.574290  43.674536  0.395041   \n",
       "155732622 -0.034950  0.643013  0.488431  0.313770  41.264803  4.426686   \n",
       "155732623  1.393517  2.642556  0.312346  4.383985  10.135852  1.043207   \n",
       "155732624  0.283436  1.671125  0.713976  0.830460  15.307978  2.741631   \n",
       "155732625 -1.305478  1.581501  0.635238  2.415941   9.820164  4.512591   \n",
       "155732626 -0.050337  0.511255  0.434172  0.141774  43.352454  9.409501   \n",
       "\n",
       "                  rt  choice  \n",
       "0           0.090612    -1.0  \n",
       "1           3.283445    -1.0  \n",
       "2          -0.493210     1.0  \n",
       "3           1.042804    -1.0  \n",
       "4           2.098449     1.0  \n",
       "5           1.440652    -1.0  \n",
       "6          19.748926     1.0  \n",
       "7           0.684367    -1.0  \n",
       "8           0.237713    -1.0  \n",
       "9           1.936733     1.0  \n",
       "10         -0.806206     1.0  \n",
       "11          2.633174    -1.0  \n",
       "12          2.155157    -1.0  \n",
       "13         -0.046828    -1.0  \n",
       "14          2.398350     1.0  \n",
       "15          2.053585    -1.0  \n",
       "16          0.290609     1.0  \n",
       "17          2.088757     1.0  \n",
       "18          1.556593     1.0  \n",
       "19          0.426916    -1.0  \n",
       "20          1.371632     1.0  \n",
       "21          1.361155    -1.0  \n",
       "22         18.003532     1.0  \n",
       "23          5.558725    -1.0  \n",
       "24         11.428328    -1.0  \n",
       "25          1.433009    -1.0  \n",
       "26          0.236939     1.0  \n",
       "27         -0.820670     1.0  \n",
       "28         -0.985680    -1.0  \n",
       "29          1.100896     1.0  \n",
       "...              ...     ...  \n",
       "155732597   0.742173     1.0  \n",
       "155732598   5.167174    -1.0  \n",
       "155732599   0.373294    -1.0  \n",
       "155732600   0.270238     1.0  \n",
       "155732601   0.306225     1.0  \n",
       "155732602   2.454539    -1.0  \n",
       "155732603   0.678016     1.0  \n",
       "155732604   0.455448    -1.0  \n",
       "155732605   0.372294    -1.0  \n",
       "155732606   5.701374    -1.0  \n",
       "155732607   1.470143    -1.0  \n",
       "155732608   0.776972     1.0  \n",
       "155732609   0.360104    -1.0  \n",
       "155732610  -0.793576    -1.0  \n",
       "155732611   1.386564     1.0  \n",
       "155732612  10.396213    -1.0  \n",
       "155732613  -0.054313     1.0  \n",
       "155732614   5.643349     1.0  \n",
       "155732615   6.800658     1.0  \n",
       "155732616   2.749999     1.0  \n",
       "155732617  11.457883    -1.0  \n",
       "155732618   0.109765     1.0  \n",
       "155732619   5.191304    -1.0  \n",
       "155732620   0.315168    -1.0  \n",
       "155732621   0.831981    -1.0  \n",
       "155732622   0.166149    -1.0  \n",
       "155732623  -0.928630     1.0  \n",
       "155732624   0.516173     1.0  \n",
       "155732625  10.638097     1.0  \n",
       "155732626   0.042045    -1.0  \n",
       "\n",
       "[155732627 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['train_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v          0.532478\n",
       "a          4.135571\n",
       "w          0.332414\n",
       "node       1.684656\n",
       "theta      0.001445\n",
       "rt        54.513734\n",
       "choice    -1.000000\n",
       "Name: 171247010, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['train_features'].iloc[171247010, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features']['log_l'] = cpm.data['train_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v</th>\n",
       "      <th>a</th>\n",
       "      <th>w</th>\n",
       "      <th>node</th>\n",
       "      <th>theta</th>\n",
       "      <th>rt</th>\n",
       "      <th>choice</th>\n",
       "      <th>log_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79216387</th>\n",
       "      <td>-1.327356</td>\n",
       "      <td>2.577692</td>\n",
       "      <td>0.325039</td>\n",
       "      <td>0.783290</td>\n",
       "      <td>1.542936</td>\n",
       "      <td>-0.339051</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694012</th>\n",
       "      <td>-2.037707</td>\n",
       "      <td>4.508431</td>\n",
       "      <td>0.280631</td>\n",
       "      <td>0.139979</td>\n",
       "      <td>1.163901</td>\n",
       "      <td>15.363290</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694014</th>\n",
       "      <td>-2.255496</td>\n",
       "      <td>4.399791</td>\n",
       "      <td>0.460537</td>\n",
       "      <td>3.353939</td>\n",
       "      <td>0.513475</td>\n",
       "      <td>-0.987525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169530381</th>\n",
       "      <td>-0.429863</td>\n",
       "      <td>2.060916</td>\n",
       "      <td>0.804049</td>\n",
       "      <td>3.338324</td>\n",
       "      <td>1.159617</td>\n",
       "      <td>-0.544030</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694049</th>\n",
       "      <td>-0.400060</td>\n",
       "      <td>3.297588</td>\n",
       "      <td>0.458565</td>\n",
       "      <td>1.322020</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>-0.613319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17786222</th>\n",
       "      <td>2.116870</td>\n",
       "      <td>1.269840</td>\n",
       "      <td>0.550752</td>\n",
       "      <td>3.140840</td>\n",
       "      <td>1.266288</td>\n",
       "      <td>13.486367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694052</th>\n",
       "      <td>0.151548</td>\n",
       "      <td>1.408500</td>\n",
       "      <td>0.344639</td>\n",
       "      <td>1.145105</td>\n",
       "      <td>1.510945</td>\n",
       "      <td>-0.800053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694054</th>\n",
       "      <td>-0.117521</td>\n",
       "      <td>2.465984</td>\n",
       "      <td>0.679334</td>\n",
       "      <td>0.018993</td>\n",
       "      <td>0.323353</td>\n",
       "      <td>-0.700669</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694056</th>\n",
       "      <td>0.599202</td>\n",
       "      <td>1.021541</td>\n",
       "      <td>0.828207</td>\n",
       "      <td>4.343027</td>\n",
       "      <td>1.184066</td>\n",
       "      <td>-0.983606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694059</th>\n",
       "      <td>-2.018082</td>\n",
       "      <td>1.856659</td>\n",
       "      <td>0.445296</td>\n",
       "      <td>3.217213</td>\n",
       "      <td>0.949524</td>\n",
       "      <td>10.984560</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694072</th>\n",
       "      <td>2.107379</td>\n",
       "      <td>2.343973</td>\n",
       "      <td>0.681255</td>\n",
       "      <td>2.463932</td>\n",
       "      <td>0.630754</td>\n",
       "      <td>-0.444383</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176430250</th>\n",
       "      <td>1.963685</td>\n",
       "      <td>0.575163</td>\n",
       "      <td>0.256055</td>\n",
       "      <td>0.680206</td>\n",
       "      <td>1.540048</td>\n",
       "      <td>-0.452332</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694073</th>\n",
       "      <td>-1.752805</td>\n",
       "      <td>1.736992</td>\n",
       "      <td>0.429023</td>\n",
       "      <td>2.441657</td>\n",
       "      <td>0.292045</td>\n",
       "      <td>-0.956007</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694084</th>\n",
       "      <td>-0.447477</td>\n",
       "      <td>4.289380</td>\n",
       "      <td>0.495218</td>\n",
       "      <td>3.511407</td>\n",
       "      <td>1.123751</td>\n",
       "      <td>-0.128748</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694086</th>\n",
       "      <td>1.993978</td>\n",
       "      <td>1.496115</td>\n",
       "      <td>0.496574</td>\n",
       "      <td>0.913374</td>\n",
       "      <td>1.219392</td>\n",
       "      <td>18.570328</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694103</th>\n",
       "      <td>1.624596</td>\n",
       "      <td>3.642863</td>\n",
       "      <td>0.399044</td>\n",
       "      <td>1.178085</td>\n",
       "      <td>1.357957</td>\n",
       "      <td>14.455102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694107</th>\n",
       "      <td>1.940140</td>\n",
       "      <td>1.372992</td>\n",
       "      <td>0.791384</td>\n",
       "      <td>3.658207</td>\n",
       "      <td>1.085042</td>\n",
       "      <td>-0.034423</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694112</th>\n",
       "      <td>2.216847</td>\n",
       "      <td>1.133589</td>\n",
       "      <td>0.435188</td>\n",
       "      <td>4.259033</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>-0.270013</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694119</th>\n",
       "      <td>1.167960</td>\n",
       "      <td>1.283075</td>\n",
       "      <td>0.650393</td>\n",
       "      <td>1.117343</td>\n",
       "      <td>1.211372</td>\n",
       "      <td>-0.391798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17786208</th>\n",
       "      <td>-2.042945</td>\n",
       "      <td>2.156227</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>4.028040</td>\n",
       "      <td>0.959517</td>\n",
       "      <td>-0.534946</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17786207</th>\n",
       "      <td>-2.131993</td>\n",
       "      <td>3.969120</td>\n",
       "      <td>0.454923</td>\n",
       "      <td>2.549455</td>\n",
       "      <td>1.253945</td>\n",
       "      <td>-0.920192</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694124</th>\n",
       "      <td>-0.609722</td>\n",
       "      <td>1.052347</td>\n",
       "      <td>0.404524</td>\n",
       "      <td>3.573418</td>\n",
       "      <td>0.302068</td>\n",
       "      <td>-0.331276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71693992</th>\n",
       "      <td>1.885539</td>\n",
       "      <td>4.215280</td>\n",
       "      <td>0.366849</td>\n",
       "      <td>3.003809</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>19.035340</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71694126</th>\n",
       "      <td>1.565617</td>\n",
       "      <td>2.916583</td>\n",
       "      <td>0.390903</td>\n",
       "      <td>1.446946</td>\n",
       "      <td>0.489401</td>\n",
       "      <td>-0.668410</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71693986</th>\n",
       "      <td>-2.076736</td>\n",
       "      <td>0.801792</td>\n",
       "      <td>0.452245</td>\n",
       "      <td>2.928562</td>\n",
       "      <td>1.202744</td>\n",
       "      <td>15.581077</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176430248</th>\n",
       "      <td>0.830617</td>\n",
       "      <td>0.872133</td>\n",
       "      <td>0.629185</td>\n",
       "      <td>2.695243</td>\n",
       "      <td>1.364894</td>\n",
       "      <td>-0.624510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71693874</th>\n",
       "      <td>1.117844</td>\n",
       "      <td>3.077961</td>\n",
       "      <td>0.189417</td>\n",
       "      <td>3.836714</td>\n",
       "      <td>1.233977</td>\n",
       "      <td>19.183640</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71693883</th>\n",
       "      <td>2.146083</td>\n",
       "      <td>2.063430</td>\n",
       "      <td>0.726939</td>\n",
       "      <td>2.907617</td>\n",
       "      <td>1.285019</td>\n",
       "      <td>-0.943173</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17786250</th>\n",
       "      <td>0.336310</td>\n",
       "      <td>0.637334</td>\n",
       "      <td>0.158258</td>\n",
       "      <td>4.689833</td>\n",
       "      <td>1.289827</td>\n",
       "      <td>13.211933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71693890</th>\n",
       "      <td>2.225285</td>\n",
       "      <td>3.565482</td>\n",
       "      <td>0.562523</td>\n",
       "      <td>1.806338</td>\n",
       "      <td>0.962140</td>\n",
       "      <td>7.720893</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100623573</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68252211</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111653322</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88839595</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76250598</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44355094</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31625427</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408425</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141586277</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147781404</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41418487</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104090620</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33712853</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101205112</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79961092</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152814435</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131907130</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131910310</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37248276</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48444457</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103400499</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76260697</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10316325</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21472839</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85697394</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637295</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38505340</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26244528</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57775481</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56979913</th>\n",
       "      <td>1.778493</td>\n",
       "      <td>2.490588</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>1.563415</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.675606e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178705167 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  v         a         w      node     theta         rt  \\\n",
       "79216387  -1.327356  2.577692  0.325039  0.783290  1.542936  -0.339051   \n",
       "71694012  -2.037707  4.508431  0.280631  0.139979  1.163901  15.363290   \n",
       "71694014  -2.255496  4.399791  0.460537  3.353939  0.513475  -0.987525   \n",
       "169530381 -0.429863  2.060916  0.804049  3.338324  1.159617  -0.544030   \n",
       "71694049  -0.400060  3.297588  0.458565  1.322020  0.033900  -0.613319   \n",
       "17786222   2.116870  1.269840  0.550752  3.140840  1.266288  13.486367   \n",
       "71694052   0.151548  1.408500  0.344639  1.145105  1.510945  -0.800053   \n",
       "71694054  -0.117521  2.465984  0.679334  0.018993  0.323353  -0.700669   \n",
       "71694056   0.599202  1.021541  0.828207  4.343027  1.184066  -0.983606   \n",
       "71694059  -2.018082  1.856659  0.445296  3.217213  0.949524  10.984560   \n",
       "71694072   2.107379  2.343973  0.681255  2.463932  0.630754  -0.444383   \n",
       "176430250  1.963685  0.575163  0.256055  0.680206  1.540048  -0.452332   \n",
       "71694073  -1.752805  1.736992  0.429023  2.441657  0.292045  -0.956007   \n",
       "71694084  -0.447477  4.289380  0.495218  3.511407  1.123751  -0.128748   \n",
       "71694086   1.993978  1.496115  0.496574  0.913374  1.219392  18.570328   \n",
       "71694103   1.624596  3.642863  0.399044  1.178085  1.357957  14.455102   \n",
       "71694107   1.940140  1.372992  0.791384  3.658207  1.085042  -0.034423   \n",
       "71694112   2.216847  1.133589  0.435188  4.259033  0.418882  -0.270013   \n",
       "71694119   1.167960  1.283075  0.650393  1.117343  1.211372  -0.391798   \n",
       "17786208  -2.042945  2.156227  0.803626  4.028040  0.959517  -0.534946   \n",
       "17786207  -2.131993  3.969120  0.454923  2.549455  1.253945  -0.920192   \n",
       "71694124  -0.609722  1.052347  0.404524  3.573418  0.302068  -0.331276   \n",
       "71693992   1.885539  4.215280  0.366849  3.003809  0.999036  19.035340   \n",
       "71694126   1.565617  2.916583  0.390903  1.446946  0.489401  -0.668410   \n",
       "71693986  -2.076736  0.801792  0.452245  2.928562  1.202744  15.581077   \n",
       "176430248  0.830617  0.872133  0.629185  2.695243  1.364894  -0.624510   \n",
       "71693874   1.117844  3.077961  0.189417  3.836714  1.233977  19.183640   \n",
       "71693883   2.146083  2.063430  0.726939  2.907617  1.285019  -0.943173   \n",
       "17786250   0.336310  0.637334  0.158258  4.689833  1.289827  13.211933   \n",
       "71693890   2.225285  3.565482  0.562523  1.806338  0.962140   7.720893   \n",
       "...             ...       ...       ...       ...       ...        ...   \n",
       "100623573  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "68252211   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "111653322  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "88839595   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "76250598   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "44355094   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "31625427   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "3408425    1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "141586277  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "147781404  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "41418487   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "104090620  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "33712853   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "101205112  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "79961092   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "152814435  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "131907130  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "131910310  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "37248276   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "48444457   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "103400499  1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "76260697   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "10316325   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "21472839   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "85697394   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "16637295   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "38505340   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "26244528   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "57775481   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "56979913   1.778493  2.490588  0.292645  0.003781  1.563415   0.020000   \n",
       "\n",
       "           choice         log_l  \n",
       "79216387     -1.0  1.000000e-29  \n",
       "71694012     -1.0  1.000000e-29  \n",
       "71694014      1.0  1.000000e-29  \n",
       "169530381    -1.0  1.000000e-29  \n",
       "71694049      1.0  1.000000e-29  \n",
       "17786222      1.0  1.000000e-29  \n",
       "71694052      1.0  1.000000e-29  \n",
       "71694054     -1.0  1.000000e-29  \n",
       "71694056      1.0  1.000000e-29  \n",
       "71694059     -1.0  1.000000e-29  \n",
       "71694072      1.0  1.000000e-29  \n",
       "176430250    -1.0  1.000000e-29  \n",
       "71694073     -1.0  1.000000e-29  \n",
       "71694084     -1.0  1.000000e-29  \n",
       "71694086     -1.0  1.000000e-29  \n",
       "71694103      1.0  1.000000e-29  \n",
       "71694107     -1.0  1.000000e-29  \n",
       "71694112     -1.0  1.000000e-29  \n",
       "71694119      1.0  1.000000e-29  \n",
       "17786208     -1.0  1.000000e-29  \n",
       "17786207     -1.0  1.000000e-29  \n",
       "71694124      1.0  1.000000e-29  \n",
       "71693992      1.0  1.000000e-29  \n",
       "71694126     -1.0  1.000000e-29  \n",
       "71693986     -1.0  1.000000e-29  \n",
       "176430248     1.0  1.000000e-29  \n",
       "71693874      1.0  1.000000e-29  \n",
       "71693883     -1.0  1.000000e-29  \n",
       "17786250      1.0  1.000000e-29  \n",
       "71693890     -1.0  1.000000e-29  \n",
       "...           ...           ...  \n",
       "100623573    -1.0  2.675606e+17  \n",
       "68252211     -1.0  2.675606e+17  \n",
       "111653322    -1.0  2.675606e+17  \n",
       "88839595     -1.0  2.675606e+17  \n",
       "76250598     -1.0  2.675606e+17  \n",
       "44355094     -1.0  2.675606e+17  \n",
       "31625427     -1.0  2.675606e+17  \n",
       "3408425      -1.0  2.675606e+17  \n",
       "141586277    -1.0  2.675606e+17  \n",
       "147781404    -1.0  2.675606e+17  \n",
       "41418487     -1.0  2.675606e+17  \n",
       "104090620    -1.0  2.675606e+17  \n",
       "33712853     -1.0  2.675606e+17  \n",
       "101205112    -1.0  2.675606e+17  \n",
       "79961092     -1.0  2.675606e+17  \n",
       "152814435    -1.0  2.675606e+17  \n",
       "131907130    -1.0  2.675606e+17  \n",
       "131910310    -1.0  2.675606e+17  \n",
       "37248276     -1.0  2.675606e+17  \n",
       "48444457     -1.0  2.675606e+17  \n",
       "103400499    -1.0  2.675606e+17  \n",
       "76260697     -1.0  2.675606e+17  \n",
       "10316325     -1.0  2.675606e+17  \n",
       "21472839     -1.0  2.675606e+17  \n",
       "85697394     -1.0  2.675606e+17  \n",
       "16637295     -1.0  2.675606e+17  \n",
       "38505340     -1.0  2.675606e+17  \n",
       "26244528     -1.0  2.675606e+17  \n",
       "57775481     -1.0  2.675606e+17  \n",
       "56979913     -1.0  2.675606e+17  \n",
       "\n",
       "[178705167 rows x 8 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['train_features'].sort_values(by = 'log_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm.data['train_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v         1.778493\n",
       "a         2.490588\n",
       "w         0.292645\n",
       "node      0.003781\n",
       "theta     1.563415\n",
       "rt        0.020000\n",
       "choice   -1.000000\n",
       "Name: 22428, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['train_features'].iloc[22428, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2.67560563e+17]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm.data['train_labels'][22428, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters concerning data storage\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, specify new set of parameters here:\n",
    "# Model params\n",
    "cpm.model_params['output_activation'] = 'linear'\n",
    "cpm.model_params['hidden_layers'] = [20, 40, 60, 80, 100, 120]\n",
    "cpm.model_params['hidden_activations'] = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "cpm.model_params['input_shape'] = cpm.data['train_features'].shape[1]\n",
    "cpm.model_params['l1_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l1_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# Train params\n",
    "cpm.train_params['batch_size'] = 200000\n",
    "cpm.train_params['max_train_epochs'] = 200\n",
    "\n",
    "# Data params\n",
    "cpm.data_params['data_type'] = 'wfpt'\n",
    "cpm.data_params['data_type_signature'] = '_kde_ddm_weibull_'\n",
    "cpm.data_params['training_data_size'] = cpm.data['train_features'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 155732627 samples, validate on 17303005 samples\n",
      "Epoch 1/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.2154 - mean_squared_error: 0.2154 - val_loss: 0.0810 - val_mean_squared_error: 0.0810\n",
      "Epoch 2/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0904 - mean_squared_error: 0.0904 - val_loss: 0.0611 - val_mean_squared_error: 0.0611\n",
      "Epoch 3/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0641 - mean_squared_error: 0.0641 - val_loss: 0.0462 - val_mean_squared_error: 0.0462\n",
      "Epoch 4/200\n",
      "155732627/155732627 [==============================] - 267s 2us/step - loss: 0.0469 - mean_squared_error: 0.0469 - val_loss: 0.0706 - val_mean_squared_error: 0.0706\n",
      "Epoch 5/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0213 - val_mean_squared_error: 0.0213\n",
      "Epoch 6/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0279 - val_mean_squared_error: 0.0279\n",
      "Epoch 7/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0146 - val_mean_squared_error: 0.0146\n",
      "Epoch 8/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0111 - val_mean_squared_error: 0.0111\n",
      "Epoch 9/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 10/200\n",
      "155732627/155732627 [==============================] - 268s 2us/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00010: val_loss improved from inf to 0.00811, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_10\n",
      "Epoch 11/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 12/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 13/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 14/200\n",
      "155732627/155732627 [==============================] - 269s 2us/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0071 - val_mean_squared_error: 0.0071\n",
      "Epoch 15/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0093 - mean_squared_error: 0.0093 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 16/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0088 - mean_squared_error: 0.0088 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
      "Epoch 17/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 18/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 19/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 20/200\n",
      "155732627/155732627 [==============================] - 268s 2us/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00811\n",
      "Epoch 21/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 22/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 23/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 24/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "Epoch 25/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 26/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 27/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 28/200\n",
      "155732627/155732627 [==============================] - 267s 2us/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 29/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 30/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00811 to 0.00352, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_30\n",
      "Epoch 31/200\n",
      "155732627/155732627 [==============================] - 267s 2us/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 32/200\n",
      "155732627/155732627 [==============================] - 267s 2us/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0119 - val_mean_squared_error: 0.0119\n",
      "Epoch 33/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 34/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 35/200\n",
      "155732627/155732627 [==============================] - 271s 2us/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 36/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 37/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 38/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 39/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 40/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00352\n",
      "Epoch 41/200\n",
      "155732627/155732627 [==============================] - 258s 2us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 42/200\n",
      "155732627/155732627 [==============================] - 233s 1us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 43/200\n",
      "155732627/155732627 [==============================] - 259s 2us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155732627/155732627 [==============================] - 243s 2us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 45/200\n",
      "155732627/155732627 [==============================] - 246s 2us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 46/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 47/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 48/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 49/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 50/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00352\n",
      "Epoch 51/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 52/200\n",
      "155732627/155732627 [==============================] - 239s 2us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 53/200\n",
      "155732627/155732627 [==============================] - 252s 2us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 54/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 55/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 56/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.4111 - mean_squared_error: 0.4111 - val_loss: 0.0148 - val_mean_squared_error: 0.0148\n",
      "Epoch 57/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0100 - mean_squared_error: 0.0100 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 58/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 59/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 60/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00352\n",
      "Epoch 61/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 62/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 63/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 64/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 65/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 66/200\n",
      "155732627/155732627 [==============================] - 268s 2us/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 67/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 68/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 69/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 70/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00352 to 0.00226, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_70\n",
      "Epoch 71/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 72/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 73/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 74/200\n",
      "155732627/155732627 [==============================] - 261s 2us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 75/200\n",
      "155732627/155732627 [==============================] - 261s 2us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 76/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 77/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 78/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 79/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 80/200\n",
      "155732627/155732627 [==============================] - 268s 2us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00226 to 0.00169, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_80\n",
      "Epoch 81/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 82/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 83/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 84/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 85/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 86/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 88/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 89/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 90/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00169 to 0.00150, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_90\n",
      "Epoch 91/200\n",
      "155732627/155732627 [==============================] - 261s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 92/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 93/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 94/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 95/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 96/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 97/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 98/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 99/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 100/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.00150 to 0.00146, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_100\n",
      "Epoch 101/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 102/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 103/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 104/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 105/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 106/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 107/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 108/200\n",
      "155732627/155732627 [==============================] - 262s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 109/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 110/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00146 to 0.00131, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_110\n",
      "Epoch 111/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 112/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 113/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 114/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 115/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 116/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 117/200\n",
      "155732627/155732627 [==============================] - 266s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 118/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 119/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "\n",
      "Epoch 00119: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "Epoch 120/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00131 to 0.00130, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_weibull_06_05_19_14_07_16/ckpt_0_120\n",
      "Epoch 121/200\n",
      "155732627/155732627 [==============================] - 265s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 122/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 123/200\n",
      "155732627/155732627 [==============================] - 263s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 124/200\n",
      "155732627/155732627 [==============================] - 264s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 00124: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081018</td>\n",
       "      <td>0.081018</td>\n",
       "      <td>0.215422</td>\n",
       "      <td>0.215422</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061097</td>\n",
       "      <td>0.061097</td>\n",
       "      <td>0.090421</td>\n",
       "      <td>0.090421</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046163</td>\n",
       "      <td>0.046163</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070580</td>\n",
       "      <td>0.070580</td>\n",
       "      <td>0.046897</td>\n",
       "      <td>0.046897</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021332</td>\n",
       "      <td>0.021332</td>\n",
       "      <td>0.035838</td>\n",
       "      <td>0.035838</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.027924</td>\n",
       "      <td>0.027924</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.023264</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.011065</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.017333</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008113</td>\n",
       "      <td>0.008113</td>\n",
       "      <td>0.014933</td>\n",
       "      <td>0.014933</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.010434</td>\n",
       "      <td>0.010434</td>\n",
       "      <td>0.013323</td>\n",
       "      <td>0.013323</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.012627</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.012034</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.010102</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005963</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.007733</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.006360</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010238</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.005792</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.006479</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.005594</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.004433</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.005701</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004678</td>\n",
       "      <td>0.004678</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.005748</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007574</td>\n",
       "      <td>0.007574</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.005545</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_loss  val_mean_squared_error      loss  mean_squared_error        lr\n",
       "0    0.081018                0.081018  0.215422            0.215422  0.002000\n",
       "1    0.061097                0.061097  0.090421            0.090421  0.002000\n",
       "2    0.046163                0.046163  0.064101            0.064101  0.002000\n",
       "3    0.070580                0.070580  0.046897            0.046897  0.002000\n",
       "4    0.021332                0.021332  0.035838            0.035838  0.002000\n",
       "5    0.027924                0.027924  0.028372            0.028372  0.002000\n",
       "6    0.014635                0.014635  0.023264            0.023264  0.002000\n",
       "7    0.011065                0.011065  0.019648            0.019648  0.002000\n",
       "8    0.017333                0.017333  0.017117            0.017117  0.002000\n",
       "9    0.008113                0.008113  0.014933            0.014933  0.002000\n",
       "10   0.010434                0.010434  0.013323            0.013323  0.002000\n",
       "11   0.012627                0.012627  0.012034            0.012034  0.002000\n",
       "12   0.016039                0.016039  0.010881            0.010881  0.002000\n",
       "13   0.007150                0.007150  0.010102            0.010102  0.002000\n",
       "14   0.008712                0.008712  0.009277            0.009277  0.002000\n",
       "15   0.009394                0.009394  0.008847            0.008847  0.002000\n",
       "16   0.005791                0.005791  0.008185            0.008185  0.002000\n",
       "17   0.005963                0.005963  0.007733            0.007733  0.002000\n",
       "18   0.006360                0.006360  0.007369            0.007369  0.002000\n",
       "19   0.010238                0.010238  0.007139            0.007139  0.002000\n",
       "20   0.005792                0.005792  0.006762            0.006762  0.002000\n",
       "21   0.004044                0.004044  0.006710            0.006710  0.002000\n",
       "22   0.006719                0.006719  0.006479            0.006479  0.002000\n",
       "23   0.005594                0.005594  0.006317            0.006317  0.002000\n",
       "24   0.004356                0.004356  0.006067            0.006067  0.002000\n",
       "25   0.004802                0.004802  0.005894            0.005894  0.002000\n",
       "26   0.004433                0.004433  0.005701            0.005701  0.002000\n",
       "27   0.004678                0.004678  0.005748            0.005748  0.002000\n",
       "28   0.007574                0.007574  0.005545            0.005545  0.002000\n",
       "29   0.003518                0.003518  0.005419            0.005419  0.002000\n",
       "..        ...                     ...       ...                 ...       ...\n",
       "94   0.001421                0.001421  0.001490            0.001490  0.000200\n",
       "95   0.001429                0.001429  0.001472            0.001472  0.000200\n",
       "96   0.001426                0.001426  0.001461            0.001461  0.000200\n",
       "97   0.001450                0.001450  0.001450            0.001450  0.000200\n",
       "98   0.001400                0.001400  0.001447            0.001447  0.000200\n",
       "99   0.001455                0.001455  0.001434            0.001434  0.000200\n",
       "100  0.002217                0.002217  0.001441            0.001441  0.000200\n",
       "101  0.001376                0.001376  0.001428            0.001428  0.000200\n",
       "102  0.001362                0.001362  0.001409            0.001409  0.000200\n",
       "103  0.001397                0.001397  0.001401            0.001401  0.000200\n",
       "104  0.001367                0.001367  0.001446            0.001446  0.000200\n",
       "105  0.001474                0.001474  0.001383            0.001383  0.000200\n",
       "106  0.002071                0.002071  0.001392            0.001392  0.000200\n",
       "107  0.001694                0.001694  0.001383            0.001383  0.000200\n",
       "108  0.001315                0.001315  0.001322            0.001322  0.000020\n",
       "109  0.001315                0.001315  0.001318            0.001318  0.000020\n",
       "110  0.001314                0.001314  0.001318            0.001318  0.000020\n",
       "111  0.001316                0.001316  0.001317            0.001317  0.000020\n",
       "112  0.001317                0.001317  0.001316            0.001316  0.000020\n",
       "113  0.001310                0.001310  0.001314            0.001314  0.000020\n",
       "114  0.001307                0.001307  0.001313            0.001313  0.000020\n",
       "115  0.001310                0.001310  0.001311            0.001311  0.000020\n",
       "116  0.001305                0.001305  0.001310            0.001310  0.000020\n",
       "117  0.001303                0.001303  0.001308            0.001308  0.000020\n",
       "118  0.001303                0.001303  0.001306            0.001306  0.000020\n",
       "119  0.001300                0.001300  0.001303            0.001303  0.000002\n",
       "120  0.001300                0.001300  0.001303            0.001303  0.000002\n",
       "121  0.001299                0.001299  0.001303            0.001303  0.000002\n",
       "122  0.001299                0.001299  0.001303            0.001303  0.000002\n",
       "123  0.001300                0.001300  0.001302            0.001302  0.000002\n",
       "\n",
       "[124 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "cpm.run_training(save_history = True, \n",
    "                 warm_start = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
