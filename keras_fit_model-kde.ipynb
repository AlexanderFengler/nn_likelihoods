{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "from kde_training_utilities import kde_load_data\n",
    "import make_data_wfpt as mdw\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ['1','2','3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6-tf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer()\n",
    "\n",
    "# Load data\n",
    "data_folder = os.getcwd() + '/data_storage/kde/kde_ddm_flexbound_train_test_extended_params/'\n",
    "\n",
    "# rt_choice\n",
    "cpm.data['train_features'], cpm.data['train_labels'], cpm.data['test_features'], cpm.data['test_labels'] = kde_load_data(folder = data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_shape': 3,\n",
       " 'output_shape': 1,\n",
       " 'output_activation': 'sigmoid',\n",
       " 'hidden_layers': [20, 20, 20, 20],\n",
       " 'hidden_activations': ['relu', 'relu', 'relu', 'relu'],\n",
       " 'l1_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l1_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'optimizer': 'Nadam',\n",
       " 'loss': 'mse',\n",
       " 'metrics': ['mse']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'callback_funs': ['ReduceLROnPlateau', 'EarlyStopping', 'ModelCheckpoint'],\n",
       " 'plateau_patience': 10,\n",
       " 'min_delta': 0.0001,\n",
       " 'early_stopping_patience': 15,\n",
       " 'callback_monitor': 'loss',\n",
       " 'min_learning_rate': 1e-07,\n",
       " 'red_coef_learning_rate': 0.1,\n",
       " 'ckpt_period': 10,\n",
       " 'ckpt_save_best_only': True,\n",
       " 'ckpt_save_weights_only': True,\n",
       " 'max_train_epochs': 2000,\n",
       " 'batch_size': 10000,\n",
       " 'warm_start': False,\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_cnt': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'choice_probabilities',\n",
       " 'model_directory': '/home/afengler/git_repos/nn_likelihoods/keras_models',\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_name': 'dnnregressor',\n",
       " 'data_type_signature': '_choice_probabilities_analytic_',\n",
       " 'timestamp': '06_02_19_15_25_32',\n",
       " 'training_data_size': 2500000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters concerning data storage\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, specify new set of parameters here:\n",
    "# Model params\n",
    "cpm.model_params['output_activation'] = 'linear'\n",
    "cpm.model_params['hidden_layers'] = [20, 40, 60, 80, 100, 120]\n",
    "cpm.model_params['hidden_activations'] = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "cpm.model_params['input_shape'] = cpm.data['train_features'].shape[1]\n",
    "cpm.model_params['l1_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l1_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# Train params\n",
    "cpm.train_params['batch_size'] = 200000\n",
    "cpm.train_params['max_train_epochs'] = 200\n",
    "\n",
    "# Data params\n",
    "cpm.data_params['data_type'] = 'wfpt'\n",
    "cpm.data_params['data_type_signature'] = '_kde_ddm_flexbnd_c1_c2'\n",
    "cpm.data_params['training_data_size'] = cpm.data['train_features'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 197646506 samples, validate on 21961430 samples\n",
      "Epoch 1/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.1220 - mean_squared_error: 0.1220 - val_loss: 0.0805 - val_mean_squared_error: 0.0805\n",
      "Epoch 2/200\n",
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 0.0373 - mean_squared_error: 0.0373 - val_loss: 0.0196 - val_mean_squared_error: 0.0196\n",
      "Epoch 3/200\n",
      "197646506/197646506 [==============================] - 317s 2us/step - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
      "Epoch 4/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Epoch 5/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0134 - val_mean_squared_error: 0.0134\n",
      "Epoch 6/200\n",
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0096 - val_mean_squared_error: 0.0096\n",
      "Epoch 7/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 8/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 9/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 10/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "\n",
      "Epoch 00010: val_loss improved from inf to 0.00804, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_10\n",
      "Epoch 11/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
      "Epoch 12/200\n",
      "197646506/197646506 [==============================] - 317s 2us/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 13/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 14/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 15/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 16/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
      "Epoch 17/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 18/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 19/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 20/200\n",
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00804 to 0.00538, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_20\n",
      "Epoch 21/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 22/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 23/200\n",
      "197646506/197646506 [==============================] - 324s 2us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 24/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 25/200\n",
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 26/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 27/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 28/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
      "Epoch 29/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 30/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00538 to 0.00383, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_30\n",
      "Epoch 31/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n",
      "Epoch 32/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 33/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
      "Epoch 34/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 35/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 36/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 37/200\n",
      "197646506/197646506 [==============================] - 323s 2us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 38/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 39/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 40/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00383 to 0.00278, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_40\n",
      "Epoch 41/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 43/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 44/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 45/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 46/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 47/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 48/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 49/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 50/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00278 to 0.00176, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_50\n",
      "Epoch 51/200\n",
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 52/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 53/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 54/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 55/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 56/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 57/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 58/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 59/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 60/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00176\n",
      "Epoch 61/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 62/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 63/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 64/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 65/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 66/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0013 - mean_squared_error: 0.0013 - val_loss: 0.0013 - val_mean_squared_error: 0.0013\n",
      "Epoch 67/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0012 - mean_squared_error: 0.0012 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "Epoch 68/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 69/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 70/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.00176 to 0.00123, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_70\n",
      "Epoch 71/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0011 - val_mean_squared_error: 0.0011\n",
      "Epoch 72/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 73/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 0.0010 - mean_squared_error: 0.0010 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n",
      "Epoch 74/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.9900e-04 - mean_squared_error: 9.9900e-04 - val_loss: 9.9784e-04 - val_mean_squared_error: 9.9784e-04\n",
      "Epoch 75/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.8657e-04 - mean_squared_error: 9.8657e-04 - val_loss: 9.8469e-04 - val_mean_squared_error: 9.8469e-04\n",
      "Epoch 76/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.7661e-04 - mean_squared_error: 9.7661e-04 - val_loss: 9.7013e-04 - val_mean_squared_error: 9.7013e-04\n",
      "Epoch 77/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 9.6722e-04 - mean_squared_error: 9.6722e-04 - val_loss: 9.6202e-04 - val_mean_squared_error: 9.6202e-04\n",
      "Epoch 78/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.5855e-04 - mean_squared_error: 9.5855e-04 - val_loss: 9.6108e-04 - val_mean_squared_error: 9.6108e-04\n",
      "Epoch 79/200\n",
      "197646506/197646506 [==============================] - 318s 2us/step - loss: 9.5097e-04 - mean_squared_error: 9.5097e-04 - val_loss: 9.5128e-04 - val_mean_squared_error: 9.5128e-04\n",
      "Epoch 80/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.4317e-04 - mean_squared_error: 9.4317e-04 - val_loss: 9.8954e-04 - val_mean_squared_error: 9.8954e-04\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00123 to 0.00099, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_80\n",
      "Epoch 81/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.3783e-04 - mean_squared_error: 9.3783e-04 - val_loss: 9.6094e-04 - val_mean_squared_error: 9.6094e-04\n",
      "Epoch 82/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.3146e-04 - mean_squared_error: 9.3146e-04 - val_loss: 9.3912e-04 - val_mean_squared_error: 9.3912e-04\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.1322e-04 - mean_squared_error: 9.1322e-04 - val_loss: 9.1411e-04 - val_mean_squared_error: 9.1411e-04\n",
      "Epoch 84/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.1244e-04 - mean_squared_error: 9.1244e-04 - val_loss: 9.1532e-04 - val_mean_squared_error: 9.1532e-04\n",
      "Epoch 85/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.1158e-04 - mean_squared_error: 9.1158e-04 - val_loss: 9.1268e-04 - val_mean_squared_error: 9.1268e-04\n",
      "Epoch 86/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.1058e-04 - mean_squared_error: 9.1058e-04 - val_loss: 9.1156e-04 - val_mean_squared_error: 9.1156e-04\n",
      "Epoch 87/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.0949e-04 - mean_squared_error: 9.0949e-04 - val_loss: 9.1131e-04 - val_mean_squared_error: 9.1131e-04\n",
      "Epoch 88/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.0822e-04 - mean_squared_error: 9.0822e-04 - val_loss: 9.0897e-04 - val_mean_squared_error: 9.0897e-04\n",
      "Epoch 89/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 9.0693e-04 - mean_squared_error: 9.0693e-04 - val_loss: 9.0818e-04 - val_mean_squared_error: 9.0818e-04\n",
      "Epoch 90/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.0565e-04 - mean_squared_error: 9.0565e-04 - val_loss: 9.0696e-04 - val_mean_squared_error: 9.0696e-04\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00099 to 0.00091, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_flexbnd_c1_c206_02_19_15_25_32/ckpt_0_90\n",
      "Epoch 91/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 9.0456e-04 - mean_squared_error: 9.0456e-04 - val_loss: 9.0628e-04 - val_mean_squared_error: 9.0628e-04\n",
      "Epoch 92/200\n",
      "197646506/197646506 [==============================] - 322s 2us/step - loss: 9.0334e-04 - mean_squared_error: 9.0334e-04 - val_loss: 9.0498e-04 - val_mean_squared_error: 9.0498e-04\n",
      "Epoch 93/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 9.0216e-04 - mean_squared_error: 9.0216e-04 - val_loss: 9.0424e-04 - val_mean_squared_error: 9.0424e-04\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "Epoch 94/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 9.0018e-04 - mean_squared_error: 9.0018e-04 - val_loss: 9.0206e-04 - val_mean_squared_error: 9.0206e-04\n",
      "Epoch 95/200\n",
      "197646506/197646506 [==============================] - 324s 2us/step - loss: 8.9995e-04 - mean_squared_error: 8.9995e-04 - val_loss: 9.0190e-04 - val_mean_squared_error: 9.0190e-04\n",
      "Epoch 96/200\n",
      "197646506/197646506 [==============================] - 320s 2us/step - loss: 8.9980e-04 - mean_squared_error: 8.9980e-04 - val_loss: 9.0186e-04 - val_mean_squared_error: 9.0186e-04\n",
      "Epoch 97/200\n",
      "197646506/197646506 [==============================] - 321s 2us/step - loss: 8.9968e-04 - mean_squared_error: 8.9968e-04 - val_loss: 9.0171e-04 - val_mean_squared_error: 9.0171e-04\n",
      "Epoch 98/200\n",
      "197646506/197646506 [==============================] - 319s 2us/step - loss: 8.9956e-04 - mean_squared_error: 8.9956e-04 - val_loss: 9.0157e-04 - val_mean_squared_error: 9.0157e-04\n",
      "Epoch 00098: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.080512</td>\n",
       "      <td>0.080512</td>\n",
       "      <td>0.122030</td>\n",
       "      <td>0.122030</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.019620</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017724</td>\n",
       "      <td>0.017724</td>\n",
       "      <td>0.023147</td>\n",
       "      <td>0.023147</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013995</td>\n",
       "      <td>0.013995</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.017111</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013368</td>\n",
       "      <td>0.013368</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>0.013790</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.009066</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.007415</td>\n",
       "      <td>0.007415</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>0.006802</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.005053</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.004639</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.004519</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.003897</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.003529</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.003921</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    val_loss  val_mean_squared_error      loss  mean_squared_error        lr\n",
       "0   0.080512                0.080512  0.122030            0.122030  0.002000\n",
       "1   0.019620                0.019620  0.037320            0.037320  0.002000\n",
       "2   0.017724                0.017724  0.023147            0.023147  0.002000\n",
       "3   0.013995                0.013995  0.017111            0.017111  0.002000\n",
       "4   0.013368                0.013368  0.013790            0.013790  0.002000\n",
       "5   0.009638                0.009638  0.011621            0.011621  0.002000\n",
       "6   0.007829                0.007829  0.010090            0.010090  0.002000\n",
       "7   0.009066                0.009066  0.008931            0.008931  0.002000\n",
       "8   0.005900                0.005900  0.008048            0.008048  0.002000\n",
       "9   0.008036                0.008036  0.007415            0.007415  0.002000\n",
       "10  0.005495                0.005495  0.006802            0.006802  0.002000\n",
       "11  0.008436                0.008436  0.006399            0.006399  0.002000\n",
       "12  0.008044                0.008044  0.005991            0.005991  0.002000\n",
       "13  0.004217                0.004217  0.005572            0.005572  0.002000\n",
       "14  0.004955                0.004955  0.005454            0.005454  0.002000\n",
       "15  0.006184                0.006184  0.005873            0.005873  0.002000\n",
       "16  0.005103                0.005103  0.005053            0.005053  0.002000\n",
       "17  0.005705                0.005705  0.004988            0.004988  0.002000\n",
       "18  0.004652                0.004652  0.005042            0.005042  0.002000\n",
       "19  0.005383                0.005383  0.004904            0.004904  0.002000\n",
       "20  0.004020                0.004020  0.004639            0.004639  0.002000\n",
       "21  0.004180                0.004180  0.004628            0.004628  0.002000\n",
       "22  0.004316                0.004316  0.004519            0.004519  0.002000\n",
       "23  0.003897                0.003897  0.004635            0.004635  0.002000\n",
       "24  0.003529                0.003529  0.004206            0.004206  0.002000\n",
       "25  0.003921                0.003921  0.004165            0.004165  0.002000\n",
       "26  0.003066                0.003066  0.003843            0.003843  0.002000\n",
       "27  0.006573                0.006573  0.004001            0.004001  0.002000\n",
       "28  0.004083                0.004083  0.003834            0.003834  0.002000\n",
       "29  0.003828                0.003828  0.005143            0.005143  0.002000\n",
       "..       ...                     ...       ...                 ...       ...\n",
       "68  0.001095                0.001095  0.001096            0.001096  0.000200\n",
       "69  0.001231                0.001231  0.001068            0.001068  0.000200\n",
       "70  0.001068                0.001068  0.001046            0.001046  0.000200\n",
       "71  0.001030                0.001030  0.001027            0.001027  0.000200\n",
       "72  0.001009                0.001009  0.001012            0.001012  0.000200\n",
       "73  0.000998                0.000998  0.000999            0.000999  0.000200\n",
       "74  0.000985                0.000985  0.000987            0.000987  0.000200\n",
       "75  0.000970                0.000970  0.000977            0.000977  0.000200\n",
       "76  0.000962                0.000962  0.000967            0.000967  0.000200\n",
       "77  0.000961                0.000961  0.000959            0.000959  0.000200\n",
       "78  0.000951                0.000951  0.000951            0.000951  0.000200\n",
       "79  0.000990                0.000990  0.000943            0.000943  0.000200\n",
       "80  0.000961                0.000961  0.000938            0.000938  0.000200\n",
       "81  0.000939                0.000939  0.000931            0.000931  0.000200\n",
       "82  0.000914                0.000914  0.000913            0.000913  0.000020\n",
       "83  0.000915                0.000915  0.000912            0.000912  0.000020\n",
       "84  0.000913                0.000913  0.000912            0.000912  0.000020\n",
       "85  0.000912                0.000912  0.000911            0.000911  0.000020\n",
       "86  0.000911                0.000911  0.000909            0.000909  0.000020\n",
       "87  0.000909                0.000909  0.000908            0.000908  0.000020\n",
       "88  0.000908                0.000908  0.000907            0.000907  0.000020\n",
       "89  0.000907                0.000907  0.000906            0.000906  0.000020\n",
       "90  0.000906                0.000906  0.000905            0.000905  0.000020\n",
       "91  0.000905                0.000905  0.000903            0.000903  0.000020\n",
       "92  0.000904                0.000904  0.000902            0.000902  0.000020\n",
       "93  0.000902                0.000902  0.000900            0.000900  0.000002\n",
       "94  0.000902                0.000902  0.000900            0.000900  0.000002\n",
       "95  0.000902                0.000902  0.000900            0.000900  0.000002\n",
       "96  0.000902                0.000902  0.000900            0.000900  0.000002\n",
       "97  0.000902                0.000902  0.000900            0.000900  0.000002\n",
       "\n",
       "[98 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "cpm.run_training(save_history = True, \n",
    "                 warm_start = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
