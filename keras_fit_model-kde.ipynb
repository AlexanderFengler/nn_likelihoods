{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "from kde_training_utilities import kde_load_data\n",
    "import make_data_wfpt as mdw\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ['0','1','2','3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer()\n",
    "\n",
    "# Load data\n",
    "data_folder = os.getcwd() + '/data_storage/kde/kde_training_dat/ddm_final_train_test'\n",
    "\n",
    "# rt_choice\n",
    "cpm.data['train_features'], cpm.data['train_labels'], cpm.data['test_features'], cpm.data['test_labels'] = kde_load_data(folder = data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_shape': 3,\n",
       " 'output_shape': 1,\n",
       " 'output_activation': 'sigmoid',\n",
       " 'hidden_layers': [20, 20, 20, 20],\n",
       " 'hidden_activations': ['relu', 'relu', 'relu', 'relu'],\n",
       " 'l1_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l1_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'optimizer': 'Nadam',\n",
       " 'loss': 'mse',\n",
       " 'metrics': ['mse']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'callback_funs': ['ReduceLROnPlateau', 'EarlyStopping', 'ModelCheckpoint'],\n",
       " 'plateau_patience': 10,\n",
       " 'min_delta': 0.0001,\n",
       " 'early_stopping_patience': 15,\n",
       " 'callback_monitor': 'loss',\n",
       " 'min_learning_rate': 1e-07,\n",
       " 'red_coef_learning_rate': 0.1,\n",
       " 'ckpt_period': 10,\n",
       " 'ckpt_save_best_only': True,\n",
       " 'ckpt_save_weights_only': True,\n",
       " 'max_train_epochs': 2000,\n",
       " 'batch_size': 10000,\n",
       " 'warm_start': False,\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_cnt': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'choice_probabilities',\n",
       " 'model_directory': '/home/afengler/git_repos/nn_likelihoods/keras_models',\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_name': 'dnnregressor',\n",
       " 'data_type_signature': '_choice_probabilities_analytic_',\n",
       " 'timestamp': '04_20_19_23_47_34',\n",
       " 'training_data_size': 2500000}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters concerning data storage\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, specify new set of parameters here:\n",
    "# Model params\n",
    "cpm.model_params['output_activation'] = 'linear'\n",
    "cpm.model_params['hidden_layers'] = [80, 80, 80, 80, 80]\n",
    "cpm.model_params['hidden_activations'] = ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "cpm.model_params['input_shape'] = cpm.data['train_features'].shape[1]\n",
    "cpm.model_params['l1_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_activation'] = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l1_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "cpm.model_params['l2_kernel'] = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "# Train params\n",
    "cpm.train_params['batch_size'] = 200000\n",
    "\n",
    "# Data params\n",
    "cpm.data_params['data_type'] = 'wfpt'\n",
    "cpm.data_params['data_type_signature'] = '_kde_ddm_'\n",
    "cpm.data_params['training_data_size'] = cpm.data['train_features'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108125520 samples, validate on 27033480 samples\n",
      "Epoch 1/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 1.4090 - mean_squared_error: 1.4090 - val_loss: 0.6603 - val_mean_squared_error: 0.6603\n",
      "Epoch 2/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.5481 - mean_squared_error: 0.5481 - val_loss: 0.4129 - val_mean_squared_error: 0.4129\n",
      "Epoch 3/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.4370 - mean_squared_error: 0.4370 - val_loss: 0.3665 - val_mean_squared_error: 0.3665\n",
      "Epoch 4/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.4026 - mean_squared_error: 0.4026 - val_loss: 0.3000 - val_mean_squared_error: 0.3000\n",
      "Epoch 5/2000\n",
      "108125520/108125520 [==============================] - 152s 1us/step - loss: 0.3787 - mean_squared_error: 0.3787 - val_loss: 0.2959 - val_mean_squared_error: 0.2959\n",
      "Epoch 6/2000\n",
      "108125520/108125520 [==============================] - 152s 1us/step - loss: 0.3661 - mean_squared_error: 0.3661 - val_loss: 0.3691 - val_mean_squared_error: 0.3691\n",
      "Epoch 7/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.3450 - mean_squared_error: 0.3450 - val_loss: 0.3725 - val_mean_squared_error: 0.3725\n",
      "Epoch 8/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.3405 - mean_squared_error: 0.3405 - val_loss: 0.3089 - val_mean_squared_error: 0.3089\n",
      "Epoch 9/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.3043 - mean_squared_error: 0.3043 - val_loss: 0.2010 - val_mean_squared_error: 0.2010\n",
      "Epoch 10/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.2686 - mean_squared_error: 0.2686 - val_loss: 0.2212 - val_mean_squared_error: 0.2212\n",
      "\n",
      "Epoch 00010: val_loss improved from inf to 0.22122, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_10\n",
      "Epoch 11/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.2018 - mean_squared_error: 0.2018 - val_loss: 0.2004 - val_mean_squared_error: 0.2004\n",
      "Epoch 12/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.1422 - mean_squared_error: 0.1422 - val_loss: 0.0722 - val_mean_squared_error: 0.0722\n",
      "Epoch 13/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.1036 - mean_squared_error: 0.1036 - val_loss: 0.0387 - val_mean_squared_error: 0.0387\n",
      "Epoch 14/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0812 - mean_squared_error: 0.0812 - val_loss: 0.0480 - val_mean_squared_error: 0.0480\n",
      "Epoch 15/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0662 - mean_squared_error: 0.0662 - val_loss: 0.1073 - val_mean_squared_error: 0.1073\n",
      "Epoch 16/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0571 - mean_squared_error: 0.0571 - val_loss: 0.1164 - val_mean_squared_error: 0.1164\n",
      "Epoch 17/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0503 - mean_squared_error: 0.0503 - val_loss: 0.0252 - val_mean_squared_error: 0.0252\n",
      "Epoch 18/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0297 - val_mean_squared_error: 0.0297\n",
      "Epoch 19/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0522 - mean_squared_error: 0.0522 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n",
      "Epoch 20/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0400 - mean_squared_error: 0.0400 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.22122 to 0.05691, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_20\n",
      "Epoch 21/2000\n",
      "108125520/108125520 [==============================] - 152s 1us/step - loss: 0.0358 - mean_squared_error: 0.0358 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
      "Epoch 22/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0471 - val_mean_squared_error: 0.0471\n",
      "Epoch 23/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0323 - mean_squared_error: 0.0323 - val_loss: 0.0303 - val_mean_squared_error: 0.0303\n",
      "Epoch 24/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0301 - mean_squared_error: 0.0301 - val_loss: 0.0326 - val_mean_squared_error: 0.0326\n",
      "Epoch 25/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0286 - mean_squared_error: 0.0286 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
      "Epoch 26/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0266 - mean_squared_error: 0.0266 - val_loss: 0.0212 - val_mean_squared_error: 0.0212\n",
      "Epoch 27/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0262 - mean_squared_error: 0.0262 - val_loss: 0.0217 - val_mean_squared_error: 0.0217\n",
      "Epoch 28/2000\n",
      "108125520/108125520 [==============================] - 151s 1us/step - loss: 0.0249 - mean_squared_error: 0.0249 - val_loss: 0.0138 - val_mean_squared_error: 0.0138\n",
      "Epoch 29/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0233 - mean_squared_error: 0.0233 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 30/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0236 - mean_squared_error: 0.0236 - val_loss: 0.0337 - val_mean_squared_error: 0.0337\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05691 to 0.03366, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_30\n",
      "Epoch 31/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0217 - mean_squared_error: 0.0217 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 32/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 33/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Epoch 34/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0205 - mean_squared_error: 0.0205 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 35/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0230 - val_mean_squared_error: 0.0230\n",
      "Epoch 36/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0231 - val_mean_squared_error: 0.0231\n",
      "Epoch 37/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "Epoch 38/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Epoch 39/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0107 - val_mean_squared_error: 0.0107\n",
      "Epoch 40/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.03366 to 0.01044, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_40\n",
      "Epoch 41/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0461 - val_mean_squared_error: 0.0461\n",
      "Epoch 42/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 43/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0158 - mean_squared_error: 0.0158 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 44/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 45/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0265 - val_mean_squared_error: 0.0265\n",
      "Epoch 46/2000\n",
      "108125520/108125520 [==============================] - 152s 1us/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 47/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0091 - val_mean_squared_error: 0.0091\n",
      "Epoch 48/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0149 - mean_squared_error: 0.0149 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 49/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 50/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0103 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.01044 to 0.01029, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_50\n",
      "Epoch 51/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0142 - mean_squared_error: 0.0142 - val_loss: 0.0115 - val_mean_squared_error: 0.0115\n",
      "Epoch 52/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
      "Epoch 53/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 54/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 55/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0113 - val_mean_squared_error: 0.0113\n",
      "Epoch 56/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 57/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0135 - mean_squared_error: 0.0135 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 58/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0141 - val_mean_squared_error: 0.0141\n",
      "Epoch 59/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0132 - mean_squared_error: 0.0132 - val_loss: 0.0054 - val_mean_squared_error: 0.0054\n",
      "Epoch 60/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.01029 to 0.00820, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_60\n",
      "Epoch 61/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Epoch 62/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0148 - val_mean_squared_error: 0.0148\n",
      "Epoch 63/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0081 - val_mean_squared_error: 0.0081\n",
      "Epoch 64/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "Epoch 65/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 66/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0072 - val_mean_squared_error: 0.0072\n",
      "Epoch 67/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 68/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 69/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 70/2000\n",
      "108125520/108125520 [==============================] - 152s 1us/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00820\n",
      "Epoch 71/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0125 - val_mean_squared_error: 0.0125\n",
      "Epoch 72/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 73/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 74/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 75/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 76/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 77/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 78/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0113 - mean_squared_error: 0.0113 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 79/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Epoch 80/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00820\n",
      "Epoch 81/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 82/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 83/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 84/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0140 - val_mean_squared_error: 0.0140\n",
      "Epoch 85/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0075 - val_mean_squared_error: 0.0075\n",
      "Epoch 86/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0134 - mean_squared_error: 0.0134 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 87/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0083 - val_mean_squared_error: 0.0083\n",
      "Epoch 88/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 89/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 90/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.00820 to 0.00489, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_90\n",
      "Epoch 91/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 92/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0214 - val_mean_squared_error: 0.0214\n",
      "Epoch 93/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 94/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 95/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0467 - val_mean_squared_error: 0.0467\n",
      "Epoch 96/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 97/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0361 - mean_squared_error: 0.0361 - val_loss: 0.0747 - val_mean_squared_error: 0.0747\n",
      "Epoch 98/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 99/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.0187 - val_mean_squared_error: 0.0187\n",
      "Epoch 100/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0056 - val_mean_squared_error: 0.0056\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00489\n",
      "Epoch 101/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0110 - mean_squared_error: 0.0110 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 102/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0150 - val_mean_squared_error: 0.0150\n",
      "Epoch 103/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0112 - mean_squared_error: 0.0112 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 104/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 105/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 106/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
      "Epoch 107/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 108/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 109/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "Epoch 110/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.00489 to 0.00466, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_110\n",
      "Epoch 111/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 112/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 113/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 114/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0046 - mean_squared_error: 0.0046 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 115/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 116/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 117/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 118/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 119/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 120/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.00466 to 0.00437, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_120\n",
      "Epoch 121/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
      "Epoch 122/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 123/2000\n",
      "108125520/108125520 [==============================] - 152s 1us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 124/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 125/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 126/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 127/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 128/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 129/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 130/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.00437 to 0.00419, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_130\n",
      "Epoch 131/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 132/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 133/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 134/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 135/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
      "Epoch 136/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 137/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 138/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 139/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 140/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.00419 to 0.00418, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_140\n",
      "Epoch 141/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 142/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 143/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 144/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 145/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 146/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 147/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 148/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 149/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "\n",
      "Epoch 00149: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 150/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00418 to 0.00396, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_150\n",
      "Epoch 151/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 152/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 153/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 154/2000\n",
      "108125520/108125520 [==============================] - 156s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 155/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 156/2000\n",
      "108125520/108125520 [==============================] - 151s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 157/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 158/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 159/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
      "Epoch 160/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00396 to 0.00395, saving model to /home/afengler/git_repos/nn_likelihoods/keras_models/dnnregressor_kde_ddm_04_20_19_23_47_34/ckpt_0_160\n",
      "Epoch 161/2000\n",
      "108125520/108125520 [==============================] - 153s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 162/2000\n",
      "108125520/108125520 [==============================] - 155s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 163/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 164/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 165/2000\n",
      "108125520/108125520 [==============================] - 154s 1us/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
      "Epoch 00165: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mean_squared_error</th>\n",
       "      <th>loss</th>\n",
       "      <th>mean_squared_error</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660332</td>\n",
       "      <td>0.660332</td>\n",
       "      <td>1.408975</td>\n",
       "      <td>1.408975</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.412944</td>\n",
       "      <td>0.412944</td>\n",
       "      <td>0.548109</td>\n",
       "      <td>0.548109</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366475</td>\n",
       "      <td>0.366475</td>\n",
       "      <td>0.437005</td>\n",
       "      <td>0.437005</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.299984</td>\n",
       "      <td>0.299984</td>\n",
       "      <td>0.402554</td>\n",
       "      <td>0.402554</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295892</td>\n",
       "      <td>0.295892</td>\n",
       "      <td>0.378733</td>\n",
       "      <td>0.378733</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.369071</td>\n",
       "      <td>0.369071</td>\n",
       "      <td>0.366150</td>\n",
       "      <td>0.366150</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.372536</td>\n",
       "      <td>0.372536</td>\n",
       "      <td>0.345037</td>\n",
       "      <td>0.345037</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.308932</td>\n",
       "      <td>0.308932</td>\n",
       "      <td>0.340466</td>\n",
       "      <td>0.340466</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.201028</td>\n",
       "      <td>0.201028</td>\n",
       "      <td>0.304310</td>\n",
       "      <td>0.304310</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.221219</td>\n",
       "      <td>0.221219</td>\n",
       "      <td>0.268580</td>\n",
       "      <td>0.268580</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.200360</td>\n",
       "      <td>0.200360</td>\n",
       "      <td>0.201757</td>\n",
       "      <td>0.201757</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.072180</td>\n",
       "      <td>0.072180</td>\n",
       "      <td>0.142228</td>\n",
       "      <td>0.142228</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.038696</td>\n",
       "      <td>0.038696</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>0.103575</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.047952</td>\n",
       "      <td>0.047952</td>\n",
       "      <td>0.081171</td>\n",
       "      <td>0.081171</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.066164</td>\n",
       "      <td>0.066164</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.116415</td>\n",
       "      <td>0.116415</td>\n",
       "      <td>0.057139</td>\n",
       "      <td>0.057139</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.025235</td>\n",
       "      <td>0.025235</td>\n",
       "      <td>0.050260</td>\n",
       "      <td>0.050260</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.029749</td>\n",
       "      <td>0.029749</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.045632</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>0.052230</td>\n",
       "      <td>0.052230</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.056909</td>\n",
       "      <td>0.056909</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.039962</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.020222</td>\n",
       "      <td>0.035757</td>\n",
       "      <td>0.035757</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.047127</td>\n",
       "      <td>0.047127</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>0.034198</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.032324</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.032560</td>\n",
       "      <td>0.032560</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.021449</td>\n",
       "      <td>0.021449</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.021232</td>\n",
       "      <td>0.021232</td>\n",
       "      <td>0.026558</td>\n",
       "      <td>0.026558</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.021678</td>\n",
       "      <td>0.021678</td>\n",
       "      <td>0.026227</td>\n",
       "      <td>0.026227</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.024930</td>\n",
       "      <td>0.024930</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.033664</td>\n",
       "      <td>0.023587</td>\n",
       "      <td>0.023587</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.004116</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.003947</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.003961</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.003959</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003943</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     val_loss  val_mean_squared_error      loss  mean_squared_error        lr\n",
       "0    0.660332                0.660332  1.408975            1.408975  0.002000\n",
       "1    0.412944                0.412944  0.548109            0.548109  0.002000\n",
       "2    0.366475                0.366475  0.437005            0.437005  0.002000\n",
       "3    0.299984                0.299984  0.402554            0.402554  0.002000\n",
       "4    0.295892                0.295892  0.378733            0.378733  0.002000\n",
       "5    0.369071                0.369071  0.366150            0.366150  0.002000\n",
       "6    0.372536                0.372536  0.345037            0.345037  0.002000\n",
       "7    0.308932                0.308932  0.340466            0.340466  0.002000\n",
       "8    0.201028                0.201028  0.304310            0.304310  0.002000\n",
       "9    0.221219                0.221219  0.268580            0.268580  0.002000\n",
       "10   0.200360                0.200360  0.201757            0.201757  0.002000\n",
       "11   0.072180                0.072180  0.142228            0.142228  0.002000\n",
       "12   0.038696                0.038696  0.103575            0.103575  0.002000\n",
       "13   0.047952                0.047952  0.081171            0.081171  0.002000\n",
       "14   0.107300                0.107300  0.066164            0.066164  0.002000\n",
       "15   0.116415                0.116415  0.057139            0.057139  0.002000\n",
       "16   0.025235                0.025235  0.050260            0.050260  0.002000\n",
       "17   0.029749                0.029749  0.045632            0.045632  0.002000\n",
       "18   0.021140                0.021140  0.052230            0.052230  0.002000\n",
       "19   0.056909                0.056909  0.039962            0.039962  0.002000\n",
       "20   0.020222                0.020222  0.035757            0.035757  0.002000\n",
       "21   0.047127                0.047127  0.034198            0.034198  0.002000\n",
       "22   0.030282                0.030282  0.032324            0.032324  0.002000\n",
       "23   0.032560                0.032560  0.030113            0.030113  0.002000\n",
       "24   0.021449                0.021449  0.028569            0.028569  0.002000\n",
       "25   0.021232                0.021232  0.026558            0.026558  0.002000\n",
       "26   0.021678                0.021678  0.026227            0.026227  0.002000\n",
       "27   0.013822                0.013822  0.024930            0.024930  0.002000\n",
       "28   0.008533                0.008533  0.023329            0.023329  0.002000\n",
       "29   0.033664                0.033664  0.023587            0.023587  0.002000\n",
       "..        ...                     ...       ...                 ...       ...\n",
       "135  0.004105                0.004105  0.004150            0.004150  0.000200\n",
       "136  0.004097                0.004097  0.004131            0.004131  0.000200\n",
       "137  0.004116                0.004116  0.004129            0.004129  0.000200\n",
       "138  0.004085                0.004085  0.004123            0.004123  0.000200\n",
       "139  0.004177                0.004177  0.004103            0.004103  0.000200\n",
       "140  0.004191                0.004191  0.004097            0.004097  0.000200\n",
       "141  0.004179                0.004179  0.004094            0.004094  0.000200\n",
       "142  0.004055                0.004055  0.004075            0.004075  0.000200\n",
       "143  0.004040                0.004040  0.004082            0.004082  0.000200\n",
       "144  0.004247                0.004247  0.004058            0.004058  0.000200\n",
       "145  0.004105                0.004105  0.004052            0.004052  0.000200\n",
       "146  0.004037                0.004037  0.004048            0.004048  0.000200\n",
       "147  0.004041                0.004041  0.004045            0.004045  0.000200\n",
       "148  0.004016                0.004016  0.004038            0.004038  0.000200\n",
       "149  0.003959                0.003959  0.003948            0.003948  0.000020\n",
       "150  0.003958                0.003958  0.003947            0.003947  0.000020\n",
       "151  0.003959                0.003959  0.003946            0.003946  0.000020\n",
       "152  0.003961                0.003961  0.003945            0.003945  0.000020\n",
       "153  0.003959                0.003959  0.003944            0.003944  0.000020\n",
       "154  0.003958                0.003958  0.003943            0.003943  0.000020\n",
       "155  0.003953                0.003953  0.003942            0.003942  0.000020\n",
       "156  0.003955                0.003955  0.003941            0.003941  0.000020\n",
       "157  0.003960                0.003960  0.003939            0.003939  0.000020\n",
       "158  0.003951                0.003951  0.003938            0.003938  0.000020\n",
       "159  0.003949                0.003949  0.003936            0.003936  0.000020\n",
       "160  0.003944                0.003944  0.003930            0.003930  0.000002\n",
       "161  0.003943                0.003943  0.003929            0.003929  0.000002\n",
       "162  0.003944                0.003944  0.003929            0.003929  0.000002\n",
       "163  0.003943                0.003943  0.003929            0.003929  0.000002\n",
       "164  0.003943                0.003943  0.003929            0.003929  0.000002\n",
       "\n",
       "[165 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "cpm.run_training(save_history = True, \n",
    "                 warm_start = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
