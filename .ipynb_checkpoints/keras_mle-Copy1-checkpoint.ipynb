{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "import make_data_wfpt as mdw\n",
    "from kde_training_utilities import kde_load_data\n",
    "import ddm_data_simulation as ddm_sim\n",
    "import boundary_functions as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14367486092535956245\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9336725846307428806\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12048773940\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14586086645395845906\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:82:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1794647537348635113\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Handle some cuda business\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f9560f0db38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "model_path = '/media/data_cifs/afengler/data/kde/linear_collapse/keras_models/dnnregressor_ddm_linear_collapse_06_22_19_23_27_28/model_0' \n",
    "ckpt_path = '/media/data_cifs/afengler/data/kde/linear_collapse/keras_models/dnnregressor_ddm_linear_collapse_06_22_19_23_27_28/ckpt_0_130'\n",
    "\n",
    "model = keras.models.load_model(model_path)\n",
    "model.load_weights(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 20)                160       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 60)                2460      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 80)                4880      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               8100      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 120)               12120     \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 28,681\n",
      "Trainable params: 28,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations -----\n",
    "n_runs = 20\n",
    "n_samples = 2500\n",
    "feature_file_path = '/media/data_cifs/afengler/data/kde/linear_collapse/train_test_data/test_features.pickle'\n",
    "mle_out_path = '/media/data_cifs/afengler/data/kde/linear_collapse/mle_runs'\n",
    "\n",
    "# NOTE PARAMETERS: WEIBULL: [v, a, w, node, shape, scale]\n",
    "param_bounds = [(-1, 1), (0.3, 2), (0.3, 0.7), (0, 1), (0, np.pi / 2.2)]\n",
    "\n",
    "\n",
    "my_optim_columns = ['v_sim', 'a_sim', 'w_sim', 'node_sim', 'theta_sim',\n",
    "                    'v_mle', 'a_mle', 'w_mle', 'node_mle', 'theta_mle', 'n_samples']\n",
    "\n",
    "# Get parameter names in correct ordering:\n",
    "dat = pickle.load(open(feature_file_path, \n",
    "                       'rb'))\n",
    "\n",
    "parameter_names = list(dat.keys())[:-2] # :-1 to get rid of 'rt' and 'choice' here\n",
    "\n",
    "# Make columns for optimizer result table\n",
    "p_sim = []\n",
    "p_mle = []\n",
    "\n",
    "for parameter_name in parameter_names:\n",
    "    p_sim.append(parameter_name + '_sim')\n",
    "    p_mle.append(parameter_name + '_mle')\n",
    "    \n",
    "my_optim_columns = p_sim + p_mle + ['n_samples']\n",
    "\n",
    "# Initialize the data frame in which to store optimizer results\n",
    "optim_results = pd.DataFrame(np.zeros((n_runs, len(my_optim_columns))), columns = my_optim_columns)\n",
    "optim_results.iloc[:, 2 * len(parameter_names)] = n_samples\n",
    "\n",
    "# define boundary\n",
    "boundary = bf.linear_collapse\n",
    "boundary_multiplicative = False\n",
    "\n",
    "# Define the likelihood function\n",
    "def log_p(params = [0, 1, 0.9], model = [], data = [], parameter_names = []):\n",
    "    # Make feature array\n",
    "    feature_array = np.zeros((data[0].shape[0], len(parameter_names) + 2))\n",
    "    \n",
    "    # Store parameters\n",
    "    cnt = 0\n",
    "    for i in range(0, len(parameter_names), 1):\n",
    "        feature_array[:, i] = params[i]\n",
    "        cnt += 1\n",
    "    \n",
    "    # Store rts and choices\n",
    "    feature_array[:, cnt] = data[0].ravel() # rts\n",
    "    feature_array[:, cnt + 1] = data[1].ravel() # choices\n",
    "    \n",
    "    # Get model predictions\n",
    "    prediction = model.predict(feature_array)\n",
    "    \n",
    "    # Some post-processing of predictions\n",
    "    prediction[prediction < 1e-29] = 1e-29\n",
    "    \n",
    "    return(- np.sum(np.log(prediction)))  \n",
    "\n",
    "def make_params(param_bounds = []):\n",
    "    params = np.zeros(len(param_bounds))\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        params[i] = np.random.uniform(low = param_bounds[i][0], high = param_bounds[i][1])\n",
    "        \n",
    "    return params\n",
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for run 0: \n",
      "[0.23569489 0.73148885 0.52230435 0.20009891 0.1871223 ]\n",
      "0  datapoints sampled\n",
      "1000  datapoints sampled\n",
      "2000  datapoints sampled\n",
      "Mean rt for current run: \n",
      "0.47322039999999854\n",
      "differential_evolution step 1: f(x)= 1828.67\n",
      "differential_evolution step 2: f(x)= 1820.82\n",
      "differential_evolution step 3: f(x)= 1807.67\n",
      "differential_evolution step 4: f(x)= 1781.4\n",
      "differential_evolution step 5: f(x)= 1781.4\n",
      "differential_evolution step 6: f(x)= 1739.26\n",
      "differential_evolution step 7: f(x)= 1730.68\n",
      "differential_evolution step 8: f(x)= 1730.68\n",
      "differential_evolution step 9: f(x)= 1730.68\n",
      "differential_evolution step 10: f(x)= 1730.68\n",
      "differential_evolution step 11: f(x)= 1730.68\n",
      "differential_evolution step 12: f(x)= 1730.68\n",
      "differential_evolution step 13: f(x)= 1730.68\n",
      "differential_evolution step 14: f(x)= 1730.68\n",
      "differential_evolution step 15: f(x)= 1730.68\n",
      "differential_evolution step 16: f(x)= 1716.8\n",
      "differential_evolution step 17: f(x)= 1716.8\n",
      "differential_evolution step 18: f(x)= 1716.8\n",
      "differential_evolution step 19: f(x)= 1716.8\n",
      "differential_evolution step 20: f(x)= 1716.8\n",
      "differential_evolution step 21: f(x)= 1716.29\n",
      "differential_evolution step 22: f(x)= 1716.29\n",
      "differential_evolution step 23: f(x)= 1715.18\n",
      "Solution vector of current run: \n",
      "[0.20859771 0.68945082 0.5225361  0.49714775 0.72169437]\n",
      "The run took: \n",
      "00:05:16\n",
      "Parameters for run 1: \n",
      "[-0.7406153   1.56617086  0.5689229   0.89325926  0.36317854]\n",
      "0  datapoints sampled\n",
      "1000  datapoints sampled\n",
      "2000  datapoints sampled\n",
      "Mean rt for current run: \n",
      "1.4664803999999398\n",
      "differential_evolution step 1: f(x)= 3543.55\n",
      "differential_evolution step 2: f(x)= 3540.55\n",
      "differential_evolution step 3: f(x)= 3534.3\n",
      "differential_evolution step 4: f(x)= 3534.3\n",
      "differential_evolution step 5: f(x)= 3534.3\n",
      "differential_evolution step 6: f(x)= 3513.18\n",
      "differential_evolution step 7: f(x)= 3513.12\n",
      "differential_evolution step 8: f(x)= 3499.23\n",
      "differential_evolution step 9: f(x)= 3499.23\n",
      "differential_evolution step 10: f(x)= 3499.23\n",
      "differential_evolution step 11: f(x)= 3499.23\n",
      "differential_evolution step 12: f(x)= 3475.48\n",
      "differential_evolution step 13: f(x)= 3475.48\n",
      "differential_evolution step 14: f(x)= 3475.48\n",
      "differential_evolution step 15: f(x)= 3475.48\n",
      "Solution vector of current run: \n",
      "[-0.99187753  1.6108008   0.56730163  0.60950702  0.3345519 ]\n",
      "The run took: \n",
      "00:04:41\n",
      "Parameters for run 2: \n",
      "[-0.1238494   0.56749652  0.5267436   0.71332224  1.40283067]\n",
      "0  datapoints sampled\n",
      "1000  datapoints sampled\n",
      "2000  datapoints sampled\n",
      "Mean rt for current run: \n",
      "0.31972400000000023\n",
      "differential_evolution step 1: f(x)= 936.715\n",
      "differential_evolution step 2: f(x)= 933.543\n",
      "differential_evolution step 3: f(x)= 890.755\n",
      "differential_evolution step 4: f(x)= 872.504\n",
      "differential_evolution step 5: f(x)= 872.504\n",
      "differential_evolution step 6: f(x)= 866.873\n",
      "differential_evolution step 7: f(x)= 864.608\n",
      "differential_evolution step 8: f(x)= 858.512\n",
      "differential_evolution step 9: f(x)= 858.512\n",
      "differential_evolution step 10: f(x)= 848.348\n",
      "differential_evolution step 11: f(x)= 848.348\n",
      "differential_evolution step 12: f(x)= 848.348\n",
      "differential_evolution step 13: f(x)= 848.348\n",
      "differential_evolution step 14: f(x)= 848.342\n",
      "differential_evolution step 15: f(x)= 848.342\n",
      "differential_evolution step 16: f(x)= 848.342\n",
      "differential_evolution step 17: f(x)= 847.266\n",
      "Solution vector of current run: \n",
      "[0.0111067  0.47028026 0.55404453 0.88291094 0.04684978]\n",
      "The run took: \n",
      "00:04:00\n",
      "Parameters for run 3: \n",
      "[0.43305389 1.47778906 0.35907017 0.30571329 1.27795318]\n",
      "0  datapoints sampled\n",
      "1000  datapoints sampled\n",
      "2000  datapoints sampled\n",
      "Mean rt for current run: \n",
      "0.5451712000000004\n",
      "differential_evolution step 1: f(x)= -103.754\n",
      "differential_evolution step 2: f(x)= -103.754\n",
      "differential_evolution step 3: f(x)= -103.754\n",
      "differential_evolution step 4: f(x)= -264.626\n",
      "differential_evolution step 5: f(x)= -264.626\n",
      "differential_evolution step 6: f(x)= -264.626\n",
      "differential_evolution step 7: f(x)= -264.626\n",
      "differential_evolution step 8: f(x)= -292.414\n",
      "differential_evolution step 9: f(x)= -311.372\n",
      "differential_evolution step 10: f(x)= -319.102\n",
      "differential_evolution step 11: f(x)= -395.917\n",
      "differential_evolution step 12: f(x)= -396.666\n",
      "differential_evolution step 13: f(x)= -396.666\n",
      "differential_evolution step 14: f(x)= -396.666\n",
      "differential_evolution step 15: f(x)= -396.666\n",
      "differential_evolution step 16: f(x)= -396.666\n",
      "differential_evolution step 17: f(x)= -416.323\n",
      "differential_evolution step 18: f(x)= -416.323\n",
      "differential_evolution step 19: f(x)= -429.236\n",
      "differential_evolution step 20: f(x)= -429.236\n",
      "differential_evolution step 21: f(x)= -429.236\n",
      "differential_evolution step 22: f(x)= -429.236\n",
      "differential_evolution step 23: f(x)= -429.236\n",
      "differential_evolution step 24: f(x)= -429.236\n",
      "differential_evolution step 25: f(x)= -429.236\n",
      "differential_evolution step 26: f(x)= -429.236\n",
      "differential_evolution step 27: f(x)= -429.236\n",
      "differential_evolution step 28: f(x)= -436.282\n",
      "differential_evolution step 29: f(x)= -438.867\n",
      "differential_evolution step 30: f(x)= -438.867\n",
      "differential_evolution step 31: f(x)= -440.281\n",
      "differential_evolution step 32: f(x)= -442.694\n",
      "differential_evolution step 33: f(x)= -443.005\n",
      "differential_evolution step 34: f(x)= -443.445\n",
      "differential_evolution step 35: f(x)= -443.445\n",
      "differential_evolution step 36: f(x)= -443.445\n",
      "differential_evolution step 37: f(x)= -443.445\n",
      "differential_evolution step 38: f(x)= -445.298\n",
      "differential_evolution step 39: f(x)= -446.002\n"
     ]
    }
   ],
   "source": [
    "# Main loop ----------- TD: Parallelize\n",
    "for i in range(0, n_runs, 1): \n",
    "    \n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "#     # Sample parameters\n",
    "#     v_sim = np.random.uniform(high = v_range[1], low = v_range[0])\n",
    "#     a_sim = np.random.uniform(high = a_range[1], low = a_range[0])\n",
    "#     w_sim = np.random.uniform(high = w_range[1], low = w_range[0])\n",
    "\n",
    "#     #c1_sim = np.random.uniform(high = c1_range[1], low = c1_range[0])\n",
    "#     #c2_sim = np.random.uniform(high = c2_range[1], low = c2_range[0])\n",
    "#     node_sim = np.random.uniform(high = node_range[1], low = node_range[0])\n",
    "#     shape_sim = np.random.uniform(high = shape_range[1], low = shape_range[0])\n",
    "#     scale_sim = np.random.uniform(high = scale_range[1], low = scale_range[0])\n",
    "\n",
    "    tmp_params = make_params(param_bounds = param_bounds)\n",
    "    \n",
    "    # Store in output file\n",
    "    optim_results.iloc[i, :len(parameter_names)] = tmp_params\n",
    "    \n",
    "    # Print some info on run\n",
    "    print('Parameters for run ' + str(i) + ': ')\n",
    "    print(tmp_params)\n",
    "    \n",
    "    # Define boundary params\n",
    "    boundary_params = {'node': tmp_params[3],\n",
    "                       'theta': tmp_params[4]}\n",
    "    \n",
    "    # Run model simulations\n",
    "    ddm_dat_tmp = ddm_sim.ddm_flexbound_simulate(v = tmp_params[0],\n",
    "                                                 a = tmp_params[1],\n",
    "                                                 w = tmp_params[2],\n",
    "                                                 s = 1,\n",
    "                                                 delta_t = 0.001,\n",
    "                                                 max_t = 20,\n",
    "                                                 n_samples = n_samples,\n",
    "                                                 boundary_fun = boundary, # function of t (and potentially other parameters) that takes in (t, *args)\n",
    "                                                 boundary_multiplicative = boundary_multiplicative, # CAREFUL: CHECK IF BOUND\n",
    "                                                 boundary_params = boundary_params)\n",
    "        \n",
    "    # Print some info on run\n",
    "    print('Mean rt for current run: ')\n",
    "    print(np.mean(ddm_dat_tmp[0]))\n",
    "    \n",
    "    # Run optimizer\n",
    "    out = differential_evolution(log_p, \n",
    "                                 bounds = param_bounds, \n",
    "                                 args = (model, ddm_dat_tmp, parameter_names), \n",
    "                                 popsize = 30,\n",
    "                                 disp = True)\n",
    "    \n",
    "    # Print some info\n",
    "    print('Solution vector of current run: ')\n",
    "    print(out.x)\n",
    "    \n",
    "    print('The run took: ')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    \n",
    "    # Store result in output file\n",
    "    optim_results.iloc[i, len(parameter_names):(2*len(parameter_names))] = out.x\n",
    "# -----------------------\n",
    "\n",
    "# Save optimization results to file\n",
    "optim_results.to_csv(mle_out_path + '/mle_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results\n",
    "optim_results = pd.read_csv(os.getcwd() + '/experiments/ddm_flexbound_kde_mle_fix_v_0_c1_0_w_unbiased_arange_2_3/optim_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['v_sim'], optim_results['v_mle'], c = optim_results['c2_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for v\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['v_mle'], 1), np.expand_dims(optim_results['v_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['v_mle'], 1), np.expand_dims(optim_results['v_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['a_sim'], optim_results['a_mle'], c = optim_results['c2_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for a\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['a_mle'], 1), np.expand_dims(optim_results['a_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['a_mle'], 1), np.expand_dims(optim_results['a_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['w_sim'], optim_results['w_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for w\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['w_mle'], 1), np.expand_dims(optim_results['w_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['w_mle'], 1), np.expand_dims(optim_results['w_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['c1_sim'], optim_results['c1_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for c1\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['c1_mle'], 1), np.expand_dims(optim_results['c1_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['c1_mle'], 1), np.expand_dims(optim_results['c1_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['c2_sim'], optim_results['c2_mle'], c = optim_results['a_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for w\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['c2_mle'], 1), np.expand_dims(optim_results['c2_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['c2_mle'], 1), np.expand_dims(optim_results['c2_sim'], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
