{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from cdwiener import array_fptd\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import yaml\n",
    "import keras_to_numpy as ktnp\n",
    "\n",
    "from kde_training_utilities import kde_load_data\n",
    "from kde_training_utilities import kde_make_train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if it does not exist, make model path\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4895160547223233940\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6238924222414981622\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12048649421\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10671409753630405408\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 13611563358073070144\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "loading data.... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afengler/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# CHOOSE ---------\n",
    "method = \"weibull_cdf_ndt\" # ddm, linear_collapse, ornstein, full, lba\n",
    "machine = 'x7'\n",
    "# ----------------\n",
    "\n",
    "# INITIALIZATIONS ----------------------------------------------------------------\n",
    "stats = pickle.load(open(\"kde_stats.pickle\", \"rb\"))[method]\n",
    "dnn_params = yaml.load(open(\"hyperparameters.yaml\"))\n",
    "\n",
    "if machine == 'x7':\n",
    "    data_folder = stats[\"data_folder_x7\"]\n",
    "    model_path = stats[\"model_folder_x7\"]\n",
    "else:\n",
    "    data_folder = stats[\"data_folder\"]\n",
    "    model_path = stats[\"model_folder\"]\n",
    "    \n",
    "model_path += dnn_params[\"model_type\"] + \"_{}_\".format(method) + datetime.now().strftime('%m_%d_%y_%H_%M_%S') + \"/\"\n",
    "\n",
    "print('if it does not exist, make model path')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "# Copy hyperparameter setup into model path\n",
    "if machine == 'x7':\n",
    "    os.system(\"cp {} {}\".format(\"/media/data_cifs/afengler/git_repos/nn_likelihoods/hyperparameters.yaml\", model_path))\n",
    "else:\n",
    "    os.system(\"cp {} {}\".format(\"/users/afengler/git_repos/nn_likelihoods/hyperparameters.yaml\", model_path))\n",
    "    \n",
    "# set up gpu to use\n",
    "if machine == 'x7':\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = dnn_params['gpu_x7'] \n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# Load the training data\n",
    "print('loading data.... ')\n",
    "\n",
    "# X, y, X_val, y_val = kde_load_data(folder = data_folder, \n",
    "#                                    return_log = True, # Dont take log if you want to train on actual likelihoods\n",
    "#                                    prelog_cutoff = 1e-7 # cut out data with likelihood lower than 1e-7\n",
    "#                                   )\n",
    "\n",
    "# X = np.array(X)\n",
    "# X_val = np.array(X_val)\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 13:58:10.719720 140547928127232 deprecation.py:506] From /home/afengler/miniconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up keras model\n",
      "STRUCTURE OF GENERATED MODEL: ....\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               700       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               12120     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 23,041\n",
      "Trainable params: 23,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1029 13:58:10.941697 140547928127232 deprecation.py:323] From /home/afengler/miniconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# MAKE MODEL ---------------------------------------------------------------------\n",
    "print('Setting up keras model')\n",
    "\n",
    "input_shape = 6 #X.shape[1]\n",
    "model = keras.Sequential()\n",
    "\n",
    "for i in range(len(dnn_params['hidden_layers'])):\n",
    "    if i == 0:\n",
    "        model.add(keras.layers.Dense(units = dnn_params[\"hidden_layers\"][i], \n",
    "                                     activation = dnn_params[\"hidden_activations\"][i], \n",
    "                                     input_dim = input_shape))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(units = dnn_params[\"hidden_layers\"][i],\n",
    "                                     activation = dnn_params[\"hidden_activations\"][i]))\n",
    "        \n",
    "# Write model specification to yaml file        \n",
    "spec = model.to_yaml()\n",
    "open(model_path + \"model_spec.yaml\", \"w\").write(spec)\n",
    "\n",
    "\n",
    "print('STRUCTURE OF GENERATED MODEL: ....')\n",
    "print(model.summary())\n",
    "\n",
    "if dnn_params['loss'] == 'huber':\n",
    "    model.compile(loss = tf.losses.huber_loss, \n",
    "                  optimizer = \"adam\", \n",
    "                  metrics = [\"mse\"])\n",
    "\n",
    "if dnn_params['loss'] == 'mse':\n",
    "    model.compile(loss = 'mse', \n",
    "                  optimizer = \"adam\", \n",
    "                  metrics = [\"mse\"])\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT MODEL -----------------------------------------------------------------\n",
    "print('Starting to fit model.....')\n",
    "\n",
    "# Define callbacks\n",
    "ckpt_filename = model_path + \"model.h5\"\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(ckpt_filename, \n",
    "                                             monitor = 'val_loss', \n",
    "                                             verbose = 1, \n",
    "                                             save_best_only = False)\n",
    "                               \n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                              min_delta = 0, \n",
    "                                              verbose = 1, \n",
    "                                              patience = 2)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                                              factor = 0.1,\n",
    "                                              patience = 1, \n",
    "                                              verbose = 1,\n",
    "                                              min_delta = 0.0001,\n",
    "                                              min_lr = 0.0000001)\n",
    "\n",
    "history = model.fit(X, y, \n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs = dnn_params[\"n_epochs\"],\n",
    "                    batch_size = dnn_params[\"batch_size\"], \n",
    "                    callbacks = [checkpoint, reduce_lr, earlystopping], \n",
    "                    verbose = 2)\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_samples(path):\n",
    "#     while True:\n",
    "#         files_ = os.listdir(path)\n",
    "#         files_ = np.random.permutation(files_)\n",
    "#         file in files_:\n",
    "#             with open(path + files) as f:\n",
    "#                 data = pickle.load(f, 'rb')\n",
    "#                 np.random.shuffle(data.values)\n",
    "#                 data.reset_index(drop = True, inplace = True)\n",
    "#                 n = data.shape[0]\n",
    "#                 n_cols = data_shape[1]\n",
    "#                 batch_size = 1024\n",
    "#                 i = 0\n",
    "#                 while (i * batch_size < (n - batch_size)):\n",
    "#                     yield (data.iloc[(i * batch_size): ((i + 1) * batch_size )), :(n_cols - 1)], \n",
    "#                            data.iloc[(i * batch_size): ((i + 1) * batch_size )), (n_cols - 1)])\n",
    "                \n",
    "                \n",
    "def generate_samples(data = []):\n",
    "    n = data.shape[0]\n",
    "    n_cols = data.shape[1]\n",
    "    batch_size = 1024\n",
    "    while True:\n",
    "        i = 0\n",
    "        while (i * batch_size < (n - batch_size)):\n",
    "            yield (data.iloc[(i * batch_size): ((i + 1) * batch_size ), :(n_cols - 1)].to_numpy(), \n",
    "                   np.expand_dims(data.iloc[(i * batch_size): ((i + 1) * batch_size ), (n_cols - 1)].to_numpy() , axis = 1))\n",
    "\n",
    "\n",
    "# def generate_arrays_from_file(path):\n",
    "#     while True:\n",
    "#         with open(path) as f:\n",
    "#             for line in f:\n",
    "#                 # create numpy arrays of input data\n",
    "#                 # and labels, from each line in the file\n",
    "#                 x1, x2, y = process_line(line)\n",
    "#                 yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "\n",
    "# model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
    "#                     steps_per_epoch=10000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SAVING --------------------------------------------------------------------\n",
    "# print('Saving model and relevant data...')\n",
    "# # Log of training output\n",
    "# pd.DataFrame(history.history).to_csv(model_path + \"training_history.csv\")\n",
    "\n",
    "# # Save Model\n",
    "# model.save(model_path + \"model_final.h5\")\n",
    "\n",
    "# # Extract model architecture as numpy arrays and save in model path\n",
    "# __, ___, ____, = ktnp.extract_architecture(model, save = True, save_path = model_path)\n",
    "\n",
    "# # Update model paths in model_path.yaml\n",
    "# model_paths = yaml.load(open(\"model_paths.yaml\"))\n",
    "# model_paths[method] = model_path\n",
    "# yaml.dump(model_paths, open(\"model_paths.yaml\", \"w\"))\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pickle.load(open('/media/data_cifs/afengler/data/kde/' + \\\n",
    "                     'weibull_cdf/base_simulations_ndt_20000/weibull_cdf_ndt_base_simulations_24.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(np.random.uniform((100000, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = generate_samples(data = t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = next(my_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 9)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[1].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(t2[1].to_numpy(), axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
