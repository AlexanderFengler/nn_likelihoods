{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from cdwiener import array_fptd\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import yaml\n",
    "import keras_to_numpy as ktnp\n",
    "\n",
    "from kde_training_utilities import kde_load_data\n",
    "from kde_training_utilities import kde_make_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afengler/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if it does not exist, make model path\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1043153159348957180\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 14641980330218019517\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12048649421\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1268403537854229946\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 99138637536522749\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n",
      "loading data.... \n"
     ]
    }
   ],
   "source": [
    "# CHOOSE ---------\n",
    "method = \"weibull_cdf_ndt\" # ddm, linear_collapse, ornstein, full, lba\n",
    "machine = 'x7'\n",
    "# ----------------\n",
    "\n",
    "# INITIALIZATIONS ----------------------------------------------------------------\n",
    "stats = pickle.load(open(\"kde_stats.pickle\", \"rb\"))[method]\n",
    "dnn_params = yaml.load(open(\"hyperparameters.yaml\"))\n",
    "\n",
    "if machine == 'x7':\n",
    "    data_folder = stats[\"data_folder_x7\"]\n",
    "    model_path = stats[\"model_folder_x7\"]\n",
    "else:\n",
    "    data_folder = stats[\"data_folder\"]\n",
    "    model_path = stats[\"model_folder\"]\n",
    "    \n",
    "model_path += dnn_params[\"model_type\"] + \"_{}_\".format(method) + datetime.now().strftime('%m_%d_%y_%H_%M_%S') + \"/\"\n",
    "\n",
    "print('if it does not exist, make model path')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "# Copy hyperparameter setup into model path\n",
    "if machine == 'x7':\n",
    "    os.system(\"cp {} {}\".format(\"/media/data_cifs/afengler/git_repos/nn_likelihoods/hyperparameters.yaml\", model_path))\n",
    "else:\n",
    "    os.system(\"cp {} {}\".format(\"/users/afengler/git_repos/nn_likelihoods/hyperparameters.yaml\", model_path))\n",
    "    \n",
    "# set up gpu to use\n",
    "if machine == 'x7':\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = dnn_params['gpu_x7'] \n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# Load the training data\n",
    "print('loading data.... ')\n",
    "\n",
    "# X, y, X_val, y_val = kde_load_data(folder = data_folder, \n",
    "#                                    return_log = True, # Dont take log if you want to train on actual likelihoods\n",
    "#                                    prelog_cutoff = 1e-7 # cut out data with likelihood lower than 1e-7\n",
    "#                                   )\n",
    "\n",
    "# X = np.array(X)\n",
    "# X_val = np.array(X_val)\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1029 15:20:36.278412 140113072776960 deprecation.py:506] From /home/afengler/miniconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up keras model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1029 15:20:39.191444 140113072776960 deprecation.py:323] From /home/afengler/miniconda3/envs/pytorch/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRUCTURE OF GENERATED MODEL: ....\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               700       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               12120     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 23,041\n",
      "Trainable params: 23,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# MAKE MODEL ---------------------------------------------------------------------\n",
    "print('Setting up keras model')\n",
    "\n",
    "input_shape = 6 #X.shape[1]\n",
    "model = keras.Sequential()\n",
    "\n",
    "for i in range(len(dnn_params['hidden_layers'])):\n",
    "    if i == 0:\n",
    "        model.add(keras.layers.Dense(units = dnn_params[\"hidden_layers\"][i], \n",
    "                                     activation = dnn_params[\"hidden_activations\"][i], \n",
    "                                     input_dim = input_shape))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(units = dnn_params[\"hidden_layers\"][i],\n",
    "                                     activation = dnn_params[\"hidden_activations\"][i]))\n",
    "        \n",
    "# Write model specification to yaml file        \n",
    "spec = model.to_yaml()\n",
    "open(model_path + \"model_spec.yaml\", \"w\").write(spec)\n",
    "\n",
    "\n",
    "print('STRUCTURE OF GENERATED MODEL: ....')\n",
    "print(model.summary())\n",
    "\n",
    "if dnn_params['loss'] == 'huber':\n",
    "    model.compile(loss = tf.losses.huber_loss, \n",
    "                  optimizer = \"adam\", \n",
    "                  metrics = [\"mse\"])\n",
    "\n",
    "if dnn_params['loss'] == 'mse':\n",
    "    model.compile(loss = 'mse', \n",
    "                  optimizer = \"adam\", \n",
    "                  metrics = [\"mse\"])\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT MODEL -----------------------------------------------------------------\n",
    "print('Starting to fit model.....')\n",
    "\n",
    "# Define callbacks\n",
    "ckpt_filename = model_path + \"model.h5\"\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(ckpt_filename, \n",
    "                                             monitor = 'val_loss', \n",
    "                                             verbose = 1, \n",
    "                                             save_best_only = False)\n",
    "                               \n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                              min_delta = 0, \n",
    "                                              verbose = 1, \n",
    "                                              patience = 2)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                                              factor = 0.1,\n",
    "                                              patience = 1, \n",
    "                                              verbose = 1,\n",
    "                                              min_delta = 0.0001,\n",
    "                                              min_lr = 0.0000001)\n",
    "\n",
    "history = model.fit(X, y, \n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs = dnn_params[\"n_epochs\"],\n",
    "                    batch_size = dnn_params[\"batch_size\"], \n",
    "                    callbacks = [checkpoint, reduce_lr, earlystopping], \n",
    "                    verbose = 2)\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_samples(path):\n",
    "#     while True:\n",
    "#         files_ = os.listdir(path)\n",
    "#         files_ = np.random.permutation(files_)\n",
    "#         file in files_:\n",
    "#             with open(path + files) as f:\n",
    "#                 data = pickle.load(f, 'rb')\n",
    "#                 np.random.shuffle(data.values)\n",
    "#                 data.reset_index(drop = True, inplace = True)\n",
    "#                 n = data.shape[0]\n",
    "#                 n_cols = data_shape[1]\n",
    "#                 batch_size = 1024\n",
    "#                 i = 0\n",
    "#                 while (i * batch_size < (n - batch_size)):\n",
    "#                     yield (data.iloc[(i * batch_size): ((i + 1) * batch_size )), :(n_cols - 1)], \n",
    "#                            data.iloc[(i * batch_size): ((i + 1) * batch_size )), (n_cols - 1)])\n",
    "                \n",
    "                \n",
    "def generate_samples(data = []):\n",
    "    n = data.shape[0]\n",
    "    n_cols = data.shape[1]\n",
    "    batch_size = 1024\n",
    "    while True:\n",
    "        i = 0\n",
    "        while (i * batch_size < (n - batch_size)):\n",
    "            yield (data.iloc[(i * batch_size): ((i + 1) * batch_size ), :(n_cols - 1)].to_numpy(), \n",
    "                   np.expand_dims(data.iloc[(i * batch_size): ((i + 1) * batch_size ), (n_cols - 1)].to_numpy() , axis = 1))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "# def generate_arrays_from_file(path):\n",
    "#     while True:\n",
    "#         with open(path) as f:\n",
    "#             for line in f:\n",
    "#                 # create numpy arrays of input data\n",
    "#                 # and labels, from each line in the file\n",
    "#                 x1, x2, y = process_line(line)\n",
    "#                 yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "\n",
    "# model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
    "#                     steps_per_epoch=10000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SAVING --------------------------------------------------------------------\n",
    "# print('Saving model and relevant data...')\n",
    "# # Log of training output\n",
    "# pd.DataFrame(history.history).to_csv(model_path + \"training_history.csv\")\n",
    "\n",
    "# # Save Model\n",
    "# model.save(model_path + \"model_final.h5\")\n",
    "\n",
    "# # Extract model architecture as numpy arrays and save in model path\n",
    "# __, ___, ____, = ktnp.extract_architecture(model, save = True, save_path = model_path)\n",
    "\n",
    "# # Update model paths in model_path.yaml\n",
    "# model_paths = yaml.load(open(\"model_paths.yaml\"))\n",
    "# model_paths[method] = model_path\n",
    "# yaml.dump(model_paths, open(\"model_paths.yaml\", \"w\"))\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pickle.load(open('/media/data_cifs/afengler/data/kde/' + \\\n",
    "                     'weibull_cdf/train_test_data_ndt/weibull_cdf_ndt_base_simulations_24.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(np.random.uniform(size = (100000, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = generate_samples(data = t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = next(my_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[0].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[1].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(t2[1].to_numpy(), axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.205961</td>\n",
       "      <td>0.752730</td>\n",
       "      <td>0.793832</td>\n",
       "      <td>0.931900</td>\n",
       "      <td>0.022322</td>\n",
       "      <td>0.560578</td>\n",
       "      <td>0.356262</td>\n",
       "      <td>0.222053</td>\n",
       "      <td>0.165362</td>\n",
       "      <td>0.946916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.546661</td>\n",
       "      <td>0.594928</td>\n",
       "      <td>0.678584</td>\n",
       "      <td>0.464412</td>\n",
       "      <td>0.367392</td>\n",
       "      <td>0.475764</td>\n",
       "      <td>0.124166</td>\n",
       "      <td>0.889422</td>\n",
       "      <td>0.201139</td>\n",
       "      <td>0.502694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.139117</td>\n",
       "      <td>0.904341</td>\n",
       "      <td>0.392153</td>\n",
       "      <td>0.040919</td>\n",
       "      <td>0.776070</td>\n",
       "      <td>0.862407</td>\n",
       "      <td>0.366219</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>0.666566</td>\n",
       "      <td>0.380420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267858</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0.307106</td>\n",
       "      <td>0.408035</td>\n",
       "      <td>0.553078</td>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.910081</td>\n",
       "      <td>0.284892</td>\n",
       "      <td>0.620891</td>\n",
       "      <td>0.894443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.678581</td>\n",
       "      <td>0.488243</td>\n",
       "      <td>0.380064</td>\n",
       "      <td>0.348321</td>\n",
       "      <td>0.531539</td>\n",
       "      <td>0.389896</td>\n",
       "      <td>0.155341</td>\n",
       "      <td>0.463131</td>\n",
       "      <td>0.686823</td>\n",
       "      <td>0.653478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.181558</td>\n",
       "      <td>0.457969</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.361580</td>\n",
       "      <td>0.985968</td>\n",
       "      <td>0.168864</td>\n",
       "      <td>0.827005</td>\n",
       "      <td>0.820099</td>\n",
       "      <td>0.559832</td>\n",
       "      <td>0.144888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.117905</td>\n",
       "      <td>0.179908</td>\n",
       "      <td>0.634849</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.306602</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.210654</td>\n",
       "      <td>0.211758</td>\n",
       "      <td>0.890701</td>\n",
       "      <td>0.463268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.702963</td>\n",
       "      <td>0.034494</td>\n",
       "      <td>0.864116</td>\n",
       "      <td>0.706103</td>\n",
       "      <td>0.307866</td>\n",
       "      <td>0.928858</td>\n",
       "      <td>0.600351</td>\n",
       "      <td>0.663327</td>\n",
       "      <td>0.865569</td>\n",
       "      <td>0.534681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.983945</td>\n",
       "      <td>0.278483</td>\n",
       "      <td>0.766428</td>\n",
       "      <td>0.019837</td>\n",
       "      <td>0.254173</td>\n",
       "      <td>0.345996</td>\n",
       "      <td>0.210059</td>\n",
       "      <td>0.073721</td>\n",
       "      <td>0.347150</td>\n",
       "      <td>0.569863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.318207</td>\n",
       "      <td>0.235657</td>\n",
       "      <td>0.117392</td>\n",
       "      <td>0.629639</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.098106</td>\n",
       "      <td>0.808621</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>0.642748</td>\n",
       "      <td>0.428816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.994832</td>\n",
       "      <td>0.579945</td>\n",
       "      <td>0.937188</td>\n",
       "      <td>0.960419</td>\n",
       "      <td>0.874817</td>\n",
       "      <td>0.378723</td>\n",
       "      <td>0.366119</td>\n",
       "      <td>0.060426</td>\n",
       "      <td>0.762179</td>\n",
       "      <td>0.419102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.272776</td>\n",
       "      <td>0.933193</td>\n",
       "      <td>0.225498</td>\n",
       "      <td>0.753867</td>\n",
       "      <td>0.019637</td>\n",
       "      <td>0.302532</td>\n",
       "      <td>0.672089</td>\n",
       "      <td>0.968116</td>\n",
       "      <td>0.106582</td>\n",
       "      <td>0.662339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.090779</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.528715</td>\n",
       "      <td>0.625381</td>\n",
       "      <td>0.412331</td>\n",
       "      <td>0.463272</td>\n",
       "      <td>0.680353</td>\n",
       "      <td>0.963150</td>\n",
       "      <td>0.369255</td>\n",
       "      <td>0.559434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.627415</td>\n",
       "      <td>0.793536</td>\n",
       "      <td>0.744582</td>\n",
       "      <td>0.336845</td>\n",
       "      <td>0.578582</td>\n",
       "      <td>0.337614</td>\n",
       "      <td>0.243639</td>\n",
       "      <td>0.849945</td>\n",
       "      <td>0.034087</td>\n",
       "      <td>0.195536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.402801</td>\n",
       "      <td>0.696428</td>\n",
       "      <td>0.890732</td>\n",
       "      <td>0.392844</td>\n",
       "      <td>0.136690</td>\n",
       "      <td>0.989814</td>\n",
       "      <td>0.298524</td>\n",
       "      <td>0.218205</td>\n",
       "      <td>0.159483</td>\n",
       "      <td>0.059446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.468181</td>\n",
       "      <td>0.695655</td>\n",
       "      <td>0.062253</td>\n",
       "      <td>0.340750</td>\n",
       "      <td>0.744692</td>\n",
       "      <td>0.621135</td>\n",
       "      <td>0.397867</td>\n",
       "      <td>0.210516</td>\n",
       "      <td>0.386057</td>\n",
       "      <td>0.156298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.199114</td>\n",
       "      <td>0.670631</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.154864</td>\n",
       "      <td>0.373549</td>\n",
       "      <td>0.345664</td>\n",
       "      <td>0.859116</td>\n",
       "      <td>0.858643</td>\n",
       "      <td>0.075119</td>\n",
       "      <td>0.227797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.271983</td>\n",
       "      <td>0.977070</td>\n",
       "      <td>0.752308</td>\n",
       "      <td>0.754374</td>\n",
       "      <td>0.961897</td>\n",
       "      <td>0.073666</td>\n",
       "      <td>0.495852</td>\n",
       "      <td>0.757288</td>\n",
       "      <td>0.166856</td>\n",
       "      <td>0.107706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.329352</td>\n",
       "      <td>0.996310</td>\n",
       "      <td>0.719027</td>\n",
       "      <td>0.941119</td>\n",
       "      <td>0.635347</td>\n",
       "      <td>0.765939</td>\n",
       "      <td>0.994667</td>\n",
       "      <td>0.202007</td>\n",
       "      <td>0.681813</td>\n",
       "      <td>0.401634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.345919</td>\n",
       "      <td>0.934395</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.830581</td>\n",
       "      <td>0.364326</td>\n",
       "      <td>0.661547</td>\n",
       "      <td>0.857579</td>\n",
       "      <td>0.299343</td>\n",
       "      <td>0.727708</td>\n",
       "      <td>0.083016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.135608</td>\n",
       "      <td>0.247532</td>\n",
       "      <td>0.372034</td>\n",
       "      <td>0.443752</td>\n",
       "      <td>0.813589</td>\n",
       "      <td>0.283779</td>\n",
       "      <td>0.728035</td>\n",
       "      <td>0.774027</td>\n",
       "      <td>0.678473</td>\n",
       "      <td>0.630954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.309997</td>\n",
       "      <td>0.074547</td>\n",
       "      <td>0.482073</td>\n",
       "      <td>0.807747</td>\n",
       "      <td>0.921738</td>\n",
       "      <td>0.190960</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.583913</td>\n",
       "      <td>0.333382</td>\n",
       "      <td>0.044302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.662282</td>\n",
       "      <td>0.037180</td>\n",
       "      <td>0.611326</td>\n",
       "      <td>0.985801</td>\n",
       "      <td>0.158272</td>\n",
       "      <td>0.243433</td>\n",
       "      <td>0.967082</td>\n",
       "      <td>0.395908</td>\n",
       "      <td>0.757042</td>\n",
       "      <td>0.258962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.686433</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.336747</td>\n",
       "      <td>0.853751</td>\n",
       "      <td>0.129311</td>\n",
       "      <td>0.464014</td>\n",
       "      <td>0.513921</td>\n",
       "      <td>0.443276</td>\n",
       "      <td>0.624436</td>\n",
       "      <td>0.267381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.846452</td>\n",
       "      <td>0.639110</td>\n",
       "      <td>0.249894</td>\n",
       "      <td>0.670798</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.668539</td>\n",
       "      <td>0.313684</td>\n",
       "      <td>0.553432</td>\n",
       "      <td>0.274983</td>\n",
       "      <td>0.628946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.822020</td>\n",
       "      <td>0.025795</td>\n",
       "      <td>0.357637</td>\n",
       "      <td>0.122199</td>\n",
       "      <td>0.577410</td>\n",
       "      <td>0.335401</td>\n",
       "      <td>0.219860</td>\n",
       "      <td>0.353884</td>\n",
       "      <td>0.054750</td>\n",
       "      <td>0.448671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.832589</td>\n",
       "      <td>0.451495</td>\n",
       "      <td>0.378683</td>\n",
       "      <td>0.412014</td>\n",
       "      <td>0.744574</td>\n",
       "      <td>0.883010</td>\n",
       "      <td>0.032333</td>\n",
       "      <td>0.266259</td>\n",
       "      <td>0.259209</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.429096</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.971395</td>\n",
       "      <td>0.037398</td>\n",
       "      <td>0.285666</td>\n",
       "      <td>0.239240</td>\n",
       "      <td>0.518312</td>\n",
       "      <td>0.878549</td>\n",
       "      <td>0.321848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.304074</td>\n",
       "      <td>0.130299</td>\n",
       "      <td>0.210665</td>\n",
       "      <td>0.388072</td>\n",
       "      <td>0.183173</td>\n",
       "      <td>0.255862</td>\n",
       "      <td>0.765204</td>\n",
       "      <td>0.724984</td>\n",
       "      <td>0.863846</td>\n",
       "      <td>0.142549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.667816</td>\n",
       "      <td>0.583901</td>\n",
       "      <td>0.290988</td>\n",
       "      <td>0.768389</td>\n",
       "      <td>0.240884</td>\n",
       "      <td>0.033877</td>\n",
       "      <td>0.931809</td>\n",
       "      <td>0.791880</td>\n",
       "      <td>0.888836</td>\n",
       "      <td>0.188053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>0.901299</td>\n",
       "      <td>0.084045</td>\n",
       "      <td>0.800021</td>\n",
       "      <td>0.989091</td>\n",
       "      <td>0.415169</td>\n",
       "      <td>0.521158</td>\n",
       "      <td>0.050280</td>\n",
       "      <td>0.780308</td>\n",
       "      <td>0.658084</td>\n",
       "      <td>0.774017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>0.106475</td>\n",
       "      <td>0.054606</td>\n",
       "      <td>0.764617</td>\n",
       "      <td>0.433854</td>\n",
       "      <td>0.735345</td>\n",
       "      <td>0.831844</td>\n",
       "      <td>0.703218</td>\n",
       "      <td>0.692005</td>\n",
       "      <td>0.452095</td>\n",
       "      <td>0.998338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>0.526424</td>\n",
       "      <td>0.826377</td>\n",
       "      <td>0.513169</td>\n",
       "      <td>0.039972</td>\n",
       "      <td>0.748886</td>\n",
       "      <td>0.842882</td>\n",
       "      <td>0.644847</td>\n",
       "      <td>0.676567</td>\n",
       "      <td>0.959793</td>\n",
       "      <td>0.109331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>0.650727</td>\n",
       "      <td>0.040572</td>\n",
       "      <td>0.316785</td>\n",
       "      <td>0.035805</td>\n",
       "      <td>0.848732</td>\n",
       "      <td>0.500766</td>\n",
       "      <td>0.589824</td>\n",
       "      <td>0.840883</td>\n",
       "      <td>0.262383</td>\n",
       "      <td>0.062775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>0.370358</td>\n",
       "      <td>0.872598</td>\n",
       "      <td>0.381020</td>\n",
       "      <td>0.947291</td>\n",
       "      <td>0.048454</td>\n",
       "      <td>0.741614</td>\n",
       "      <td>0.244881</td>\n",
       "      <td>0.401336</td>\n",
       "      <td>0.482101</td>\n",
       "      <td>0.185830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.154882</td>\n",
       "      <td>0.250283</td>\n",
       "      <td>0.012054</td>\n",
       "      <td>0.054436</td>\n",
       "      <td>0.903306</td>\n",
       "      <td>0.443816</td>\n",
       "      <td>0.173780</td>\n",
       "      <td>0.143478</td>\n",
       "      <td>0.527533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>0.582898</td>\n",
       "      <td>0.836153</td>\n",
       "      <td>0.609029</td>\n",
       "      <td>0.547384</td>\n",
       "      <td>0.975755</td>\n",
       "      <td>0.821390</td>\n",
       "      <td>0.483276</td>\n",
       "      <td>0.151591</td>\n",
       "      <td>0.089064</td>\n",
       "      <td>0.419202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>0.195341</td>\n",
       "      <td>0.732948</td>\n",
       "      <td>0.263283</td>\n",
       "      <td>0.114817</td>\n",
       "      <td>0.732190</td>\n",
       "      <td>0.208790</td>\n",
       "      <td>0.883497</td>\n",
       "      <td>0.406249</td>\n",
       "      <td>0.638846</td>\n",
       "      <td>0.004345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>0.610261</td>\n",
       "      <td>0.260848</td>\n",
       "      <td>0.704003</td>\n",
       "      <td>0.830197</td>\n",
       "      <td>0.358328</td>\n",
       "      <td>0.515965</td>\n",
       "      <td>0.170293</td>\n",
       "      <td>0.696782</td>\n",
       "      <td>0.138579</td>\n",
       "      <td>0.027209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>0.613228</td>\n",
       "      <td>0.076795</td>\n",
       "      <td>0.110359</td>\n",
       "      <td>0.736368</td>\n",
       "      <td>0.833824</td>\n",
       "      <td>0.767194</td>\n",
       "      <td>0.478303</td>\n",
       "      <td>0.258643</td>\n",
       "      <td>0.792118</td>\n",
       "      <td>0.829028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>0.348914</td>\n",
       "      <td>0.446660</td>\n",
       "      <td>0.244604</td>\n",
       "      <td>0.699061</td>\n",
       "      <td>0.668308</td>\n",
       "      <td>0.807250</td>\n",
       "      <td>0.948232</td>\n",
       "      <td>0.740865</td>\n",
       "      <td>0.723951</td>\n",
       "      <td>0.669176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>0.551321</td>\n",
       "      <td>0.517915</td>\n",
       "      <td>0.590590</td>\n",
       "      <td>0.621895</td>\n",
       "      <td>0.938602</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.965872</td>\n",
       "      <td>0.174674</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>0.893580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>0.510703</td>\n",
       "      <td>0.641008</td>\n",
       "      <td>0.597058</td>\n",
       "      <td>0.836380</td>\n",
       "      <td>0.486169</td>\n",
       "      <td>0.707761</td>\n",
       "      <td>0.171017</td>\n",
       "      <td>0.992987</td>\n",
       "      <td>0.867125</td>\n",
       "      <td>0.103003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>0.896420</td>\n",
       "      <td>0.069793</td>\n",
       "      <td>0.242407</td>\n",
       "      <td>0.813713</td>\n",
       "      <td>0.125486</td>\n",
       "      <td>0.624688</td>\n",
       "      <td>0.114793</td>\n",
       "      <td>0.855732</td>\n",
       "      <td>0.727102</td>\n",
       "      <td>0.922743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>0.204058</td>\n",
       "      <td>0.908315</td>\n",
       "      <td>0.832946</td>\n",
       "      <td>0.277607</td>\n",
       "      <td>0.671420</td>\n",
       "      <td>0.214868</td>\n",
       "      <td>0.028487</td>\n",
       "      <td>0.788154</td>\n",
       "      <td>0.819188</td>\n",
       "      <td>0.212661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>0.667661</td>\n",
       "      <td>0.570533</td>\n",
       "      <td>0.492793</td>\n",
       "      <td>0.723563</td>\n",
       "      <td>0.140785</td>\n",
       "      <td>0.595664</td>\n",
       "      <td>0.364166</td>\n",
       "      <td>0.380546</td>\n",
       "      <td>0.658169</td>\n",
       "      <td>0.637146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>0.057238</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.315702</td>\n",
       "      <td>0.213568</td>\n",
       "      <td>0.243748</td>\n",
       "      <td>0.242536</td>\n",
       "      <td>0.282915</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>0.744751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>0.869812</td>\n",
       "      <td>0.710203</td>\n",
       "      <td>0.738248</td>\n",
       "      <td>0.774844</td>\n",
       "      <td>0.962040</td>\n",
       "      <td>0.679713</td>\n",
       "      <td>0.775336</td>\n",
       "      <td>0.295225</td>\n",
       "      <td>0.277380</td>\n",
       "      <td>0.252673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>0.551277</td>\n",
       "      <td>0.646111</td>\n",
       "      <td>0.450136</td>\n",
       "      <td>0.935562</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>0.800320</td>\n",
       "      <td>0.473424</td>\n",
       "      <td>0.735194</td>\n",
       "      <td>0.363953</td>\n",
       "      <td>0.985046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>0.084433</td>\n",
       "      <td>0.766219</td>\n",
       "      <td>0.900194</td>\n",
       "      <td>0.544504</td>\n",
       "      <td>0.117080</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.565527</td>\n",
       "      <td>0.820625</td>\n",
       "      <td>0.250383</td>\n",
       "      <td>0.358244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>0.808619</td>\n",
       "      <td>0.477433</td>\n",
       "      <td>0.678510</td>\n",
       "      <td>0.514407</td>\n",
       "      <td>0.082057</td>\n",
       "      <td>0.060509</td>\n",
       "      <td>0.215683</td>\n",
       "      <td>0.367292</td>\n",
       "      <td>0.917254</td>\n",
       "      <td>0.319785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>0.669252</td>\n",
       "      <td>0.102862</td>\n",
       "      <td>0.534131</td>\n",
       "      <td>0.909298</td>\n",
       "      <td>0.044616</td>\n",
       "      <td>0.966442</td>\n",
       "      <td>0.694916</td>\n",
       "      <td>0.986454</td>\n",
       "      <td>0.906468</td>\n",
       "      <td>0.269044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>0.565543</td>\n",
       "      <td>0.296225</td>\n",
       "      <td>0.203937</td>\n",
       "      <td>0.955739</td>\n",
       "      <td>0.403871</td>\n",
       "      <td>0.750305</td>\n",
       "      <td>0.359163</td>\n",
       "      <td>0.804151</td>\n",
       "      <td>0.317093</td>\n",
       "      <td>0.958971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>0.361501</td>\n",
       "      <td>0.425501</td>\n",
       "      <td>0.761733</td>\n",
       "      <td>0.911328</td>\n",
       "      <td>0.565218</td>\n",
       "      <td>0.111656</td>\n",
       "      <td>0.248241</td>\n",
       "      <td>0.767460</td>\n",
       "      <td>0.946398</td>\n",
       "      <td>0.970970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>0.730241</td>\n",
       "      <td>0.903154</td>\n",
       "      <td>0.903179</td>\n",
       "      <td>0.720177</td>\n",
       "      <td>0.869825</td>\n",
       "      <td>0.698690</td>\n",
       "      <td>0.820143</td>\n",
       "      <td>0.195587</td>\n",
       "      <td>0.212087</td>\n",
       "      <td>0.835486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0.680807</td>\n",
       "      <td>0.139323</td>\n",
       "      <td>0.130641</td>\n",
       "      <td>0.651744</td>\n",
       "      <td>0.166402</td>\n",
       "      <td>0.944825</td>\n",
       "      <td>0.742101</td>\n",
       "      <td>0.092252</td>\n",
       "      <td>0.505294</td>\n",
       "      <td>0.560880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0.738594</td>\n",
       "      <td>0.319122</td>\n",
       "      <td>0.874457</td>\n",
       "      <td>0.733942</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.555561</td>\n",
       "      <td>0.648197</td>\n",
       "      <td>0.431764</td>\n",
       "      <td>0.267838</td>\n",
       "      <td>0.085513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0.952205</td>\n",
       "      <td>0.240989</td>\n",
       "      <td>0.875906</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>0.783399</td>\n",
       "      <td>0.087340</td>\n",
       "      <td>0.451245</td>\n",
       "      <td>0.533468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0.789187</td>\n",
       "      <td>0.378905</td>\n",
       "      <td>0.093275</td>\n",
       "      <td>0.144499</td>\n",
       "      <td>0.953608</td>\n",
       "      <td>0.628651</td>\n",
       "      <td>0.756670</td>\n",
       "      <td>0.900834</td>\n",
       "      <td>0.247202</td>\n",
       "      <td>0.158151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0.074884</td>\n",
       "      <td>0.182344</td>\n",
       "      <td>0.741149</td>\n",
       "      <td>0.520930</td>\n",
       "      <td>0.285287</td>\n",
       "      <td>0.901929</td>\n",
       "      <td>0.482509</td>\n",
       "      <td>0.365267</td>\n",
       "      <td>0.912343</td>\n",
       "      <td>0.150371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.205961  0.752730  0.793832  0.931900  0.022322  0.560578  0.356262   \n",
       "1      0.546661  0.594928  0.678584  0.464412  0.367392  0.475764  0.124166   \n",
       "2      0.139117  0.904341  0.392153  0.040919  0.776070  0.862407  0.366219   \n",
       "3      0.267858  0.016171  0.307106  0.408035  0.553078  0.024696  0.910081   \n",
       "4      0.678581  0.488243  0.380064  0.348321  0.531539  0.389896  0.155341   \n",
       "5      0.181558  0.457969  0.428100  0.361580  0.985968  0.168864  0.827005   \n",
       "6      0.117905  0.179908  0.634849  0.062925  0.306602  0.002233  0.210654   \n",
       "7      0.702963  0.034494  0.864116  0.706103  0.307866  0.928858  0.600351   \n",
       "8      0.983945  0.278483  0.766428  0.019837  0.254173  0.345996  0.210059   \n",
       "9      0.318207  0.235657  0.117392  0.629639  0.016978  0.098106  0.808621   \n",
       "10     0.994832  0.579945  0.937188  0.960419  0.874817  0.378723  0.366119   \n",
       "11     0.272776  0.933193  0.225498  0.753867  0.019637  0.302532  0.672089   \n",
       "12     0.090779  0.407881  0.528715  0.625381  0.412331  0.463272  0.680353   \n",
       "13     0.627415  0.793536  0.744582  0.336845  0.578582  0.337614  0.243639   \n",
       "14     0.402801  0.696428  0.890732  0.392844  0.136690  0.989814  0.298524   \n",
       "15     0.468181  0.695655  0.062253  0.340750  0.744692  0.621135  0.397867   \n",
       "16     0.199114  0.670631  0.002600  0.154864  0.373549  0.345664  0.859116   \n",
       "17     0.271983  0.977070  0.752308  0.754374  0.961897  0.073666  0.495852   \n",
       "18     0.329352  0.996310  0.719027  0.941119  0.635347  0.765939  0.994667   \n",
       "19     0.345919  0.934395  0.005800  0.830581  0.364326  0.661547  0.857579   \n",
       "20     0.135608  0.247532  0.372034  0.443752  0.813589  0.283779  0.728035   \n",
       "21     0.309997  0.074547  0.482073  0.807747  0.921738  0.190960  0.039481   \n",
       "22     0.662282  0.037180  0.611326  0.985801  0.158272  0.243433  0.967082   \n",
       "23     0.686433  0.012451  0.336747  0.853751  0.129311  0.464014  0.513921   \n",
       "24     0.846452  0.639110  0.249894  0.670798  0.005653  0.668539  0.313684   \n",
       "25     0.822020  0.025795  0.357637  0.122199  0.577410  0.335401  0.219860   \n",
       "26     0.832589  0.451495  0.378683  0.412014  0.744574  0.883010  0.032333   \n",
       "27     0.429096  0.587700  0.721500  0.971395  0.037398  0.285666  0.239240   \n",
       "28     0.304074  0.130299  0.210665  0.388072  0.183173  0.255862  0.765204   \n",
       "29     0.667816  0.583901  0.290988  0.768389  0.240884  0.033877  0.931809   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99970  0.901299  0.084045  0.800021  0.989091  0.415169  0.521158  0.050280   \n",
       "99971  0.106475  0.054606  0.764617  0.433854  0.735345  0.831844  0.703218   \n",
       "99972  0.526424  0.826377  0.513169  0.039972  0.748886  0.842882  0.644847   \n",
       "99973  0.650727  0.040572  0.316785  0.035805  0.848732  0.500766  0.589824   \n",
       "99974  0.370358  0.872598  0.381020  0.947291  0.048454  0.741614  0.244881   \n",
       "99975  0.872774  0.154882  0.250283  0.012054  0.054436  0.903306  0.443816   \n",
       "99976  0.582898  0.836153  0.609029  0.547384  0.975755  0.821390  0.483276   \n",
       "99977  0.195341  0.732948  0.263283  0.114817  0.732190  0.208790  0.883497   \n",
       "99978  0.610261  0.260848  0.704003  0.830197  0.358328  0.515965  0.170293   \n",
       "99979  0.613228  0.076795  0.110359  0.736368  0.833824  0.767194  0.478303   \n",
       "99980  0.348914  0.446660  0.244604  0.699061  0.668308  0.807250  0.948232   \n",
       "99981  0.551321  0.517915  0.590590  0.621895  0.938602  0.013544  0.965872   \n",
       "99982  0.510703  0.641008  0.597058  0.836380  0.486169  0.707761  0.171017   \n",
       "99983  0.896420  0.069793  0.242407  0.813713  0.125486  0.624688  0.114793   \n",
       "99984  0.204058  0.908315  0.832946  0.277607  0.671420  0.214868  0.028487   \n",
       "99985  0.667661  0.570533  0.492793  0.723563  0.140785  0.595664  0.364166   \n",
       "99986  0.057238  0.804979  0.315702  0.213568  0.243748  0.242536  0.282915   \n",
       "99987  0.869812  0.710203  0.738248  0.774844  0.962040  0.679713  0.775336   \n",
       "99988  0.551277  0.646111  0.450136  0.935562  0.023471  0.800320  0.473424   \n",
       "99989  0.084433  0.766219  0.900194  0.544504  0.117080  0.592871  0.565527   \n",
       "99990  0.808619  0.477433  0.678510  0.514407  0.082057  0.060509  0.215683   \n",
       "99991  0.669252  0.102862  0.534131  0.909298  0.044616  0.966442  0.694916   \n",
       "99992  0.565543  0.296225  0.203937  0.955739  0.403871  0.750305  0.359163   \n",
       "99993  0.361501  0.425501  0.761733  0.911328  0.565218  0.111656  0.248241   \n",
       "99994  0.730241  0.903154  0.903179  0.720177  0.869825  0.698690  0.820143   \n",
       "99995  0.680807  0.139323  0.130641  0.651744  0.166402  0.944825  0.742101   \n",
       "99996  0.738594  0.319122  0.874457  0.733942  0.898350  0.555561  0.648197   \n",
       "99997  0.952205  0.240989  0.875906  0.004081  0.180400  0.963737  0.783399   \n",
       "99998  0.789187  0.378905  0.093275  0.144499  0.953608  0.628651  0.756670   \n",
       "99999  0.074884  0.182344  0.741149  0.520930  0.285287  0.901929  0.482509   \n",
       "\n",
       "              7         8         9  \n",
       "0      0.222053  0.165362  0.946916  \n",
       "1      0.889422  0.201139  0.502694  \n",
       "2      0.084257  0.666566  0.380420  \n",
       "3      0.284892  0.620891  0.894443  \n",
       "4      0.463131  0.686823  0.653478  \n",
       "5      0.820099  0.559832  0.144888  \n",
       "6      0.211758  0.890701  0.463268  \n",
       "7      0.663327  0.865569  0.534681  \n",
       "8      0.073721  0.347150  0.569863  \n",
       "9      0.047932  0.642748  0.428816  \n",
       "10     0.060426  0.762179  0.419102  \n",
       "11     0.968116  0.106582  0.662339  \n",
       "12     0.963150  0.369255  0.559434  \n",
       "13     0.849945  0.034087  0.195536  \n",
       "14     0.218205  0.159483  0.059446  \n",
       "15     0.210516  0.386057  0.156298  \n",
       "16     0.858643  0.075119  0.227797  \n",
       "17     0.757288  0.166856  0.107706  \n",
       "18     0.202007  0.681813  0.401634  \n",
       "19     0.299343  0.727708  0.083016  \n",
       "20     0.774027  0.678473  0.630954  \n",
       "21     0.583913  0.333382  0.044302  \n",
       "22     0.395908  0.757042  0.258962  \n",
       "23     0.443276  0.624436  0.267381  \n",
       "24     0.553432  0.274983  0.628946  \n",
       "25     0.353884  0.054750  0.448671  \n",
       "26     0.266259  0.259209  0.210100  \n",
       "27     0.518312  0.878549  0.321848  \n",
       "28     0.724984  0.863846  0.142549  \n",
       "29     0.791880  0.888836  0.188053  \n",
       "...         ...       ...       ...  \n",
       "99970  0.780308  0.658084  0.774017  \n",
       "99971  0.692005  0.452095  0.998338  \n",
       "99972  0.676567  0.959793  0.109331  \n",
       "99973  0.840883  0.262383  0.062775  \n",
       "99974  0.401336  0.482101  0.185830  \n",
       "99975  0.173780  0.143478  0.527533  \n",
       "99976  0.151591  0.089064  0.419202  \n",
       "99977  0.406249  0.638846  0.004345  \n",
       "99978  0.696782  0.138579  0.027209  \n",
       "99979  0.258643  0.792118  0.829028  \n",
       "99980  0.740865  0.723951  0.669176  \n",
       "99981  0.174674  0.028416  0.893580  \n",
       "99982  0.992987  0.867125  0.103003  \n",
       "99983  0.855732  0.727102  0.922743  \n",
       "99984  0.788154  0.819188  0.212661  \n",
       "99985  0.380546  0.658169  0.637146  \n",
       "99986  0.894500  0.011059  0.744751  \n",
       "99987  0.295225  0.277380  0.252673  \n",
       "99988  0.735194  0.363953  0.985046  \n",
       "99989  0.820625  0.250383  0.358244  \n",
       "99990  0.367292  0.917254  0.319785  \n",
       "99991  0.986454  0.906468  0.269044  \n",
       "99992  0.804151  0.317093  0.958971  \n",
       "99993  0.767460  0.946398  0.970970  \n",
       "99994  0.195587  0.212087  0.835486  \n",
       "99995  0.092252  0.505294  0.560880  \n",
       "99996  0.431764  0.267838  0.085513  \n",
       "99997  0.087340  0.451245  0.533468  \n",
       "99998  0.900834  0.247202  0.158151  \n",
       "99999  0.365267  0.912343  0.150371  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
