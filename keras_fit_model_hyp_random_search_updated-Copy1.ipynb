{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "from datetime import datetime\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "import make_data_wfpt as mdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(os.getcwd() + '/data_storage/data_11000000_from_simulation_mix_09_12_18_18_20_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some cleaning of the data\n",
    "data = data[['v', 'a', 'w', 'rt', 'choice', 'nf_likelihood']]\n",
    "data = data.loc[data['w'] > 0.1]\n",
    "data = data.loc[data['w'] < 0.9]\n",
    "data = data.loc[data['a'] > 0.5]\n",
    "\n",
    "mini_data = data.loc[1:10000]\n",
    "\n",
    "\n",
    "train_f, train_l, test_f, test_l = mdw.train_test_split_rt_choice(data = data,\n",
    "                                                                  write_to_file = False,\n",
    "                                                                  from_file = False,\n",
    "                                                                  p_train = 0.8,\n",
    "                                                                  backend = 'keras')\n",
    "# Choice probabilities\n",
    "# train_f, train_l, test_f, test_l = mdw.train_test_from_file_choice_probabilities(n_samples = 2500000,\n",
    "#                                                             f_signature = '_choice_probabilities_analytic_',\n",
    "#                                                                                 backend = 'keras')\n",
    "\n",
    "# rt_choice\n",
    "# train_f, train_l, test_f, test_l = mdw.train_test_from_file_rt_choice(n_samples = 11000000,\n",
    "#                                                                       f_signature = '_from_simulation_mix_',\n",
    "#                                                                       backend = 'keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dnnk class (cpm for choice probability model)\n",
    "cpm = dnnk.dnn_trainer()\n",
    "cpm.data['train_features'] = train_f\n",
    "cpm.data['train_labels'] = train_l\n",
    "cpm.data['test_features'] = test_f\n",
    "cpm.data['test_labels'] = test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_shape': 3,\n",
       " 'output_shape': 1,\n",
       " 'output_activation': 'sigmoid',\n",
       " 'hidden_layers': [20, 20, 20, 20],\n",
       " 'hidden_activations': ['relu', 'relu', 'relu', 'relu'],\n",
       " 'l1_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_activation': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l1_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'l2_kernel': [0.0, 0.0, 0.0, 0.0],\n",
       " 'optimizer': 'Nadam',\n",
       " 'loss': 'mse',\n",
       " 'metrics': ['mse']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make all parameters we can specify explicit\n",
    "# Model parameters\n",
    "cpm.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'callback_funs': ['ReduceLROnPlateau', 'EarlyStopping', 'ModelCheckpoint'],\n",
       " 'plateau_patience': 10,\n",
       " 'min_delta': 0.0001,\n",
       " 'early_stopping_patience': 15,\n",
       " 'callback_monitor': 'loss',\n",
       " 'min_learning_rate': 1e-07,\n",
       " 'red_coef_learning_rate': 0.1,\n",
       " 'ckpt_period': 10,\n",
       " 'ckpt_save_best_only': True,\n",
       " 'ckpt_save_weights_only': True,\n",
       " 'max_train_epochs': 2000,\n",
       " 'batch_size': 10000,\n",
       " 'warm_start': False,\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_cnt': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters governing training\n",
    "cpm.train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_type': 'choice_probabilities',\n",
       " 'model_directory': '/home/afengler/git_repos/nn_likelihoods/keras_models',\n",
       " 'checkpoint': 'ckpt',\n",
       " 'model_name': 'dnnregressor',\n",
       " 'data_type_signature': '_choice_probabilities_analytic_',\n",
       " 'timestamp': '09_22_18_18_56_49',\n",
       " 'training_data_size': 2500000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters concerning data storage\n",
    "cpm.data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFYING META PARAMETERS THAT STAY CONSTANT DURING HYPERPARAMETER OPTIMIZATION\n",
    "\n",
    "# Model params\n",
    "cpm.model_params['output_activation'] = 'linear'\n",
    "cpm.model_params['input_shape'] = 5\n",
    "\n",
    "# Training params\n",
    "# Meta\n",
    "cpm.train_params['early_stopping_patience'] = 5\n",
    "cpm.train_params['plateau_patience'] = 3\n",
    "cpm.train_params['min_delta'] = 0.002\n",
    "cpm.train_params['ckpt_period'] = 1\n",
    "cpm.train_params['model_cnt'] = 0\n",
    "cpm.train_params['max_train_epochs'] = 120\n",
    "\n",
    "# Hyper\n",
    "#cpm.train_params['l1_kernel']\n",
    "cpm.model_params['hidden_layers'] = [5, 5, 5, 5]\n",
    "#cpm.train_params['hidden_activations']\n",
    "#cpm.train_params['l2_kernel'] = [0.5, 0.5, 0.5, 0.5]\n",
    "#cpm.train_params['l2_activation'] = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "# Data params\n",
    "cpm.data_params['data_type'] = 'wfpt'\n",
    "cpm.data_params['data_type_signature'] = '_choice_rt_'\n",
    "cpm.data_params['training_data_size'] = 11000000\n",
    "\n",
    "# Update timestamp\n",
    "cpm.data_params['timestamp'] = datetime.now().strftime('%m_%d_%y_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model\n",
    "# cpm.keras_model_generate(save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# cpm.run_training(save_history = True, \n",
    "#                  warm_start = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  10000\n",
      "layers:  [100, 100, 100, 100]\n",
      "hidden_activations: ['relu', 'relu', 'relu', 'relu']\n",
      "l1_activatons:  [0.0, 0.0, 0.0, 0.0]\n",
      "l2_activations: [0.0, 0.0, 0.0, 0.0]\n",
      "l1_kernel:  [0.0, 0.0, 0.0, 0.0]\n",
      "l2_kernel:  [0.0, 0.0, 0.0, 0.0]\n",
      "Train on 6070023 samples, validate on 1517984 samples\n",
      "Epoch 1/120\n",
      "2868576/6070023 [=============>................] - ETA: 2:11 - loss: 3.0869 - mean_squared_error: 3.0869"
     ]
    }
   ],
   "source": [
    "# Hyperparameter training loop:\n",
    "\n",
    "# Runs: \n",
    "num_runs = 1\n",
    "cnt = 0\n",
    "max_layers = 5\n",
    "layer_sizes = [100, 100]\n",
    "batch_sizes = [10000]\n",
    "regularization_sizes = [0.05, 0.1, 0.2]\n",
    "\n",
    "# Update model directory to make sure we collect all our models from this hyperparameter optimization run in the same place\n",
    "cpm.data_params['model_directory'] =  '/home/afengler/git_repos/nn_likelihoods/keras_models/'\n",
    "cpm.data_params['model_name'] = 'dnnregressor_wftp_hyp_opt'\n",
    "cpm.train_params['model_cnt'] = 0\n",
    "\n",
    "histories = []\n",
    "\n",
    "while cnt < num_runs:\n",
    "    cnt += 1\n",
    "    \n",
    "    # Sample # layers \n",
    "    num_layers =  4 # np.random.choice(np.arange(4, max_layers + 1, 1))\n",
    "    \n",
    "    # Layer sizes\n",
    "    layers = []\n",
    "    activations = []\n",
    "    regularizers_l1 = []\n",
    "    regularizers_l2 = []\n",
    "    regularizer = np.random.choice(['none', 'none'])\n",
    "    regularizer_size = np.random.choice(regularization_sizes)\n",
    "    \n",
    "    for i in range(0, num_layers, 1):\n",
    "        layers.append(np.random.choice(layer_sizes))\n",
    "        activations.append('relu')\n",
    "        if regularizer == 'l1':\n",
    "            regularizers_l1.append(regularizer_size)\n",
    "            regularizers_l2.append(0.0)\n",
    "        if regularizer == 'l2':\n",
    "            regularizers_l1.append(0.0)\n",
    "            regularizers_l2.append(regularizer_size)\n",
    "        else:\n",
    "            regularizers_l1.append(0.0)\n",
    "            regularizers_l2.append(0.0)\n",
    "        \n",
    "    # Batch size\n",
    "    batch_size = np.random.choice(batch_sizes)\n",
    "    \n",
    "    # Update relevant model parameters\n",
    "    cpm.train_params['batch_size'] = batch_size\n",
    "    print('batch_size: ', batch_size)\n",
    "    cpm.model_params['hidden_layers'] = layers\n",
    "    print('layers: ', layers)\n",
    "    cpm.model_params['hidden_activations'] = activations\n",
    "    print('hidden_activations:', activations)\n",
    "    cpm.model_params['l1_activation'] = regularizers_l1\n",
    "    print('l1_activatons: ', regularizers_l1)\n",
    "    cpm.model_params['l2_activation'] = regularizers_l2\n",
    "    print('l2_activations:', regularizers_l2)\n",
    "    cpm.model_params['l1_kernel'] = regularizers_l1\n",
    "    print('l1_kernel: ', regularizers_l1)\n",
    "    cpm.model_params['l2_kernel'] = regularizers_l2\n",
    "    print('l2_kernel: ', regularizers_l2)\n",
    "    \n",
    "    # Make new timestamp\n",
    "    #cpm.data_params['timestamp'] = datetime.now().strftime('%m_%d_%y_%H_%M_%S')\n",
    "    \n",
    "    # Make model\n",
    "    cpm.keras_model_generate(save_model = True)\n",
    "    \n",
    "    # Train model\n",
    "    cpm.run_training(save_history = True, \n",
    "                     warm_start = False) # Note that this increments model count automatically !\n",
    "    \n",
    "#     histories[-1]['model_cnt'] = cpm.train_params['model_cnt']\n",
    "#     histories[-1]['num_layers'] = num_layers\n",
    "#     histories[-1]['size_layers'] = str(layers)\n",
    "#     histories[-1]['activations'] = str(activations) \n",
    "#     histories[-1]['batch_size'] = batch_size\n",
    "    \n",
    "    print(cnt)\n",
    "    \n",
    "# histories = pd.concat(histories)\n",
    "# histories['optimizer'] = cpm.model_params['optimizer']\n",
    "# histories['timestamp'] = datetime.now().strftime('%m_%d_%y_%H_%M_%S')\n",
    "# histories.to_csv(cpm.data_params['model_directory'] + cpm.data_params['model_name'] + '_choice_rt_' +\\\n",
    "#                  cpm.data_params['timestamp'] + '/hyp_opt_histories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
