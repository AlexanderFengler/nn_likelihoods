{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from cdwiener import array_fptd\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import yaml\n",
    "import keras_to_numpy as ktnp\n",
    "import glob\n",
    "\n",
    "from kde_training_utilities import kde_load_data\n",
    "from kde_training_utilities import kde_make_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE ---------\n",
    "method = \"weibull_cdf_ndt\" # ddm, linear_collapse, ornstein, full, lba\n",
    "machine = 'x7'\n",
    "# ----------------\n",
    "\n",
    "# INITIALIZATIONS ----------------------------------------------------------------\n",
    "stats = pickle.load(open(\"kde_stats.pickle\", \"rb\"))[method]\n",
    "dnn_params = yaml.load(open(\"hyperparameters.yaml\"))\n",
    "\n",
    "if machine == 'x7':\n",
    "    data_folder = stats[\"data_folder_x7\"]\n",
    "    model_path = stats[\"model_folder_x7\"]\n",
    "else:\n",
    "    data_folder = stats[\"data_folder\"]\n",
    "    model_path = stats[\"model_folder\"]\n",
    "    \n",
    "model_path += dnn_params[\"model_type\"] + \"_{}_\".format(method) + datetime.now().strftime('%m_%d_%y_%H_%M_%S') + \"/\"\n",
    "\n",
    "print('if it does not exist, make model path')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "# Copy hyperparameter setup into model path\n",
    "if machine == 'x7':\n",
    "    os.system(\"cp {} {}\".format(\"/media/data_cifs/afengler/git_repos/nn_likelihoods/hyperparameters.yaml\", model_path))\n",
    "else:\n",
    "    os.system(\"cp {} {}\".format(\"/users/afengler/git_repos/nn_likelihoods/hyperparameters.yaml\", model_path))\n",
    "    \n",
    "# set up gpu to use\n",
    "if machine == 'x7':\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = dnn_params['gpu_x7'] \n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# Load the training data\n",
    "print('loading data.... ')\n",
    "\n",
    "# X, y, X_val, y_val = kde_load_data(folder = data_folder, \n",
    "#                                    return_log = True, # Dont take log if you want to train on actual likelihoods\n",
    "#                                    prelog_cutoff = 1e-7 # cut out data with likelihood lower than 1e-7\n",
    "#                                   )\n",
    "\n",
    "# X = np.array(X)\n",
    "# X_val = np.array(X_val)\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE MODEL ---------------------------------------------------------------------\n",
    "print('Setting up keras model')\n",
    "\n",
    "input_shape = 8 #X.shape[1]\n",
    "model = keras.Sequential()\n",
    "\n",
    "for i in range(len(dnn_params['hidden_layers'])):\n",
    "    if i == 0:\n",
    "        model.add(keras.layers.Dense(units = dnn_params[\"hidden_layers\"][i], \n",
    "                                     activation = dnn_params[\"hidden_activations\"][i], \n",
    "                                     input_dim = input_shape))\n",
    "    else:\n",
    "        model.add(keras.layers.Dense(units = dnn_params[\"hidden_layers\"][i],\n",
    "                                     activation = dnn_params[\"hidden_activations\"][i]))\n",
    "        \n",
    "# Write model specification to yaml file        \n",
    "spec = model.to_yaml()\n",
    "open(model_path + \"model_spec.yaml\", \"w\").write(spec)\n",
    "\n",
    "\n",
    "print('STRUCTURE OF GENERATED MODEL: ....')\n",
    "print(model.summary())\n",
    "\n",
    "if dnn_params['loss'] == 'huber':\n",
    "    model.compile(loss = tf.losses.huber_loss, \n",
    "                  optimizer = \"adam\", \n",
    "                  metrics = [\"mse\"])\n",
    "\n",
    "if dnn_params['loss'] == 'mse':\n",
    "    model.compile(loss = 'mse', \n",
    "                  optimizer = \"adam\", \n",
    "                  metrics = [\"mse\"])\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(path):\n",
    "    while True:\n",
    "        files_ = glob.glob(path + '/data_*')\n",
    "        files_ = np.random.permutation(files_)\n",
    "        for file_ in files_:\n",
    "            with open(file_, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                np.random.shuffle(data.values)\n",
    "                data.reset_index(drop = True, inplace = True)\n",
    "                n = data.shape[0]\n",
    "                n_cols = data.shape[1]\n",
    "                batch_size = 200000\n",
    "                i = 0\n",
    "                while (i * batch_size < (n - batch_size)):\n",
    "                    yield (data.iloc[(i * batch_size): ((i + 1) * batch_size ), :(n_cols - 1)].to_numpy(), \n",
    "                           np.expand_dims(data.iloc[(i * batch_size): ((i + 1) * batch_size ), (n_cols - 1)].to_numpy() , axis = 1))\n",
    "            \n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT MODEL -----------------------------------------------------------------\n",
    "print('Starting to fit model.....')\n",
    "\n",
    "# Define callbacks\n",
    "ckpt_filename = model_path + \"model.h5\"\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(ckpt_filename, \n",
    "                                             monitor = 'val_loss', \n",
    "                                             verbose = 1, \n",
    "                                             save_best_only = False)\n",
    "                               \n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', \n",
    "                                              min_delta = 0, \n",
    "                                              verbose = 1, \n",
    "                                              patience = 2)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                                              factor = 0.1,\n",
    "                                              patience = 1, \n",
    "                                              verbose = 1,\n",
    "                                              min_delta = 0.0001,\n",
    "                                              min_lr = 0.0000001)\n",
    "\n",
    "history = model.fit_generator(generate_samples('/media/data_cifs/afengler/data/kde/' + \\\n",
    "                              'weibull_cdf/train_test_data_ndt_20000/'),\n",
    "                              steps_per_epoch = 1000,\n",
    "                              epochs = 50,\n",
    "                              #batch_size = dnn_params[\"batch_size\"], \n",
    "                              #callbacks = [checkpoint, reduce_lr, earlystopping], \n",
    "                              verbose = 1,\n",
    "                              max_queue_size = 25)\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, \n",
    "                 list_IDs, \n",
    "                 file_IDs, # np.array\n",
    "                 labels,\n",
    "                 batch_size = 32,\n",
    "                 dim = (32,32,32), \n",
    "                 n_channels = 1,\n",
    "                 n_classes = 10, \n",
    "                 shuffle = True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.file_IDs = file_IDs\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.max_index = labels.shape[0]\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        # Load new file into memory if index signals end of file\n",
    "        if index == self.max_index or index == 0:\n",
    "            np.random.shuffle(self.file_IDs)\n",
    "            \n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        #y = np.empty((self.batch_size), dtype=int)\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size, 1))\n",
    "        \n",
    "        # Generate data\n",
    "        \n",
    "        # IF ID signals e-o-f\n",
    "        # Load new file\n",
    "        \n",
    "        # Else yield ...\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes = self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING --------------------------------------------------------------------\n",
    "# print('Saving model and relevant data...')\n",
    "# # Log of training output\n",
    "# pd.DataFrame(history.history).to_csv(model_path + \"training_history.csv\")\n",
    "\n",
    "# # Save Model\n",
    "# model.save(model_path + \"model_final.h5\")\n",
    "\n",
    "# # Extract model architecture as numpy arrays and save in model path\n",
    "# __, ___, ____, = ktnp.extract_architecture(model, save = True, save_path = model_path)\n",
    "\n",
    "# # Update model paths in model_path.yaml\n",
    "# model_paths = yaml.load(open(\"model_paths.yaml\"))\n",
    "# model_paths[method] = model_path\n",
    "# yaml.dump(model_paths, open(\"model_paths.yaml\", \"w\"))\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "t = pickle.load(open('/media/data_cifs/afengler/data/kde/' + \\\n",
    "                     'weibull_cdf/train_test_data_ndt_20000/data_77ff8ed6fa2411e9aea9073b18a43faf.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/media/data_cifs/afengler/data/tmp/npsavetest.npy', t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "test = np.load('/media/data_cifs/afengler/data/tmp/npsavetest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(np.random.uniform(size = (100000, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen = generate_samples(path = '/media/data_cifs/afengler/data/kde/' + \\\n",
    "                          'weibull_cdf/train_test_data_ndt_20000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = next(my_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[0].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[1].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(t2[1].to_numpy(), axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path = '',\n",
    "                              p_train = 0.8,\n",
    "                              n_files_out = 10,\n",
    "                              file_in_list = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glob.glob('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob('/media/data_cifs/afengler/data/kde/weibull_cdf/train_test_data_ndt_20000/data_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check if we have a train and test sets already\n",
      "read, concatenate and shuffle data\n",
      "get training and test indices\n",
      "writing to file...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kde_make_train_test_split(path = '/media/data_cifs/afengler/data/kde/weibull_cdf/train_test_data_ndt_20000/',\n",
    "                         n_files_out = 10,\n",
    "                         file_in_list = ['data_e6f83854fa2111e999dcf5d97ddd6f15.pickle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('/media/data_cifs/afengler/data/kde/weibull_cdf/train_test_data_ndt_20000/data_e6f83854fa2111e999dcf5d97ddd6f15.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
